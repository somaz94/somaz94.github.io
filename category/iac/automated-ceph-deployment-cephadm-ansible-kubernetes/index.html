<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    

    
    

    
    

    
    

    <!-- ✅ Google Tag Manager 추가 -->
    <script>
        (function(w,d,s,l,i){
            w[l]=w[l]||[];
            w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});
            var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';
            j.async=true;
            j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;
            f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer','GTM-MBP83N4Q');
    </script>
      <!-- ✅ End Google Tag Manager -->

    <!-- Mermaid.js 직접 로드 -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>

    <title>Automated Ceph Cluster Deployment with Cephadm-Ansible and Kubernetes Integration | somaz</title>
    <meta name="description" content="Step-by-step guide to deploying Ceph clusters using cephadm-ansible for automated host setup, cluster management, and purging, followed by Kubernetes integra...">
    
        <meta name="keywords" content="cephadm-ansible, ceph, ansible, kubernetes, ceph-csi, infrastructure-as-code, automation, storage, terraform, kubespray">
    

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Automated Ceph Cluster Deployment with Cephadm-Ansible and Kubernetes Integration | somaz">
    <meta name="twitter:description" content="Step-by-step guide to deploying Ceph clusters using cephadm-ansible for automated host setup, cluster management, and purging, followed by Kubernetes integra...">

    
        <meta property="twitter:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1755078890/cephadm-ansible-1_bataxt.png">
    
    
    
        <meta name="twitter:site" content="@twitter_username">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="https://somaz.blog/category/iac/automated-ceph-deployment-cephadm-ansible-kubernetes/">
    <meta property="og:title" content="Automated Ceph Cluster Deployment with Cephadm-Ansible and Kubernetes Integration | somaz">
    <meta property="og:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1755078890/cephadm-ansible-1_bataxt.png">
    <meta property="og:description" content="Step-by-step guide to deploying Ceph clusters using cephadm-ansible for automated host setup, cluster management, and purging, followed by Kubernetes integra...">
    <meta property="og:site_name" content="Somaz Tech Blog">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="somaz">
    <meta name="msapplication-TileColor" content="#141414">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#141414">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:300,400,700" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="canonical" href="https://somaz.blog/category/iac/automated-ceph-deployment-cephadm-ansible-kubernetes/">
    <link rel="alternate" type="application/rss+xml" title="Somaz Tech Blog" href="https://somaz.blog/feed.xml" />

    <!-- Include extra styles -->
    

    <!-- JavaScript enabled/disabled -->
    <script>
        document.querySelector('html').classList.remove('no-js');
    </script>

    <!-- Google Adsense -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8725590811736154"
        crossorigin="anonymous"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet"> -->
    <!-- <link href="https://cdn.jsdelivr.net/gh/sunn-us/SUIT/fonts/variable/woff2/SUIT-Variable.css" rel="stylesheet"> -->
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css2?family=Albert+Sans:wght@400;500;700&display=swap" rel="stylesheet"> -->

    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml" />

</head>
<!-- ✅ Google Tag Manager (noscript) -->
<noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MBP83N4Q"
            height="0" width="0" style="display:none;visibility:hidden">
    </iframe>
</noscript>
<!-- ✅ End Google Tag Manager (noscript) -->
    <body class="has-push-menu">
        





        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 1000 1000"><path d="M969.8,870.3c27,27.7,27,71.8,0,99.1C955.7,983,937.9,990,920,990c-17.9,0-35.7-7-49.7-20.7L500,599L129.6,969.4C115.6,983,97.8,990,79.9,990s-35.7-7-49.7-20.7c-27-27.3-27-71.4,0-99.1L400.9,500L30.3,129.3c-27-27.3-27-71.4,0-99.1c27.3-27,71.8-27,99.4,0L500,400.9L870.4,30.2c27.7-27,71.8-27,99.4,0c27,27.7,27,71.8,0,99.1L599.1,500L969.8,870.3z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-clock" viewBox="0 0 1000 1000"><path d="M500,10C229.8,10,10,229.8,10,500c0,270.2,219.8,490,490,490c270.2,0,490-219.8,490-490C990,229.8,770.2,10,500,10z M500,910.2c-226.2,0-410.2-184-410.2-410.2c0-226.2,184-410.2,410.2-410.2c226.2,0,410.2,184,410.2,410.2C910.2,726.1,726.2,910.2,500,910.2z M753.1,374c8.2,11.9,5.2,28.1-6.6,36.3L509.9,573.7c-4.4,3.1-9.6,4.6-14.8,4.6c-4.1,0-8.3-1-12.1-3c-8.6-4.5-14-13.4-14-23.1V202.5c0-14.4,11.7-26.1,26.1-26.1c14.4,0,26.1,11.7,26.1,26.1v300l195.6-135.1C728.7,359.2,744.9,362.1,753.1,374z"/></symbol><symbol id="icon-calendar" viewBox="0 0 1000 1000"><path d="M920,500v420H80V500H920 M990,430H10v490c0,38.7,31.3,70,70,70h840c38.7,0,70-31.3,70-70V430L990,430z"/><path d="M850,80v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80H360v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80C72.8,80,10,142.7,10,220v140h980V220C990,142.7,927.2,80,850,80z"/><path d="M255,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C290,25.8,274.3,10,255,10z"/><path d="M745,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C780,25.8,764.3,10,745,10z"/></symbol><symbol id="icon-github" viewBox="0 0 12 14"><path d="M6 1q1.633 0 3.012 0.805t2.184 2.184 0.805 3.012q0 1.961-1.145 3.527t-2.957 2.168q-0.211 0.039-0.312-0.055t-0.102-0.234q0-0.023 0.004-0.598t0.004-1.051q0-0.758-0.406-1.109 0.445-0.047 0.801-0.141t0.734-0.305 0.633-0.52 0.414-0.82 0.16-1.176q0-0.93-0.617-1.609 0.289-0.711-0.062-1.594-0.219-0.070-0.633 0.086t-0.719 0.344l-0.297 0.187q-0.727-0.203-1.5-0.203t-1.5 0.203q-0.125-0.086-0.332-0.211t-0.652-0.301-0.664-0.105q-0.352 0.883-0.062 1.594-0.617 0.68-0.617 1.609 0 0.664 0.16 1.172t0.41 0.82 0.629 0.523 0.734 0.305 0.801 0.141q-0.305 0.281-0.383 0.805-0.164 0.078-0.352 0.117t-0.445 0.039-0.512-0.168-0.434-0.488q-0.148-0.25-0.379-0.406t-0.387-0.187l-0.156-0.023q-0.164 0-0.227 0.035t-0.039 0.090 0.070 0.109 0.102 0.094l0.055 0.039q0.172 0.078 0.34 0.297t0.246 0.398l0.078 0.18q0.102 0.297 0.344 0.48t0.523 0.234 0.543 0.055 0.434-0.027l0.18-0.031q0 0.297 0.004 0.691t0.004 0.426q0 0.141-0.102 0.234t-0.312 0.055q-1.812-0.602-2.957-2.168t-1.145-3.527q0-1.633 0.805-3.012t2.184-2.184 3.012-0.805zM2.273 9.617q0.023-0.055-0.055-0.094-0.078-0.023-0.102 0.016-0.023 0.055 0.055 0.094 0.070 0.047 0.102-0.016zM2.516 9.883q0.055-0.039-0.016-0.125-0.078-0.070-0.125-0.023-0.055 0.039 0.016 0.125 0.078 0.078 0.125 0.023zM2.75 10.234q0.070-0.055 0-0.148-0.062-0.102-0.133-0.047-0.070 0.039 0 0.141t0.133 0.055zM3.078 10.562q0.062-0.062-0.031-0.148-0.094-0.094-0.156-0.023-0.070 0.062 0.031 0.148 0.094 0.094 0.156 0.023zM3.523 10.758q0.023-0.086-0.102-0.125-0.117-0.031-0.148 0.055t0.102 0.117q0.117 0.047 0.148-0.047zM4.016 10.797q0-0.102-0.133-0.086-0.125 0-0.125 0.086 0 0.102 0.133 0.086 0.125 0 0.125-0.086zM4.469 10.719q-0.016-0.086-0.141-0.070-0.125 0.023-0.109 0.117t0.141 0.062 0.109-0.109z"></path></symbol><symbol id="icon-medium" viewBox="0 0 1000 1000"><path d="M336.5,240.2v641.5c0,9.1-2.3,16.9-6.8,23.2s-11.2,9.6-20,9.6c-6.2,0-12.2-1.5-18-4.4L37.3,782.7c-7.7-3.6-14.1-9.8-19.4-18.3S10,747.4,10,739V115.5c0-7.3,1.8-13.5,5.5-18.6c3.6-5.1,8.9-7.7,15.9-7.7c5.1,0,13.1,2.7,24.1,8.2l279.5,140C335.9,238.6,336.5,239.5,336.5,240.2L336.5,240.2z M371.5,295.5l292,473.6l-292-145.5V295.5z M990,305.3v576.4c0,9.1-2.6,16.5-7.7,22.1c-5.1,5.7-12,8.5-20.8,8.5s-17.3-2.4-25.7-7.1L694.7,784.9L990,305.3z M988.4,239.7c0,1.1-46.8,77.6-140.3,229.4C754.6,621,699.8,709.8,683.8,735.7L470.5,389l177.2-288.2c6.2-10.2,15.7-15.3,28.4-15.3c5.1,0,9.8,1.1,14.2,3.3l295.9,147.7C987.6,237.1,988.4,238.2,988.4,239.7L988.4,239.7z"/></symbol><symbol id="icon-instagram" viewBox="0 0 489.84 489.84"><path d="M249.62,50.46c65.4,0,73.14.25,99,1.43C372.47,53,385.44,57,394.07,60.32a75.88,75.88,0,0,1,28.16,18.32,75.88,75.88,0,0,1,18.32,28.16c3.35,8.63,7.34,21.6,8.43,45.48,1.18,25.83,1.43,33.57,1.43,99s-0.25,73.14-1.43,99c-1.09,23.88-5.08,36.85-8.43,45.48a81.11,81.11,0,0,1-46.48,46.48c-8.63,3.35-21.6,7.34-45.48,8.43-25.82,1.18-33.57,1.43-99,1.43s-73.15-.25-99-1.43c-23.88-1.09-36.85-5.08-45.48-8.43A75.88,75.88,0,0,1,77,423.86,75.88,75.88,0,0,1,58.69,395.7c-3.35-8.63-7.34-21.6-8.43-45.48-1.18-25.83-1.43-33.57-1.43-99s0.25-73.14,1.43-99c1.09-23.88,5.08-36.85,8.43-45.48A75.88,75.88,0,0,1,77,78.64a75.88,75.88,0,0,1,28.16-18.32c8.63-3.35,21.6-7.34,45.48-8.43,25.83-1.18,33.57-1.43,99-1.43m0-44.13c-66.52,0-74.86.28-101,1.47s-43.87,5.33-59.45,11.38A120.06,120.06,0,0,0,45.81,47.44,120.06,120.06,0,0,0,17.56,90.82C11.5,106.4,7.36,124.2,6.17,150.27s-1.47,34.46-1.47,101,0.28,74.86,1.47,101,5.33,43.87,11.38,59.45a120.06,120.06,0,0,0,28.25,43.38,120.06,120.06,0,0,0,43.38,28.25c15.58,6.05,33.38,10.19,59.45,11.38s34.46,1.47,101,1.47,74.86-.28,101-1.47,43.87-5.33,59.45-11.38a125.24,125.24,0,0,0,71.63-71.63c6.05-15.58,10.19-33.38,11.38-59.45s1.47-34.46,1.47-101-0.28-74.86-1.47-101-5.33-43.87-11.38-59.45a120.06,120.06,0,0,0-28.25-43.38,120.06,120.06,0,0,0-43.38-28.25C394.47,13.13,376.67,9,350.6,7.8s-34.46-1.47-101-1.47h0Z" transform="translate(-4.7 -6.33)" /><path d="M249.62,125.48A125.77,125.77,0,1,0,375.39,251.25,125.77,125.77,0,0,0,249.62,125.48Zm0,207.41a81.64,81.64,0,1,1,81.64-81.64A81.64,81.64,0,0,1,249.62,332.89Z" transform="translate(-4.7 -6.33)"/><circle cx="375.66" cy="114.18" r="29.39" /></symbol><symbol id="icon-linkedin" viewBox="0 0 12 14"><path d="M2.727 4.883v7.742h-2.578v-7.742h2.578zM2.891 2.492q0.008 0.57-0.395 0.953t-1.059 0.383h-0.016q-0.641 0-1.031-0.383t-0.391-0.953q0-0.578 0.402-0.957t1.051-0.379 1.039 0.379 0.398 0.957zM12 8.187v4.437h-2.57v-4.141q0-0.82-0.316-1.285t-0.988-0.465q-0.492 0-0.824 0.27t-0.496 0.668q-0.086 0.234-0.086 0.633v4.32h-2.57q0.016-3.117 0.016-5.055t-0.008-2.313l-0.008-0.375h2.57v1.125h-0.016q0.156-0.25 0.32-0.438t0.441-0.406 0.68-0.34 0.895-0.121q1.336 0 2.148 0.887t0.813 2.598z"></path></symbol><symbol id="icon-heart" viewBox="0 0 34 30"><path d="M17,29.7 L16.4,29.2 C3.5,18.7 0,15 0,9 C0,4 4,0 9,0 C13.1,0 15.4,2.3 17,4.1 C18.6,2.3 20.9,0 25,0 C30,0 34,4 34,9 C34,15 30.5,18.7 17.6,29.2 L17,29.7 Z M9,2 C5.1,2 2,5.1 2,9 C2,14.1 5.2,17.5 17,27.1 C28.8,17.5 32,14.1 32,9 C32,5.1 28.9,2 25,2 C21.5,2 19.6,4.1 18.1,5.8 L17,7.1 L15.9,5.8 C14.4,4.1 12.5,2 9,2 Z" id="Shape"></path></symbol><symbol id="icon-arrow-right" viewBox="0 0 25.452 25.452"><path d="M4.471,24.929v-2.004l12.409-9.788c0.122-0.101,0.195-0.251,0.195-0.411c0-0.156-0.073-0.31-0.195-0.409L4.471,2.526V0.522c0-0.2,0.115-0.384,0.293-0.469c0.18-0.087,0.396-0.066,0.552,0.061l15.47,12.202c0.123,0.1,0.195,0.253,0.195,0.409c0,0.16-0.072,0.311-0.195,0.411L5.316,25.34c-0.155,0.125-0.372,0.147-0.552,0.061C4.586,25.315,4.471,25.13,4.471,24.929z"/></symbol><symbol id="icon-star" viewBox="0 0 48 48"><path fill="currentColor" d="M44,24c0,11.045-8.955,20-20,20S4,35.045,4,24S12.955,4,24,4S44,12.955,44,24z"/><path fill="#ffffff" d="M24,11l3.898,7.898l8.703,1.301l-6.301,6.102l1.5,8.699L24,30.898L16.199,35l1.5-8.699l-6.301-6.102  l8.703-1.301L24,11z"/></symbol><symbol id="icon-read" viewBox="0 0 32 32"><path fill="currentColor" d="M29,4H3C1.343,4,0,5.343,0,7v18c0,1.657,1.343,3,3,3h10c0,0.552,0.448,1,1,1h4c0.552,0,1-0.448,1-1h10  c1.657,0,3-1.343,3-3V7C32,5.343,30.657,4,29,4z M29,5v20H18.708c-0.618,0-1.236,0.146-1.789,0.422l-0.419,0.21V5H29z M15.5,5  v20.632l-0.419-0.21C14.528,25.146,13.91,25,13.292,25H3V5H15.5z M31,25c0,1.103-0.897,2-2,2H18v1h-4v-1H3c-1.103,0-2-0.897-2-2V7  c0-0.737,0.405-1.375,1-1.722V25c0,0.552,0.448,1,1,1h10.292c0.466,0,0.925,0.108,1.342,0.317l0.919,0.46  c0.141,0.07,0.294,0.106,0.447,0.106c0.153,0,0.306-0.035,0.447-0.106l0.919-0.46C17.783,26.108,18.242,26,18.708,26H29  c0.552,0,1-0.448,1-1V5.278C30.595,5.625,31,6.263,31,7V25z M6,12.5C6,12.224,6.224,12,6.5,12h5c0.276,0,0.5,0.224,0.5,0.5  S11.776,13,11.5,13h-5C6.224,13,6,12.776,6,12.5z M6,14.5C6,14.224,6.224,14,6.5,14h5c0.276,0,0.5,0.224,0.5,0.5S11.776,15,11.5,15  h-5C6.224,15,6,14.776,6,14.5z M6,16.5C6,16.224,6.224,16,6.5,16h5c0.276,0,0.5,0.224,0.5,0.5S11.776,17,11.5,17h-5  C6.224,17,6,16.776,6,16.5z M20,12.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,13,25.5,13h-5  C20.224,13,20,12.776,20,12.5z M20,14.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,15,25.5,15h-5  C20.224,15,20,14.776,20,14.5z M20,16.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,17,25.5,17h-5  C20.224,17,20,16.776,20,16.5z"></path></symbol><symbol id="icon-tistory" viewBox="0 0 24 24"><path d="M4 4h16v3h-6v13h-4V7H4V4z"/></symbol></defs></svg>

        <header class="bar-header">
    <a id="menu" role="button">
        <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    </a>
    <h1 class="logo">
        <a href="/">
            
                somaz <span class="version">v3.1.2</span>
            
        </a>
    </h1>
    <a id="search" class="dosearch" role="button">
        <svg class="icon-search"><use xlink:href="#icon-search"></use></svg>
    </a>
    
        <a href="https://github.com/thiagorossener/jekflix-template" class="get-theme" role="button">
            Get this theme!
        </a>
    
</header>

<div id="mask" class="overlay"></div>

<aside class="sidebar" id="sidebar">
    <nav id="navigation">
      <h2>Menu</h2>
      <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>

    </nav>
</aside>

<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>



        <section class="post two-columns">
            <article role="article" class="post-content">
                <p class="post-info">
                    
                        <svg class="icon-calendar" id="date"><use xlink:href="#icon-calendar"></use></svg>
                        <time class="date" datetime="2025-11-02T00:00:00+00:00">
                            


November 2, 2025

                        </time>
                    
                    <svg id="clock" class="icon-clock"><use xlink:href="#icon-clock"></use></svg>
                    <span>16 min to read</span>
                </p>
                <h1 class="post-title">Automated Ceph Cluster Deployment with Cephadm-Ansible and Kubernetes Integration</h1>
                <p class="post-subtitle">Complete automation workflow for Ceph deployment using cephadm-ansible and seamless integration with Kubernetes via Ceph-CSI</p>

                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755078890/cephadm-ansible-1_bataxt.png" alt="Featured image" class="post-cover">
                

                <!-- Pagination links -->



                <!-- Add your table of contents here -->


                <p><br /></p>

<hr />

<h2 id="overview">Overview</h2>

<p>In this article, we’ll cover the complete process of installing Ceph clusters in an automated manner using cephadm-ansible and integrating them with Kubernetes environments.</p>

<p>Cephadm is a container-based Ceph management tool, while cephadm-ansible provides separate Ansible automation for repetitive tasks such as initial host setup, cluster management, and purging operations that are not directly handled by cephadm itself.</p>

<p>The installation process involves provisioning VMs with Terraform, building a Kubernetes cluster with Kubespray, then using Cephadm and Ansible scripts to bootstrap the Ceph cluster and configure OSD nodes, MON/MGR nodes.</p>

<p>We’ll also install Ceph-CSI drivers to enable Kubernetes integration with Ceph RBD storage, and configure StorageClass and test Pods to demonstrate the complete workflow.</p>

<p><br /></p>

<hr />

<h2 id="what-is-cephadm">What is Cephadm?</h2>

<p>Cephadm is Ceph’s latest deployment and management tool, introduced starting with the Ceph Octopus release.</p>

<p>It’s designed to simplify deploying, configuring, managing, and scaling Ceph clusters. It can bootstrap a cluster with a single command and deploys Ceph services using container technology.</p>

<p>Cephadm doesn’t rely on external configuration tools like Ansible, Rook, or Salt. However, these external configuration tools can be used to automate tasks not performed by cephadm itself.</p>

<h4 id="related-resources">Related Resources:</h4>
<ul>
  <li><a href="https://github.com/ceph/cephadm-ansible">cephadm-ansible</a></li>
  <li><a href="https://rook.io/docs/rook/v1.10/Getting-Started/intro/">Rook Introduction</a></li>
  <li><a href="https://github.com/ceph/ceph-salt">ceph-salt</a></li>
</ul>

<p><br /></p>

<hr />

<h2 id="what-is-cephadm-ansible">What is Cephadm-ansible?</h2>

<p>Cephadm-ansible is a collection of Ansible playbooks designed to simplify workflows not covered by cephadm itself.</p>

<p>The workflows it handles include:</p>

<ul>
  <li><strong>Preflight</strong>: Initial host setup before bootstrapping the cluster</li>
  <li><strong>Client</strong>: Client host configuration</li>
  <li><strong>Purge</strong>: Ceph cluster removal</li>
</ul>

<p><br /></p>

<hr />

<h2 id="kubernetes-installation">Kubernetes Installation</h2>

<p>Refer to the linked article for Kubernetes installation. <strong>Important note</strong>: Storage nodes must have at least 32GB of memory.</p>

<p><br /></p>

<h3 id="infrastructure-configuration">Infrastructure Configuration</h3>

<h4 id="master-node-control-plane">Master Node (Control Plane)</h4>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>IP</th>
      <th>CPU</th>
      <th>Memory</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>test-server</td>
      <td>10.77.101.47</td>
      <td>16</td>
      <td>32G</td>
    </tr>
  </tbody>
</table>

<h4 id="worker-nodes">Worker Nodes</h4>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>IP</th>
      <th>CPU</th>
      <th>Memory</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>test-server-agent</td>
      <td>10.77.101.43</td>
      <td>16</td>
      <td>32G</td>
    </tr>
    <tr>
      <td>test-server-storage</td>
      <td>10.77.101.48</td>
      <td>16</td>
      <td>32G</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h3 id="terraform-configuration-for-storage-node">Terraform Configuration for Storage Node</h3>

<p>Add the following configuration:</p>

<script src="https://gist.github.com/somaz94/0df945df11ce47d28fd8f19028b78d69.js"></script>

<p><br /></p>

<h3 id="verify-kubernetes-installation">Verify Kubernetes Installation</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get nodes
NAME                STATUS   ROLES           AGE     VERSION
test-server         Ready    control-plane   6m27s   v1.29.1
test-server-agent   Ready    &lt;none&gt;          5m55s   v1.29.1
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="cephadm-ansible-installation">Cephadm-Ansible Installation</h2>

<p><br /></p>

<h3 id="clone-repository-and-setup-environment">Clone Repository and Setup Environment</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/ceph/cephadm-ansible

<span class="nv">VENVDIR</span><span class="o">=</span>cephadm-venv
<span class="nv">CEPAHADMDIR</span><span class="o">=</span>cephadm-ansible
python3.10 <span class="nt">-m</span> venv <span class="nv">$VENVDIR</span>
<span class="nb">source</span> <span class="nv">$VENVDIR</span>/bin/activate
<span class="nb">cd</span> <span class="nv">$CEPAHADMDIR</span>

pip <span class="nb">install</span> <span class="nt">-U</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div>

<p><br /></p>

<h3 id="verify-storage-disks">Verify Storage Disks</h3>

<p>Check disk configuration on the test-server-storage node:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
loop0     7:0    0  63.9M  1 loop /snap/core20/2105
loop1     7:1    0 368.2M  1 loop /snap/google-cloud-cli/207
loop2     7:2    0  40.4M  1 loop /snap/snapd/20671
loop3     7:3    0  91.9M  1 loop /snap/lxd/24061
sda       8:0    0    50G  0 disk
├─sda1    8:1    0  49.9G  0 part /
├─sda14   8:14   0     4M  0 part
└─sda15   8:15   0   106M  0 part /boot/efi
sdb       8:16   0    50G  0 disk
sdc       8:32   0    50G  0 disk
sdd       8:48   0    50G  0 disk
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="configuration-files-setup">Configuration Files Setup</h2>

<p><br /></p>

<h3 id="create-inventory-file">Create Inventory File</h3>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># inventory.ini
</span><span class="nn">[all]</span>
<span class="err">test-server</span> <span class="py">ansible_host</span><span class="p">=</span><span class="s">10.77.101.47</span>
<span class="err">test-server-agent</span> <span class="py">ansible_host</span><span class="p">=</span><span class="s">10.77.101.43</span>
<span class="err">test-server-storage</span> <span class="py">ansible_host</span><span class="p">=</span><span class="s">10.77.101.48</span>

<span class="c"># Ceph Client Nodes (Kubernetes nodes that require access to Ceph storage)
</span><span class="nn">[clients]</span>
<span class="err">test-server</span>
<span class="err">test-server-agent</span>
<span class="err">test-server-storage</span>

<span class="c"># Admin Node (Usually the first monitor node)
</span><span class="nn">[admin]</span>
<span class="err">test-server</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="update-cephadm-distribute-ssh-keyyml">Update cephadm-distribute-ssh-key.yml</h3>

<p>Fix the file attribute check:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Change this line:</span>
<span class="c1"># or not cephadm_pubkey_path_stat.stat.isfile | bool</span>
<span class="c1"># To:</span>
<span class="s">or not cephadm_pubkey_path_stat.stat.isreg | bool</span>
</code></pre></div></div>

<p><br /></p>

<p>Complete corrected file:</p>

<script src="https://gist.github.com/somaz94/3d991bdd4a0c4f5d3e11de67d4686f2b.js"></script>

<p><br /></p>

<h3 id="update-cephadm-preflightyml">Update cephadm-preflight.yml</h3>

<p>Add Ubuntu-specific tasks:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Ubuntu related tasks</span>
  <span class="na">when</span><span class="pi">:</span> <span class="s">ansible_facts['distribution'] == 'Ubuntu'</span>
  <span class="na">block</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Update apt cache</span>
      <span class="na">apt</span><span class="pi">:</span>
        <span class="na">update_cache</span><span class="pi">:</span> <span class="s">yes</span>
        <span class="na">cache_valid_time</span><span class="pi">:</span> <span class="m">3600</span>
      <span class="na">changed_when</span><span class="pi">:</span> <span class="no">false</span>

    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">install container engine</span>
      <span class="na">block</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">install docker</span>
          <span class="c1"># ... existing docker installation tasks</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">ensure Docker is running</span>
          <span class="na">service</span><span class="pi">:</span>
            <span class="na">name</span><span class="pi">:</span> <span class="s">docker</span>
            <span class="na">state</span><span class="pi">:</span> <span class="s">started</span>
            <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>

    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">install jq package</span>
      <span class="na">apt</span><span class="pi">:</span>
       <span class="na">name</span><span class="pi">:</span> <span class="s">jq</span>
       <span class="na">state</span><span class="pi">:</span> <span class="s">present</span>
       <span class="na">update_cache</span><span class="pi">:</span> <span class="s">yes</span>
      <span class="na">register</span><span class="pi">:</span> <span class="s">result</span>
      <span class="na">until</span><span class="pi">:</span> <span class="s">result is succeeded</span>
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="automation-scripts">Automation Scripts</h2>

<p><br /></p>

<h3 id="variables-configuration-ceph_varssh">Variables Configuration (<code class="language-plaintext highlighter-rouge">ceph_vars.sh</code>)</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="c"># Define variables (Modify as needed)</span>
<span class="nv">SSH_KEY</span><span class="o">=</span><span class="s2">"/home/somaz/.ssh/id_rsa_ansible"</span> <span class="c"># SSH KEY Path</span>
<span class="nv">INVENTORY_FILE</span><span class="o">=</span><span class="s2">"inventory.ini"</span> <span class="c"># Inventory Path</span>
<span class="nv">CEPHADM_PREFLIGHT_PLAYBOOK</span><span class="o">=</span><span class="s2">"cephadm-preflight.yml"</span>
<span class="nv">CEPHADM_CLIENTS_PLAYBOOK</span><span class="o">=</span><span class="s2">"cephadm-clients.yml"</span>
<span class="nv">CEPHADM_DISTRIBUTE_SSHKEY_PLAYBOOK</span><span class="o">=</span><span class="s2">"cephadm-distribute-ssh-key.yml"</span>
<span class="nv">HOST_GROUP</span><span class="o">=(</span>test-server test-server-agent test-server-storage<span class="o">)</span> <span class="c"># All host group</span>
<span class="nv">ADMIN_HOST</span><span class="o">=</span><span class="s2">"test-server"</span> <span class="c"># Admin host name</span>
<span class="nv">OSD_HOST</span><span class="o">=</span><span class="s2">"test-server-storage"</span> <span class="c"># OSD host name</span>
<span class="nv">HOST_IPS</span><span class="o">=(</span><span class="s2">"10.77.101.47"</span> <span class="s2">"10.77.101.43"</span> <span class="s2">"10.77.101.48"</span><span class="o">)</span> <span class="c"># Corresponding IPs</span>
<span class="nv">OSD_DEVICES</span><span class="o">=(</span><span class="s2">"sdb"</span> <span class="s2">"sdc"</span> <span class="s2">"sdd"</span><span class="o">)</span> <span class="c"># OSD devices, without /dev/ prefix</span>
<span class="nv">CLUSTER_NETWORK</span><span class="o">=</span><span class="s2">"10.77.101.0/24"</span> <span class="c"># Cluster network CIDR</span>
<span class="nv">SSH_USER</span><span class="o">=</span><span class="s2">"somaz"</span> <span class="c"># SSH user</span>
<span class="nv">CLEANUP_CEPH</span><span class="o">=</span><span class="s2">"false"</span> <span class="c"># Reset based on user input</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="function-library-ceph_functionssh">Function Library (<code class="language-plaintext highlighter-rouge">ceph_functions.sh</code>)</h3>

<script src="https://gist.github.com/somaz94/bc6c8f63ce3202b780e1adc416f192e4.js"></script>

<p><br /></p>

<h3 id="main-setup-script-setup_ceph_clustersh">Main Setup Script (<code class="language-plaintext highlighter-rouge">setup_ceph_cluster.sh</code>)</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="c"># Load functions from ceph_functions.sh and ceph_vars.sh</span>
<span class="nb">source </span>ceph_vars.sh
<span class="nb">source </span>ceph_functions.sh

<span class="nb">read</span> <span class="nt">-p</span> <span class="s2">"Do you want to cleanup existing Ceph cluster? (yes/no): "</span> user_confirmation
<span class="k">if</span> <span class="o">[[</span> <span class="s2">"</span><span class="nv">$user_confirmation</span><span class="s2">"</span> <span class="o">==</span> <span class="s2">"yes"</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then
    </span><span class="nv">CLEANUP_CEPH</span><span class="o">=</span><span class="s2">"true"</span>
<span class="k">else
    </span><span class="nv">CLEANUP_CEPH</span><span class="o">=</span><span class="s2">"false"</span>
<span class="k">fi

</span><span class="nb">echo</span> <span class="s2">"Starting Ceph cluster setup..."</span>

<span class="c"># Check for existing SSH key and generate if it does not exist</span>
<span class="k">if</span> <span class="o">[</span> <span class="o">!</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$SSH_KEY</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">echo</span> <span class="s2">"Generating SSH key..."</span>
    ssh-keygen <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$SSH_KEY</span><span class="s2">"</span> <span class="nt">-N</span> <span class="s1">''</span>
    <span class="nb">echo</span> <span class="s2">"SSH key generated successfully."</span>
<span class="k">else
    </span><span class="nb">echo</span> <span class="s2">"SSH key already exists. Skipping generation."</span>
<span class="k">fi</span>

<span class="c"># Copy SSH key to each host in the group</span>
<span class="k">for </span>host <span class="k">in</span> <span class="s2">"</span><span class="k">${</span><span class="nv">HOST_GROUP</span><span class="p">[@]</span><span class="k">}</span><span class="s2">"</span><span class="p">;</span> <span class="k">do
    </span><span class="nb">echo</span> <span class="s2">"Copying SSH key to </span><span class="nv">$host</span><span class="s2">..."</span>
    ssh-copy-id <span class="nt">-i</span> <span class="s2">"</span><span class="k">${</span><span class="nv">SSH_KEY</span><span class="k">}</span><span class="s2">.pub"</span> <span class="nt">-o</span> <span class="nv">StrictHostKeyChecking</span><span class="o">=</span>no <span class="s2">"</span><span class="nv">$host</span><span class="s2">"</span>
<span class="k">done</span>

<span class="c"># Cleanup existing Ceph setup if confirmed</span>
cleanup_ceph_cluster

<span class="c"># Wipe OSD devices</span>
<span class="nb">echo</span> <span class="s2">"Wiping OSD devices on </span><span class="nv">$OSD_HOST</span><span class="s2">..."</span>
<span class="k">for </span>device <span class="k">in</span> <span class="k">${</span><span class="nv">OSD_DEVICES</span><span class="p">[@]</span><span class="k">}</span><span class="p">;</span> <span class="k">do
    if </span>ssh <span class="nv">$OSD_HOST</span> <span class="s2">"sudo wipefs --all /dev/</span><span class="nv">$device</span><span class="s2">"</span><span class="p">;</span> <span class="k">then
        </span><span class="nb">echo</span> <span class="s2">"Wiped </span><span class="nv">$device</span><span class="s2"> successfully."</span>
    <span class="k">else
        </span><span class="nb">echo</span> <span class="s2">"Failed to wipe </span><span class="nv">$device</span><span class="s2">."</span>
    <span class="k">fi
done</span>

<span class="c"># Run cephadm-ansible preflight playbook</span>
<span class="nb">echo</span> <span class="s2">"Running cephadm-ansible preflight setup..."</span>
run_ansible_playbook <span class="nv">$CEPHADM_PREFLIGHT_PLAYBOOK</span> <span class="s2">""</span>

<span class="c"># Create a temporary Ceph configuration file for initial settings</span>
<span class="nv">TEMP_CONFIG_FILE</span><span class="o">=</span><span class="si">$(</span><span class="nb">mktemp</span><span class="si">)</span>
<span class="nb">echo</span> <span class="s2">"[global]
osd crush chooseleaf type = 0
osd_pool_default_size = 1"</span> <span class="o">&gt;</span> <span class="nv">$TEMP_CONFIG_FILE</span>

<span class="c"># Bootstrap the Ceph cluster</span>
<span class="nv">MON_IP</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">HOST_IPS</span><span class="p">[0]</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"Bootstrapping Ceph cluster with MON_IP: </span><span class="nv">$MON_IP</span><span class="s2">"</span>
add_to_known_hosts <span class="nv">$MON_IP</span>
<span class="nb">sudo </span>cephadm bootstrap <span class="nt">--mon-ip</span> <span class="nv">$MON_IP</span> <span class="nt">--cluster-network</span> <span class="nv">$CLUSTER_NETWORK</span> <span class="nt">--ssh-user</span> <span class="nv">$SSH_USER</span> <span class="nt">-c</span> <span class="nv">$TEMP_CONFIG_FILE</span> <span class="nt">--allow-overwrite</span> <span class="nt">--log-to-file</span>
<span class="nb">rm</span> <span class="nt">-f</span> <span class="nv">$TEMP_CONFIG_FILE</span>

<span class="c"># Distribute Cephadm SSH keys to all hosts</span>
<span class="nb">echo</span> <span class="s2">"Distributing Cephadm SSH keys to all hosts..."</span>
run_ansible_playbook <span class="nv">$CEPHADM_DISTRIBUTE_SSHKEY_PLAYBOOK</span> <span class="s2">"-e cephadm_ssh_user=</span><span class="nv">$SSH_USER</span><span class="s2"> -e admin_node=</span><span class="nv">$ADMIN_HOST</span><span class="s2"> -e cephadm_pubkey_path=</span><span class="nv">$SSH_KEY</span><span class="s2">.pub"</span>

<span class="c"># Fetch FSID of the Ceph cluster</span>
<span class="nv">FSID</span><span class="o">=</span><span class="si">$(</span><span class="nb">sudo </span>ceph fsid<span class="si">)</span>
<span class="nb">echo</span> <span class="s2">"Ceph FSID: </span><span class="nv">$FSID</span><span class="s2">"</span>

<span class="c"># Add and label hosts in the Ceph cluster</span>
add_host_and_label

<span class="c"># Prepare and add OSDs</span>
<span class="nb">sleep </span>60
add_osds_and_wait

<span class="c"># Check Ceph cluster status and OSD creation</span>
check_osd_creation

<span class="nb">echo</span> <span class="s2">"Ceph cluster setup and client configuration completed successfully."</span>
</code></pre></div></div>

<h3 id="execute-the-setup">Execute the Setup</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Make script executable</span>
<span class="nb">chmod</span> +x setup_ceph_cluster.sh
./setup_ceph_cluster.sh
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="verification-and-management">Verification and Management</h2>

<p><br /></p>

<h3 id="ceph-dashboard-access">Ceph Dashboard Access</h3>

<p>After successful installation, you’ll see:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Ceph Dashboard is now available at:
             URL: https://test-server:8443/
            User: admin
        Password: 9m16nzu1h7
</code></pre></div></div>

<p><br /></p>

<h3 id="cluster-status-commands">Cluster Status Commands</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check Ceph hosts</span>
<span class="nb">sudo </span>ceph orch host <span class="nb">ls
</span>test-server          10.77.101.47  _admin
test-server-agent    10.77.101.43
test-server-storage  10.77.101.48  osd
3 hosts <span class="k">in </span>cluster

<span class="c"># Check Ceph status</span>
<span class="nb">sudo </span>ceph <span class="nt">-s</span>
  cluster:
    <span class="nb">id</span>:     43d4ca77-cf91-11ee-8e5d-831aa89df15f
    health: HEALTH_WARN
            1 pool<span class="o">(</span>s<span class="o">)</span> have no replicas configured

  services:
    mon: 1 daemons, quorum test-server <span class="o">(</span>age 7m<span class="o">)</span>
    mgr: test-server.nckhts<span class="o">(</span>active, since 6m<span class="o">)</span>
    osd: 3 osds: 3 up <span class="o">(</span>since 3m<span class="o">)</span>, 3 <span class="k">in</span> <span class="o">(</span>since 3m<span class="o">)</span>

  data:
    pools:   1 pools, 1 pgs
    objects: 2 objects, 577 KiB
    usage:   872 MiB used, 149 GiB / 150 GiB avail
    pgs:     1 active+clean

<span class="c"># Check OSD tree</span>
<span class="nb">sudo </span>ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                     STATUS  REWEIGHT  PRI-AFF
<span class="nt">-1</span>         0.14639  root default
<span class="nt">-3</span>         0.14639      host test-server-storage
 0    ssd  0.04880          osd.0                     up   1.00000  1.00000
 1    ssd  0.04880          osd.1                     up   1.00000  1.00000
 2    ssd  0.04880          osd.2                     up   1.00000  1.00000

<span class="c"># Check service status</span>
<span class="nb">sudo </span>ceph orch <span class="nb">ls</span> <span class="nt">--service-type</span> mon
<span class="nb">sudo </span>ceph orch <span class="nb">ls</span> <span class="nt">--service-type</span> mgr
<span class="nb">sudo </span>ceph orch <span class="nb">ls</span> <span class="nt">--service-type</span> osd
</code></pre></div></div>

<p><br /></p>

<h3 id="pool-management">Pool Management</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check current pools</span>
ceph <span class="nb">df</span>

<span class="c"># Create a new pool</span>
ceph osd pool create kube 128

<span class="c"># Check pool replica settings</span>
ceph osd pool get kube size

<span class="c"># Modify pool settings</span>
ceph osd pool <span class="nb">set </span>kube size 3

<span class="c"># Set placement groups</span>
ceph osd pool <span class="nb">set </span>kube pg_num 256

<span class="c"># Delete pool (if needed)</span>
<span class="c"># ceph osd pool delete {pool-name} {pool-name} --yes-i-really-really-mean-it</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="understanding-placement-groups-pgs">Understanding Placement Groups (PGs)</h3>

<p>Placement Groups (PGs) are logical units for data distribution and management within Ceph clusters. Ceph stores data as objects and assigns these objects to PGs, which are then distributed across various OSDs in the cluster. PGs optimize cluster scalability, performance, and resilience.</p>

<p><br /></p>

<hr />

<h2 id="kubernetes-integration-with-ceph-csi">Kubernetes Integration with Ceph-CSI</h2>

<p><br /></p>

<h3 id="install-ceph-csi-driver">Install Ceph-CSI Driver</h3>

<p>To use Ceph as a storage solution for Kubernetes Pods, install Ceph-CSI (Container Storage Interface). Ceph-CSI enables Ceph as persistent storage for Kubernetes, supporting both block and file storage.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add Helm repository</span>
helm repo add ceph-csi https://ceph.github.io/csi-charts
helm repo update

<span class="c"># Search available charts</span>
helm search repo ceph-csi
NAME                            CHART VERSION   APP VERSION     DESCRIPTION
ceph-csi/ceph-csi-cephfs        3.10.2          3.10.2          Container Storage Interface <span class="o">(</span>CSI<span class="o">)</span> driver, provi...
ceph-csi/ceph-csi-rbd           3.10.2          3.10.2          Container Storage Interface <span class="o">(</span>CSI<span class="o">)</span> driver, provi...

<span class="c"># Install RBD driver</span>
helm <span class="nb">install </span>ceph-csi-rbd ceph-csi/ceph-csi-rbd <span class="nt">--namespace</span> ceph-csi <span class="nt">--create-namespace</span>

<span class="c"># Install CephFS driver (optional)</span>
helm <span class="nb">install </span>ceph-csi-cephfs ceph-csi/ceph-csi-cephfs <span class="nt">--namespace</span> ceph-csi <span class="nt">--create-namespace</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="create-ceph-csi-values-configuration">Create Ceph-CSI Values Configuration</h3>

<p>Since we have one worker node, set replicaCount to 1:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Get Ceph cluster ID</span>
<span class="nb">sudo </span>ceph fsid
afdfd487-cef1-11ee-8e5d-831aa89df15f

<span class="c"># Check monitor endpoint</span>
ss <span class="nt">-nlpt</span> | <span class="nb">grep </span>6789
LISTEN  0        512         10.77.101.47:6789           0.0.0.0:<span class="k">*</span>
</code></pre></div></div>

<p>Create <code class="language-plaintext highlighter-rouge">ceph-csi-values.yaml</code>:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">csiConfig</span><span class="pi">:</span>
<span class="pi">-</span> <span class="na">clusterID</span><span class="pi">:</span> <span class="s2">"</span><span class="s">afdfd487-cef1-11ee-8e5d-831aa89df15f"</span> <span class="c1"># ceph cluster id</span>
  <span class="na">monitors</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">10.77.101.47:6789"</span>
<span class="na">provisioner</span><span class="pi">:</span>
  <span class="na">replicaCount</span><span class="pi">:</span> <span class="m">1</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="install-ceph-csi-driver-with-custom-values">Install Ceph-CSI Driver with Custom Values</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create namespace</span>
kubectl create namespace ceph-csi

<span class="c"># Install with custom values</span>
helm <span class="nb">install</span> <span class="nt">-n</span> ceph-csi ceph-csi ceph-csi/ceph-csi-rbd <span class="nt">-f</span> ceph-csi-values.yaml

<span class="c"># Verify installation</span>
kubectl get all <span class="nt">-n</span> ceph-csi
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="storageclass-configuration">StorageClass Configuration</h2>

<p><br /></p>

<h3 id="create-secret-and-storageclass">Create Secret and StorageClass</h3>

<p>First, get the Ceph authentication information:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ceph auth list
<span class="c"># Look for client.admin key</span>
client.admin
        key: AQC899JlcL6CKBAAQsBOJqWw/CVTQKUD+2FbyQ<span class="o">==</span>
        caps: <span class="o">[</span>mds] allow <span class="k">*</span>
        caps: <span class="o">[</span>mgr] allow <span class="k">*</span>
        caps: <span class="o">[</span>mon] allow <span class="k">*</span>
        caps: <span class="o">[</span>osd] allow <span class="k">*</span>
</code></pre></div></div>

<p>Create the StorageClass configuration:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ceph-csi-storageclass.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Secret</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">csi-rbd-secret</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
<span class="na">stringData</span><span class="pi">:</span>
  <span class="na">userID</span><span class="pi">:</span> <span class="s">admin</span> <span class="c1"># ceph user id (client.admin) - admin is the user</span>
  <span class="na">userKey</span><span class="pi">:</span> <span class="s2">"</span><span class="s">AQC899JlcL6CKBAAQsBOJqWw/CVTQKUD+2FbyQ=="</span> <span class="c1"># client.admin key</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="na">metadata</span><span class="pi">:</span>
   <span class="na">name</span><span class="pi">:</span> <span class="s">rbd</span>
   <span class="na">annotations</span><span class="pi">:</span>
     <span class="na">storageclass.beta.kubernetes.io/is-default-class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">true"</span>
     <span class="na">storageclass.kubesphere.io/supported-access-modes</span><span class="pi">:</span> <span class="s1">'</span><span class="s">["ReadWriteOnce","ReadOnlyMany","ReadWriteMany"]'</span>
<span class="na">provisioner</span><span class="pi">:</span> <span class="s">rbd.csi.ceph.com</span>
<span class="na">parameters</span><span class="pi">:</span>
   <span class="na">clusterID</span><span class="pi">:</span> <span class="s2">"</span><span class="s">afdfd487-cef1-11ee-8e5d-831aa89df15f"</span>
   <span class="na">pool</span><span class="pi">:</span> <span class="s2">"</span><span class="s">kube"</span>
   <span class="na">imageFeatures</span><span class="pi">:</span> <span class="s">layering</span>
   <span class="na">csi.storage.k8s.io/provisioner-secret-name</span><span class="pi">:</span> <span class="s">csi-rbd-secret</span>
   <span class="na">csi.storage.k8s.io/provisioner-secret-namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
   <span class="na">csi.storage.k8s.io/controller-expand-secret-name</span><span class="pi">:</span> <span class="s">csi-rbd-secret</span>
   <span class="na">csi.storage.k8s.io/controller-expand-secret-namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
   <span class="na">csi.storage.k8s.io/node-stage-secret-name</span><span class="pi">:</span> <span class="s">csi-rbd-secret</span>
   <span class="na">csi.storage.k8s.io/node-stage-secret-namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
   <span class="na">csi.storage.k8s.io/fstype</span><span class="pi">:</span> <span class="s">ext4</span>
<span class="na">reclaimPolicy</span><span class="pi">:</span> <span class="s">Delete</span>
<span class="na">allowVolumeExpansion</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">mountOptions</span><span class="pi">:</span>
   <span class="pi">-</span> <span class="s">discard</span>
</code></pre></div></div>

<p>Apply the configuration:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> ceph-csi-storageclass.yaml

<span class="c"># Verify StorageClass</span>
kubectl get storageclass
NAME            PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
rbd <span class="o">(</span>default<span class="o">)</span>   rbd.csi.ceph.com   Delete          Immediate           <span class="nb">true                   </span>21s
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="testing-the-integration">Testing the Integration</h2>

<p><br /></p>

<h3 id="deploy-test-pod-with-persistent-volume">Deploy Test Pod with Persistent Volume</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># test-pod.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ceph-rbd-pvc</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">rbd</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">1Gi</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod-using-ceph-rbd</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">my-container</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/var/lib/www/html"</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">mypd</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">mypd</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">ceph-rbd-pvc</span>
</code></pre></div></div>

<p><br /></p>

<p>Deploy and verify:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Apply test configuration</span>
kubectl apply <span class="nt">-f</span> test-pod.yaml

<span class="c"># Verify resources</span>
kubectl get pod,pv,pvc
NAME                     READY   STATUS    RESTARTS   AGE
pod/pod-using-ceph-rbd   1/1     Running   0          16s

NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE
persistentvolume/pvc-83fd673e-077c-4d24-b9c9-290118586bd3   1Gi        RWO            Delete           Bound    default/ceph-rbd-pvc   rbd                     16s

NAME                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/ceph-rbd-pvc   Bound    pvc-83fd673e-077c-4d24-b9c9-290118586bd3   1Gi        RWO            rbd            16s
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="best-practices-and-recommendations">Best Practices and Recommendations</h2>

<p><br /></p>

<h3 id="infrastructure-as-code-benefits">Infrastructure as Code Benefits</h3>

<ol>
  <li><strong>Automation</strong>: Cephadm-ansible eliminates manual configuration steps</li>
  <li><strong>Repeatability</strong>: Scripts ensure consistent deployments across environments</li>
  <li><strong>Version Control</strong>: Infrastructure configurations can be versioned and tracked</li>
  <li><strong>Disaster Recovery</strong>: Automated procedures enable rapid cluster recovery</li>
</ol>

<p><br /></p>

<h3 id="production-considerations">Production Considerations</h3>

<ol>
  <li><strong>Multi-Node Setup</strong>: Use multiple MON and MGR nodes for high availability</li>
  <li><strong>Storage Replication</strong>: Configure appropriate replication levels for data durability</li>
  <li><strong>Monitoring</strong>: Implement comprehensive monitoring for cluster health</li>
  <li><strong>Backup Strategy</strong>: Establish regular backup procedures for critical data</li>
</ol>

<p><br /></p>

<h3 id="troubleshooting-tips">Troubleshooting Tips</h3>

<ol>
  <li><strong>Log Analysis</strong>: Check cephadm and Ansible logs for deployment issues</li>
  <li><strong>Network Connectivity</strong>: Ensure proper network configuration between nodes</li>
  <li><strong>Disk Preparation</strong>: Verify disk wiping and LVM cleanup before deployment</li>
  <li><strong>Service Dependencies</strong>: Ensure container runtime is properly configured</li>
</ol>

<p><br /></p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>Cephadm-ansible provides a powerful automation framework for Ceph cluster deployment, significantly simplifying the complex process of distributed storage setup. The integration with Kubernetes through Ceph-CSI creates a robust storage solution for cloud-native applications.</p>

<p><br /></p>

<h3 id="key-achievements">Key Achievements:</h3>

<ol>
  <li><strong>Automated Deployment</strong>: Complete Ceph cluster automation using scripts</li>
  <li><strong>Infrastructure as Code</strong>: Terraform-based VM provisioning</li>
  <li><strong>Kubernetes Integration</strong>: Seamless storage integration via Ceph-CSI</li>
  <li><strong>Production Ready</strong>: Scalable architecture with monitoring capabilities</li>
</ol>

<p><br /></p>

<h3 id="operational-benefits">Operational Benefits:</h3>

<ul>
  <li><strong>Reduced Manual Work</strong>: Automated host setup, SSH key distribution, OSD disk cleanup</li>
  <li><strong>Cluster Management</strong>: Automated purging and labeling operations</li>
  <li><strong>Storage Integration</strong>: Native Kubernetes persistent storage support</li>
  <li><strong>Scalability</strong>: Foundation for enterprise-grade storage automation</li>
</ul>

<h4 id="future-enhancements">Future Enhancements:</h4>
<ul>
  <li>Implement multi-cluster federation</li>
  <li>Add automated backup and disaster recovery</li>
  <li>Integrate with CI/CD pipelines for infrastructure updates</li>
  <li>Develop custom monitoring and alerting solutions</li>
</ul>

<p>Learning to configure clusters directly through Cephadm and Ansible integration, understanding OSD configuration flows, and storage class integration provides invaluable experience for enterprise-level storage operation automation.</p>

<blockquote>
  <p><em>“Mastering automation tools like cephadm-ansible is essential for building reliable, scalable infrastructure in modern cloud environments.”</em></p>
</blockquote>

<p><br /></p>

<hr />

<h2 id="references">References</h2>

<ul>
  <li><a href="https://github.com/ceph/cephadm-ansible">Cephadm-ansible GitHub Repository</a></li>
  <li><a href="https://docs.ceph.com/en/latest/cephadm/">Cephadm Official Documentation</a></li>
  <li><a href="https://github.com/ceph/ceph-csi">Ceph-CSI Documentation</a></li>
  <li><a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">Kubernetes Storage Classes</a></li>
  <li><a href="https://docs.ansible.com/">Ansible Automation Platform</a></li>
  <li><a href="https://www.terraform.io/docs/">Terraform Infrastructure as Code</a></li>
</ul>


                <!-- Pagination links -->


            </article>

            
                <aside class="see-also">
                    <h2>See also</h2>
                    <ul>
                        
                        
                        
                            <li>
                                <a href="/category/data-engineering/oltp-olap/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755575326/oltp-olap_gqmvnx.png">
                                    
                                    <h3>Large-Scale Data Processing and Data Architecture Design</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/virtualization/kvm-nested/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755588581/kvm-nested_efue7m.png">
                                    
                                    <h3>KVM Nested Virtualization Complete Guide</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/data-engineering/data-flow-etl/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755575394/data-etl-1_sjgszi.png">
                                    
                                    <h3>Data ETL Pipeline Components and Architecture Guide</h3>
                                </a>
                            </li>
                        
                    </ul>
                </aside>
            

        </section>

        <!-- Add time bar only for pages without pagination -->
        
            <div class="time-bar" data-minutes="16">
    <span class="time-completed"></span>
    <span class="time-remaining"></span>
    <div class="bar">
        <span class="completed" style="width:0%;"></span>
        <span class="remaining" style="width:100%;"></span>
    </div>
</div>

            <button class="toggle-preview" onclick="togglePreview()">
    <span>Hide Preview ▼</span>
</button>

<div id="recommendationSection" class="recommendation">
    <div class="message">
        <strong>Why don't you read something next?</strong>
        <div>
            <button>
                <svg><use xlink:href="#icon-arrow-right"></use></svg>
                <span>Go back to top</span>
            </button>
        </div>
    </div>
    <div id="previewSection" class="preview-section">
        
        <a href="/category/kubernetes/kubernetes-redis-redlock-in-kubernetes/" class="post-preview">
            <div class="image">
                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1754990727/redlock-redis_hkxaio.png">
                
            </div>
            <h3 class="title">Implementing Redis Redlock on Kubernetes — A Complete Guide to Distributed Locks</h3>
        </a>
    </div>
</div>

<style>
.toggle-preview {
    position: fixed;
    bottom: 20px;
    right: 20px;
    background: #333;
    color: white;
    border: none;
    padding: 8px 15px;
    border-radius: 4px;
    cursor: pointer;
    z-index: 1000;
    opacity: 0;
    transition: opacity 0.3s ease;
}

.toggle-preview:hover {
    background: #444;
}

.toggle-preview.visible {
    opacity: 1;
}

.recommendation {
    margin-top: 1000px;
    display: block;
    transition: all 0.3s ease;
}

.recommendation.hidden {
    display: none;
}

.hide-preview {
    margin-left: 10px;
    background: none;
    border: 1px solid #666;
    color: #666;
    padding: 5px 10px;
    border-radius: 4px;
    cursor: pointer;
}

.hide-preview:hover {
    background: #f0f0f0;
}

.preview-section {
    max-height: 1000px;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
}

.preview-section.hidden {
    max-height: 0;
}
</style>

<script>
function togglePreview() {
    const recommendation = document.getElementById('recommendationSection');
    const button = document.querySelector('.toggle-preview span');
    
    if (recommendation.classList.contains('hidden')) {
        recommendation.classList.remove('hidden');
        button.textContent = 'Hide Preview ▼';
    } else {
        recommendation.classList.add('hidden');
        button.textContent = 'Show Preview ▲';
    }
}

window.addEventListener('scroll', function() {
    const toggleButton = document.querySelector('.toggle-preview');
    const recommendation = document.getElementById('recommendationSection');
    const rect = recommendation.getBoundingClientRect();
    
    if (rect.top <= window.innerHeight) {
        toggleButton.classList.add('visible');
    } else {
        toggleButton.classList.remove('visible');
    }
});
</script>

        

        <!-- Show modal if the post is the last one -->
        

        <!-- Show modal before user leaves the page -->
        

        <!-- Add your newsletter subscription form here -->

        <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;Step-by-step guide to deploying Ceph clusters using cephadm-ansible for automated host setup, cluster management, and purging, followed by Kubernetes integration through Ceph-CSI drivers and StorageClass configuration.&quot;%20https://somaz.blog/category/iac/automated-ceph-deployment-cephadm-ansible-kubernetes/%20via%20&#64;twitter_username&hashtags=cephadm-ansible,ceph,ansible,kubernetes,ceph-csi,infrastructure-as-code,automation,storage,terraform,kubespray"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://somaz.blog/category/iac/automated-ceph-deployment-cephadm-ansible-kubernetes/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

        

  <section class="author">
    <div class="details">
      
        <img class="img-rounded" src="/assets/img/uploads/profile.png" alt="Somaz">
      
      <p class="def">Author</p>
      <h3 class="name">
        <a href="/authors/somaz/">Somaz</a>
      </h3>
      <p class="desc">DevOps engineer focused on cloud infrastructure and automation</p>
      <p>
        
          <a href="https://github.com/somaz94" title="Github">
            <svg><use xlink:href="#icon-github"></use></svg>
          </a>
        
        
        
        
        
        
          <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
            <svg><use xlink:href="#icon-linkedin"></use></svg>
          </a>
        
        
          <a href="https://somaz.tistory.com" title="Tistory">
            <svg><use xlink:href="#icon-tistory"></use></svg>
          </a>
        
      </p>
    </div>
  </section>

  
  
  
  
  
  
  
  

  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Somaz",
      
      "image": "/assets/img/uploads/profile.png",
      
      "jobTitle": "DevOps Engineer",
      "url": "https://somaz.blog/authors/somaz/",
      "sameAs": [
        "https://github.com/somaz94","https://www.linkedin.com/in/somaz","https://{{ author.tistory_username }}.tistory.com"
      ]
  }
  </script>


        

<section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'https-somaz94-github-io';
        var disqus_title = '';
        var disqus_url = '/category/iac/automated-ceph-deployment-cephadm-ansible-kubernetes/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>



        <footer>
    <p>
      
        <a href="https://github.com/somaz94" title="Github">
          <svg><use xlink:href="#icon-github"></use></svg>
        </a>
      
      
        <a href="https://www.facebook.com/facebook_username" title="Facebook">
          <svg><use xlink:href="#icon-facebook"></use></svg>
        </a>
      
      
        <a href="https://twitter.com/twitter_username" title="Twitter">
          <svg><use xlink:href="#icon-twitter"></use></svg>
        </a>
      
      
        <a href="https://medium.com/@medium_username" title="Medium">
          <svg><use xlink:href="#icon-medium"></use></svg>
        </a>
      
      
        <a href="https://www.instagram.com/instagram_username" title="Instagram">
          <svg><use xlink:href="#icon-instagram"></use></svg>
        </a>
      
      
        <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
          <svg><use xlink:href="#icon-linkedin"></use></svg>
        </a>
      
      
        <a href="https://somaz.tistory.com" title="Tistory">
          <svg><use xlink:href="#icon-tistory"></use></svg>
        </a>
      
    </p>

    <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>


    <p>
      <a href="https://somaz.blog/sitemap.xml" title="sitemap">Sitemap</a> |
      <a href="https://somaz.blog/privacy-policy" title="Privacy Policy">Privacy Policy</a>
    </p>

    <p>
      <span>Somaz Tech Blog</span> <svg class="love"><use xlink:href="#icon-heart"></use></svg>
    </p>
</footer>










<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "somaz",
  "description": "DevOps engineer's tech blog.",
  "url": "https://somaz.blog/",
  "logo": {
      "@type": "ImageObject",
      "url": "https://somaz.blog/assets/img/icons/mediumtile.png",
      "width": "600",
      "height": "315"
  },
  "sameAs": [
    "https://github.com/somaz94","https://www.facebook.com/facebook_username","https://twitter.com/twitter_username","https://medium.com/@medium_username","https://www.instagram.com/instagram_username","https://www.linkedin.com/in/somaz","https://{{ site.tistory_username }}.tistory.com"
  ]
}
</script>

<!-- Include the script that allows Netlify CMS login -->
<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>

<!-- Include the website scripts -->
<script src="/assets/js/scripts.min.js"></script>

<!-- Include Google Analytics script -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  var host = window.location.hostname;
  if (host != 'localhost') {
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-XXXXXXXX-X');
  }
</script>
  


<!-- Include extra scripts -->



        

        
        
        
        
        
        
        
        
        <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "BlogPosting",
            "name": "Automated Ceph Cluster Deployment with Cephadm-Ansible and Kubernetes Integration",
            "headline": "Complete automation workflow for Ceph deployment using cephadm-ansible and seamless integration with Kubernetes via Ceph-CSI",
            "description": "Step-by-step guide to deploying Ceph clusters using cephadm-ansible for automated host setup, cluster management, and purging, followed by Kubernetes integration through Ceph-CSI drivers and StorageClass configuration.",
            "image": "https://res.cloudinary.com/dkcm26aem/image/upload/v1755078890/cephadm-ansible-1_bataxt.png",
            "url": "https://somaz.blog/category/iac/automated-ceph-deployment-cephadm-ansible-kubernetes/",
            "articleBody": "



Overview

In this article, we’ll cover the complete process of installing Ceph clusters in an automated manner using cephadm-ansible and integrating them with Kubernetes environments.

Cephadm is a container-based Ceph management tool, while cephadm-ansible provides separate Ansible automation for repetitive tasks such as initial host setup, cluster management, and purging operations that are not directly handled by cephadm itself.

The installation process involves provisioning VMs with Terraform, building a Kubernetes cluster with Kubespray, then using Cephadm and Ansible scripts to bootstrap the Ceph cluster and configure OSD nodes, MON/MGR nodes.

We’ll also install Ceph-CSI drivers to enable Kubernetes integration with Ceph RBD storage, and configure StorageClass and test Pods to demonstrate the complete workflow.





What is Cephadm?

Cephadm is Ceph’s latest deployment and management tool, introduced starting with the Ceph Octopus release.

It’s designed to simplify deploying, configuring, managing, and scaling Ceph clusters. It can bootstrap a cluster with a single command and deploys Ceph services using container technology.

Cephadm doesn’t rely on external configuration tools like Ansible, Rook, or Salt. However, these external configuration tools can be used to automate tasks not performed by cephadm itself.

Related Resources:

  cephadm-ansible
  Rook Introduction
  ceph-salt






What is Cephadm-ansible?

Cephadm-ansible is a collection of Ansible playbooks designed to simplify workflows not covered by cephadm itself.

The workflows it handles include:


  Preflight: Initial host setup before bootstrapping the cluster
  Client: Client host configuration
  Purge: Ceph cluster removal






Kubernetes Installation

Refer to the linked article for Kubernetes installation. Important note: Storage nodes must have at least 32GB of memory.



Infrastructure Configuration

Master Node (Control Plane)


  
    
      Component
      IP
      CPU
      Memory
    
  
  
    
      test-server
      10.77.101.47
      16
      32G
    
  


Worker Nodes


  
    
      Component
      IP
      CPU
      Memory
    
  
  
    
      test-server-agent
      10.77.101.43
      16
      32G
    
    
      test-server-storage
      10.77.101.48
      16
      32G
    
  




Terraform Configuration for Storage Node

Add the following configuration:





Verify Kubernetes Installation

kubectl get nodes
NAME                STATUS   ROLES           AGE     VERSION
test-server         Ready    control-plane   6m27s   v1.29.1
test-server-agent   Ready    &amp;lt;none&amp;gt;          5m55s   v1.29.1






Cephadm-Ansible Installation



Clone Repository and Setup Environment

git clone https://github.com/ceph/cephadm-ansible

VENVDIR=cephadm-venv
CEPAHADMDIR=cephadm-ansible
python3.10 -m venv $VENVDIR
source $VENVDIR/bin/activate
cd $CEPAHADMDIR

pip install -U -r requirements.txt




Verify Storage Disks

Check disk configuration on the test-server-storage node:

lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
loop0     7:0    0  63.9M  1 loop /snap/core20/2105
loop1     7:1    0 368.2M  1 loop /snap/google-cloud-cli/207
loop2     7:2    0  40.4M  1 loop /snap/snapd/20671
loop3     7:3    0  91.9M  1 loop /snap/lxd/24061
sda       8:0    0    50G  0 disk
├─sda1    8:1    0  49.9G  0 part /
├─sda14   8:14   0     4M  0 part
└─sda15   8:15   0   106M  0 part /boot/efi
sdb       8:16   0    50G  0 disk
sdc       8:32   0    50G  0 disk
sdd       8:48   0    50G  0 disk






Configuration Files Setup



Create Inventory File

# inventory.ini
[all]
test-server ansible_host=10.77.101.47
test-server-agent ansible_host=10.77.101.43
test-server-storage ansible_host=10.77.101.48

# Ceph Client Nodes (Kubernetes nodes that require access to Ceph storage)
[clients]
test-server
test-server-agent
test-server-storage

# Admin Node (Usually the first monitor node)
[admin]
test-server




Update cephadm-distribute-ssh-key.yml

Fix the file attribute check:

# Change this line:
# or not cephadm_pubkey_path_stat.stat.isfile | bool
# To:
or not cephadm_pubkey_path_stat.stat.isreg | bool




Complete corrected file:





Update cephadm-preflight.yml

Add Ubuntu-specific tasks:

- name: Ubuntu related tasks
  when: ansible_facts[&apos;distribution&apos;] == &apos;Ubuntu&apos;
  block:
    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600
      changed_when: false

    - name: install container engine
      block:
        - name: install docker
          # ... existing docker installation tasks
        - name: ensure Docker is running
          service:
            name: docker
            state: started
            enabled: true

    - name: install jq package
      apt:
       name: jq
       state: present
       update_cache: yes
      register: result
      until: result is succeeded






Automation Scripts



Variables Configuration (ceph_vars.sh)

#!/bin/bash

# Define variables (Modify as needed)
SSH_KEY=&quot;/home/somaz/.ssh/id_rsa_ansible&quot; # SSH KEY Path
INVENTORY_FILE=&quot;inventory.ini&quot; # Inventory Path
CEPHADM_PREFLIGHT_PLAYBOOK=&quot;cephadm-preflight.yml&quot;
CEPHADM_CLIENTS_PLAYBOOK=&quot;cephadm-clients.yml&quot;
CEPHADM_DISTRIBUTE_SSHKEY_PLAYBOOK=&quot;cephadm-distribute-ssh-key.yml&quot;
HOST_GROUP=(test-server test-server-agent test-server-storage) # All host group
ADMIN_HOST=&quot;test-server&quot; # Admin host name
OSD_HOST=&quot;test-server-storage&quot; # OSD host name
HOST_IPS=(&quot;10.77.101.47&quot; &quot;10.77.101.43&quot; &quot;10.77.101.48&quot;) # Corresponding IPs
OSD_DEVICES=(&quot;sdb&quot; &quot;sdc&quot; &quot;sdd&quot;) # OSD devices, without /dev/ prefix
CLUSTER_NETWORK=&quot;10.77.101.0/24&quot; # Cluster network CIDR
SSH_USER=&quot;somaz&quot; # SSH user
CLEANUP_CEPH=&quot;false&quot; # Reset based on user input




Function Library (ceph_functions.sh)





Main Setup Script (setup_ceph_cluster.sh)

#!/bin/bash

# Load functions from ceph_functions.sh and ceph_vars.sh
source ceph_vars.sh
source ceph_functions.sh

read -p &quot;Do you want to cleanup existing Ceph cluster? (yes/no): &quot; user_confirmation
if [[ &quot;$user_confirmation&quot; == &quot;yes&quot; ]]; then
    CLEANUP_CEPH=&quot;true&quot;
else
    CLEANUP_CEPH=&quot;false&quot;
fi

echo &quot;Starting Ceph cluster setup...&quot;

# Check for existing SSH key and generate if it does not exist
if [ ! -f &quot;$SSH_KEY&quot; ]; then
    echo &quot;Generating SSH key...&quot;
    ssh-keygen -f &quot;$SSH_KEY&quot; -N &apos;&apos;
    echo &quot;SSH key generated successfully.&quot;
else
    echo &quot;SSH key already exists. Skipping generation.&quot;
fi

# Copy SSH key to each host in the group
for host in &quot;${HOST_GROUP[@]}&quot;; do
    echo &quot;Copying SSH key to $host...&quot;
    ssh-copy-id -i &quot;${SSH_KEY}.pub&quot; -o StrictHostKeyChecking=no &quot;$host&quot;
done

# Cleanup existing Ceph setup if confirmed
cleanup_ceph_cluster

# Wipe OSD devices
echo &quot;Wiping OSD devices on $OSD_HOST...&quot;
for device in ${OSD_DEVICES[@]}; do
    if ssh $OSD_HOST &quot;sudo wipefs --all /dev/$device&quot;; then
        echo &quot;Wiped $device successfully.&quot;
    else
        echo &quot;Failed to wipe $device.&quot;
    fi
done

# Run cephadm-ansible preflight playbook
echo &quot;Running cephadm-ansible preflight setup...&quot;
run_ansible_playbook $CEPHADM_PREFLIGHT_PLAYBOOK &quot;&quot;

# Create a temporary Ceph configuration file for initial settings
TEMP_CONFIG_FILE=$(mktemp)
echo &quot;[global]
osd crush chooseleaf type = 0
osd_pool_default_size = 1&quot; &amp;gt; $TEMP_CONFIG_FILE

# Bootstrap the Ceph cluster
MON_IP=&quot;${HOST_IPS[0]}&quot;
echo &quot;Bootstrapping Ceph cluster with MON_IP: $MON_IP&quot;
add_to_known_hosts $MON_IP
sudo cephadm bootstrap --mon-ip $MON_IP --cluster-network $CLUSTER_NETWORK --ssh-user $SSH_USER -c $TEMP_CONFIG_FILE --allow-overwrite --log-to-file
rm -f $TEMP_CONFIG_FILE

# Distribute Cephadm SSH keys to all hosts
echo &quot;Distributing Cephadm SSH keys to all hosts...&quot;
run_ansible_playbook $CEPHADM_DISTRIBUTE_SSHKEY_PLAYBOOK &quot;-e cephadm_ssh_user=$SSH_USER -e admin_node=$ADMIN_HOST -e cephadm_pubkey_path=$SSH_KEY.pub&quot;

# Fetch FSID of the Ceph cluster
FSID=$(sudo ceph fsid)
echo &quot;Ceph FSID: $FSID&quot;

# Add and label hosts in the Ceph cluster
add_host_and_label

# Prepare and add OSDs
sleep 60
add_osds_and_wait

# Check Ceph cluster status and OSD creation
check_osd_creation

echo &quot;Ceph cluster setup and client configuration completed successfully.&quot;


Execute the Setup

# Make script executable
chmod +x setup_ceph_cluster.sh
./setup_ceph_cluster.sh






Verification and Management



Ceph Dashboard Access

After successful installation, you’ll see:

Ceph Dashboard is now available at:
             URL: https://test-server:8443/
            User: admin
        Password: 9m16nzu1h7




Cluster Status Commands

# Check Ceph hosts
sudo ceph orch host ls
test-server          10.77.101.47  _admin
test-server-agent    10.77.101.43
test-server-storage  10.77.101.48  osd
3 hosts in cluster

# Check Ceph status
sudo ceph -s
  cluster:
    id:     43d4ca77-cf91-11ee-8e5d-831aa89df15f
    health: HEALTH_WARN
            1 pool(s) have no replicas configured

  services:
    mon: 1 daemons, quorum test-server (age 7m)
    mgr: test-server.nckhts(active, since 6m)
    osd: 3 osds: 3 up (since 3m), 3 in (since 3m)

  data:
    pools:   1 pools, 1 pgs
    objects: 2 objects, 577 KiB
    usage:   872 MiB used, 149 GiB / 150 GiB avail
    pgs:     1 active+clean

# Check OSD tree
sudo ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                     STATUS  REWEIGHT  PRI-AFF
-1         0.14639  root default
-3         0.14639      host test-server-storage
 0    ssd  0.04880          osd.0                     up   1.00000  1.00000
 1    ssd  0.04880          osd.1                     up   1.00000  1.00000
 2    ssd  0.04880          osd.2                     up   1.00000  1.00000

# Check service status
sudo ceph orch ls --service-type mon
sudo ceph orch ls --service-type mgr
sudo ceph orch ls --service-type osd




Pool Management

# Check current pools
ceph df

# Create a new pool
ceph osd pool create kube 128

# Check pool replica settings
ceph osd pool get kube size

# Modify pool settings
ceph osd pool set kube size 3

# Set placement groups
ceph osd pool set kube pg_num 256

# Delete pool (if needed)
# ceph osd pool delete {pool-name} {pool-name} --yes-i-really-really-mean-it




Understanding Placement Groups (PGs)

Placement Groups (PGs) are logical units for data distribution and management within Ceph clusters. Ceph stores data as objects and assigns these objects to PGs, which are then distributed across various OSDs in the cluster. PGs optimize cluster scalability, performance, and resilience.





Kubernetes Integration with Ceph-CSI



Install Ceph-CSI Driver

To use Ceph as a storage solution for Kubernetes Pods, install Ceph-CSI (Container Storage Interface). Ceph-CSI enables Ceph as persistent storage for Kubernetes, supporting both block and file storage.

# Add Helm repository
helm repo add ceph-csi https://ceph.github.io/csi-charts
helm repo update

# Search available charts
helm search repo ceph-csi
NAME                            CHART VERSION   APP VERSION     DESCRIPTION
ceph-csi/ceph-csi-cephfs        3.10.2          3.10.2          Container Storage Interface (CSI) driver, provi...
ceph-csi/ceph-csi-rbd           3.10.2          3.10.2          Container Storage Interface (CSI) driver, provi...

# Install RBD driver
helm install ceph-csi-rbd ceph-csi/ceph-csi-rbd --namespace ceph-csi --create-namespace

# Install CephFS driver (optional)
helm install ceph-csi-cephfs ceph-csi/ceph-csi-cephfs --namespace ceph-csi --create-namespace




Create Ceph-CSI Values Configuration

Since we have one worker node, set replicaCount to 1:

# Get Ceph cluster ID
sudo ceph fsid
afdfd487-cef1-11ee-8e5d-831aa89df15f

# Check monitor endpoint
ss -nlpt | grep 6789
LISTEN  0        512         10.77.101.47:6789           0.0.0.0:*


Create ceph-csi-values.yaml:

csiConfig:
- clusterID: &quot;afdfd487-cef1-11ee-8e5d-831aa89df15f&quot; # ceph cluster id
  monitors:
  - &quot;10.77.101.47:6789&quot;
provisioner:
  replicaCount: 1




Install Ceph-CSI Driver with Custom Values

# Create namespace
kubectl create namespace ceph-csi

# Install with custom values
helm install -n ceph-csi ceph-csi ceph-csi/ceph-csi-rbd -f ceph-csi-values.yaml

# Verify installation
kubectl get all -n ceph-csi






StorageClass Configuration



Create Secret and StorageClass

First, get the Ceph authentication information:

sudo ceph auth list
# Look for client.admin key
client.admin
        key: AQC899JlcL6CKBAAQsBOJqWw/CVTQKUD+2FbyQ==
        caps: [mds] allow *
        caps: [mgr] allow *
        caps: [mon] allow *
        caps: [osd] allow *


Create the StorageClass configuration:

# ceph-csi-storageclass.yaml
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: kube-system
stringData:
  userID: admin # ceph user id (client.admin) - admin is the user
  userKey: &quot;AQC899JlcL6CKBAAQsBOJqWw/CVTQKUD+2FbyQ==&quot; # client.admin key
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rbd
   annotations:
     storageclass.beta.kubernetes.io/is-default-class: &quot;true&quot;
     storageclass.kubesphere.io/supported-access-modes: &apos;[&quot;ReadWriteOnce&quot;,&quot;ReadOnlyMany&quot;,&quot;ReadWriteMany&quot;]&apos;
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: &quot;afdfd487-cef1-11ee-8e5d-831aa89df15f&quot;
   pool: &quot;kube&quot;
   imageFeatures: layering
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: kube-system
   csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
   csi.storage.k8s.io/controller-expand-secret-namespace: kube-system
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: kube-system
   csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
   - discard


Apply the configuration:

kubectl apply -f ceph-csi-storageclass.yaml

# Verify StorageClass
kubectl get storageclass
NAME            PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
rbd (default)   rbd.csi.ceph.com   Delete          Immediate           true                   21s






Testing the Integration



Deploy Test Pod with Persistent Volume

# test-pod.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ceph-rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: rbd
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-ceph-rbd
spec:
  containers:
  - name: my-container
    image: nginx
    volumeMounts:
    - mountPath: &quot;/var/lib/www/html&quot;
      name: mypd
  volumes:
  - name: mypd
    persistentVolumeClaim:
      claimName: ceph-rbd-pvc




Deploy and verify:

# Apply test configuration
kubectl apply -f test-pod.yaml

# Verify resources
kubectl get pod,pv,pvc
NAME                     READY   STATUS    RESTARTS   AGE
pod/pod-using-ceph-rbd   1/1     Running   0          16s

NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS   REASON   AGE
persistentvolume/pvc-83fd673e-077c-4d24-b9c9-290118586bd3   1Gi        RWO            Delete           Bound    default/ceph-rbd-pvc   rbd                     16s

NAME                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/ceph-rbd-pvc   Bound    pvc-83fd673e-077c-4d24-b9c9-290118586bd3   1Gi        RWO            rbd            16s






Best Practices and Recommendations



Infrastructure as Code Benefits


  Automation: Cephadm-ansible eliminates manual configuration steps
  Repeatability: Scripts ensure consistent deployments across environments
  Version Control: Infrastructure configurations can be versioned and tracked
  Disaster Recovery: Automated procedures enable rapid cluster recovery




Production Considerations


  Multi-Node Setup: Use multiple MON and MGR nodes for high availability
  Storage Replication: Configure appropriate replication levels for data durability
  Monitoring: Implement comprehensive monitoring for cluster health
  Backup Strategy: Establish regular backup procedures for critical data




Troubleshooting Tips


  Log Analysis: Check cephadm and Ansible logs for deployment issues
  Network Connectivity: Ensure proper network configuration between nodes
  Disk Preparation: Verify disk wiping and LVM cleanup before deployment
  Service Dependencies: Ensure container runtime is properly configured






Conclusion

Cephadm-ansible provides a powerful automation framework for Ceph cluster deployment, significantly simplifying the complex process of distributed storage setup. The integration with Kubernetes through Ceph-CSI creates a robust storage solution for cloud-native applications.



Key Achievements:


  Automated Deployment: Complete Ceph cluster automation using scripts
  Infrastructure as Code: Terraform-based VM provisioning
  Kubernetes Integration: Seamless storage integration via Ceph-CSI
  Production Ready: Scalable architecture with monitoring capabilities




Operational Benefits:


  Reduced Manual Work: Automated host setup, SSH key distribution, OSD disk cleanup
  Cluster Management: Automated purging and labeling operations
  Storage Integration: Native Kubernetes persistent storage support
  Scalability: Foundation for enterprise-grade storage automation


Future Enhancements:

  Implement multi-cluster federation
  Add automated backup and disaster recovery
  Integrate with CI/CD pipelines for infrastructure updates
  Develop custom monitoring and alerting solutions


Learning to configure clusters directly through Cephadm and Ansible integration, understanding OSD configuration flows, and storage class integration provides invaluable experience for enterprise-level storage operation automation.


  “Mastering automation tools like cephadm-ansible is essential for building reliable, scalable infrastructure in modern cloud environments.”






References


  Cephadm-ansible GitHub Repository
  Cephadm Official Documentation
  Ceph-CSI Documentation
  Kubernetes Storage Classes
  Ansible Automation Platform
  Terraform Infrastructure as Code

",
            "wordcount": "2939",
            "inLanguage": "en",
            "dateCreated": "2025-11-02/",
            "datePublished": "2025-11-02/",
            "dateModified": "2025-11-02/",
            "author": {
                "@type": "Person",
                "name": "Somaz",
                
                "image": "/assets/img/uploads/profile.png",
                
                "jobTitle": "DevOps Engineer",
                "url": "https://somaz.blog/authors/somaz/",
                "sameAs": [
                    "https://github.com/somaz94","https://www.linkedin.com/in/somaz"
                ]
            },
            "publisher": {
                "@type": "Organization",
                "name": "somaz",
                "url": "https://somaz.blog/",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://somaz.blog/assets/img/blog-image.png",
                    "width": "600",
                    "height": "315"
                }
            },
            "mainEntityOfPage": "True",
            "genre": "IAC",
            "articleSection": "IAC",
            "keywords": ["cephadm-ansible","ceph","ansible","kubernetes","ceph-csi","infrastructure-as-code","automation","storage","terraform","kubespray"]
        }
        </script>
    </body>
</html>
