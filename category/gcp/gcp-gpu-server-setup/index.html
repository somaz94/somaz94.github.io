<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    

    
    

    
    

    
    

    <!-- ✅ Google Tag Manager 추가 -->
    <script>
        (function(w,d,s,l,i){
            w[l]=w[l]||[];
            w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});
            var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';
            j.async=true;
            j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;
            f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer','GTM-MBP83N4Q');
    </script>
      <!-- ✅ End Google Tag Manager -->

    <!-- Mermaid.js 직접 로드 -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>

    <title>Setting up GPU Server on Google Cloud Platform | somaz</title>
    <meta name="description" content="A comprehensive guide to setting up and configuring NVIDIA A100 GPU servers on GCP">
    
        <meta name="keywords" content="gcp, gpu, nvidia, cloud-computing, machine-learning, a100">
    

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Setting up GPU Server on Google Cloud Platform | somaz">
    <meta name="twitter:description" content="A comprehensive guide to setting up and configuring NVIDIA A100 GPU servers on GCP">

    
        <meta property="twitter:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1737358848/gpu_keixqz.png">
    
    
    
        <meta name="twitter:site" content="@twitter_username">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="https://somaz.blog/category/gcp/gcp-gpu-server-setup/">
    <meta property="og:title" content="Setting up GPU Server on Google Cloud Platform | somaz">
    <meta property="og:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1737358848/gpu_keixqz.png">
    <meta property="og:description" content="A comprehensive guide to setting up and configuring NVIDIA A100 GPU servers on GCP">
    <meta property="og:site_name" content="Somaz Tech Blog">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="somaz">
    <meta name="msapplication-TileColor" content="#141414">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#141414">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:300,400,700" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="canonical" href="https://somaz.blog/category/gcp/gcp-gpu-server-setup/">
    <link rel="alternate" type="application/rss+xml" title="Somaz Tech Blog" href="https://somaz.blog/feed.xml" />

    <!-- Include extra styles -->
    

    <!-- JavaScript enabled/disabled -->
    <script>
        document.querySelector('html').classList.remove('no-js');
    </script>

    <!-- Google Adsense -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8725590811736154"
        crossorigin="anonymous"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet"> -->
    <!-- <link href="https://cdn.jsdelivr.net/gh/sunn-us/SUIT/fonts/variable/woff2/SUIT-Variable.css" rel="stylesheet"> -->
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css2?family=Albert+Sans:wght@400;500;700&display=swap" rel="stylesheet"> -->

    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml" />

</head>
<!-- ✅ Google Tag Manager (noscript) -->
<noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MBP83N4Q"
            height="0" width="0" style="display:none;visibility:hidden">
    </iframe>
</noscript>
<!-- ✅ End Google Tag Manager (noscript) -->
    <body class="has-push-menu">
        





        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 1000 1000"><path d="M969.8,870.3c27,27.7,27,71.8,0,99.1C955.7,983,937.9,990,920,990c-17.9,0-35.7-7-49.7-20.7L500,599L129.6,969.4C115.6,983,97.8,990,79.9,990s-35.7-7-49.7-20.7c-27-27.3-27-71.4,0-99.1L400.9,500L30.3,129.3c-27-27.3-27-71.4,0-99.1c27.3-27,71.8-27,99.4,0L500,400.9L870.4,30.2c27.7-27,71.8-27,99.4,0c27,27.7,27,71.8,0,99.1L599.1,500L969.8,870.3z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-clock" viewBox="0 0 1000 1000"><path d="M500,10C229.8,10,10,229.8,10,500c0,270.2,219.8,490,490,490c270.2,0,490-219.8,490-490C990,229.8,770.2,10,500,10z M500,910.2c-226.2,0-410.2-184-410.2-410.2c0-226.2,184-410.2,410.2-410.2c226.2,0,410.2,184,410.2,410.2C910.2,726.1,726.2,910.2,500,910.2z M753.1,374c8.2,11.9,5.2,28.1-6.6,36.3L509.9,573.7c-4.4,3.1-9.6,4.6-14.8,4.6c-4.1,0-8.3-1-12.1-3c-8.6-4.5-14-13.4-14-23.1V202.5c0-14.4,11.7-26.1,26.1-26.1c14.4,0,26.1,11.7,26.1,26.1v300l195.6-135.1C728.7,359.2,744.9,362.1,753.1,374z"/></symbol><symbol id="icon-calendar" viewBox="0 0 1000 1000"><path d="M920,500v420H80V500H920 M990,430H10v490c0,38.7,31.3,70,70,70h840c38.7,0,70-31.3,70-70V430L990,430z"/><path d="M850,80v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80H360v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80C72.8,80,10,142.7,10,220v140h980V220C990,142.7,927.2,80,850,80z"/><path d="M255,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C290,25.8,274.3,10,255,10z"/><path d="M745,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C780,25.8,764.3,10,745,10z"/></symbol><symbol id="icon-github" viewBox="0 0 12 14"><path d="M6 1q1.633 0 3.012 0.805t2.184 2.184 0.805 3.012q0 1.961-1.145 3.527t-2.957 2.168q-0.211 0.039-0.312-0.055t-0.102-0.234q0-0.023 0.004-0.598t0.004-1.051q0-0.758-0.406-1.109 0.445-0.047 0.801-0.141t0.734-0.305 0.633-0.52 0.414-0.82 0.16-1.176q0-0.93-0.617-1.609 0.289-0.711-0.062-1.594-0.219-0.070-0.633 0.086t-0.719 0.344l-0.297 0.187q-0.727-0.203-1.5-0.203t-1.5 0.203q-0.125-0.086-0.332-0.211t-0.652-0.301-0.664-0.105q-0.352 0.883-0.062 1.594-0.617 0.68-0.617 1.609 0 0.664 0.16 1.172t0.41 0.82 0.629 0.523 0.734 0.305 0.801 0.141q-0.305 0.281-0.383 0.805-0.164 0.078-0.352 0.117t-0.445 0.039-0.512-0.168-0.434-0.488q-0.148-0.25-0.379-0.406t-0.387-0.187l-0.156-0.023q-0.164 0-0.227 0.035t-0.039 0.090 0.070 0.109 0.102 0.094l0.055 0.039q0.172 0.078 0.34 0.297t0.246 0.398l0.078 0.18q0.102 0.297 0.344 0.48t0.523 0.234 0.543 0.055 0.434-0.027l0.18-0.031q0 0.297 0.004 0.691t0.004 0.426q0 0.141-0.102 0.234t-0.312 0.055q-1.812-0.602-2.957-2.168t-1.145-3.527q0-1.633 0.805-3.012t2.184-2.184 3.012-0.805zM2.273 9.617q0.023-0.055-0.055-0.094-0.078-0.023-0.102 0.016-0.023 0.055 0.055 0.094 0.070 0.047 0.102-0.016zM2.516 9.883q0.055-0.039-0.016-0.125-0.078-0.070-0.125-0.023-0.055 0.039 0.016 0.125 0.078 0.078 0.125 0.023zM2.75 10.234q0.070-0.055 0-0.148-0.062-0.102-0.133-0.047-0.070 0.039 0 0.141t0.133 0.055zM3.078 10.562q0.062-0.062-0.031-0.148-0.094-0.094-0.156-0.023-0.070 0.062 0.031 0.148 0.094 0.094 0.156 0.023zM3.523 10.758q0.023-0.086-0.102-0.125-0.117-0.031-0.148 0.055t0.102 0.117q0.117 0.047 0.148-0.047zM4.016 10.797q0-0.102-0.133-0.086-0.125 0-0.125 0.086 0 0.102 0.133 0.086 0.125 0 0.125-0.086zM4.469 10.719q-0.016-0.086-0.141-0.070-0.125 0.023-0.109 0.117t0.141 0.062 0.109-0.109z"></path></symbol><symbol id="icon-medium" viewBox="0 0 1000 1000"><path d="M336.5,240.2v641.5c0,9.1-2.3,16.9-6.8,23.2s-11.2,9.6-20,9.6c-6.2,0-12.2-1.5-18-4.4L37.3,782.7c-7.7-3.6-14.1-9.8-19.4-18.3S10,747.4,10,739V115.5c0-7.3,1.8-13.5,5.5-18.6c3.6-5.1,8.9-7.7,15.9-7.7c5.1,0,13.1,2.7,24.1,8.2l279.5,140C335.9,238.6,336.5,239.5,336.5,240.2L336.5,240.2z M371.5,295.5l292,473.6l-292-145.5V295.5z M990,305.3v576.4c0,9.1-2.6,16.5-7.7,22.1c-5.1,5.7-12,8.5-20.8,8.5s-17.3-2.4-25.7-7.1L694.7,784.9L990,305.3z M988.4,239.7c0,1.1-46.8,77.6-140.3,229.4C754.6,621,699.8,709.8,683.8,735.7L470.5,389l177.2-288.2c6.2-10.2,15.7-15.3,28.4-15.3c5.1,0,9.8,1.1,14.2,3.3l295.9,147.7C987.6,237.1,988.4,238.2,988.4,239.7L988.4,239.7z"/></symbol><symbol id="icon-instagram" viewBox="0 0 489.84 489.84"><path d="M249.62,50.46c65.4,0,73.14.25,99,1.43C372.47,53,385.44,57,394.07,60.32a75.88,75.88,0,0,1,28.16,18.32,75.88,75.88,0,0,1,18.32,28.16c3.35,8.63,7.34,21.6,8.43,45.48,1.18,25.83,1.43,33.57,1.43,99s-0.25,73.14-1.43,99c-1.09,23.88-5.08,36.85-8.43,45.48a81.11,81.11,0,0,1-46.48,46.48c-8.63,3.35-21.6,7.34-45.48,8.43-25.82,1.18-33.57,1.43-99,1.43s-73.15-.25-99-1.43c-23.88-1.09-36.85-5.08-45.48-8.43A75.88,75.88,0,0,1,77,423.86,75.88,75.88,0,0,1,58.69,395.7c-3.35-8.63-7.34-21.6-8.43-45.48-1.18-25.83-1.43-33.57-1.43-99s0.25-73.14,1.43-99c1.09-23.88,5.08-36.85,8.43-45.48A75.88,75.88,0,0,1,77,78.64a75.88,75.88,0,0,1,28.16-18.32c8.63-3.35,21.6-7.34,45.48-8.43,25.83-1.18,33.57-1.43,99-1.43m0-44.13c-66.52,0-74.86.28-101,1.47s-43.87,5.33-59.45,11.38A120.06,120.06,0,0,0,45.81,47.44,120.06,120.06,0,0,0,17.56,90.82C11.5,106.4,7.36,124.2,6.17,150.27s-1.47,34.46-1.47,101,0.28,74.86,1.47,101,5.33,43.87,11.38,59.45a120.06,120.06,0,0,0,28.25,43.38,120.06,120.06,0,0,0,43.38,28.25c15.58,6.05,33.38,10.19,59.45,11.38s34.46,1.47,101,1.47,74.86-.28,101-1.47,43.87-5.33,59.45-11.38a125.24,125.24,0,0,0,71.63-71.63c6.05-15.58,10.19-33.38,11.38-59.45s1.47-34.46,1.47-101-0.28-74.86-1.47-101-5.33-43.87-11.38-59.45a120.06,120.06,0,0,0-28.25-43.38,120.06,120.06,0,0,0-43.38-28.25C394.47,13.13,376.67,9,350.6,7.8s-34.46-1.47-101-1.47h0Z" transform="translate(-4.7 -6.33)" /><path d="M249.62,125.48A125.77,125.77,0,1,0,375.39,251.25,125.77,125.77,0,0,0,249.62,125.48Zm0,207.41a81.64,81.64,0,1,1,81.64-81.64A81.64,81.64,0,0,1,249.62,332.89Z" transform="translate(-4.7 -6.33)"/><circle cx="375.66" cy="114.18" r="29.39" /></symbol><symbol id="icon-linkedin" viewBox="0 0 12 14"><path d="M2.727 4.883v7.742h-2.578v-7.742h2.578zM2.891 2.492q0.008 0.57-0.395 0.953t-1.059 0.383h-0.016q-0.641 0-1.031-0.383t-0.391-0.953q0-0.578 0.402-0.957t1.051-0.379 1.039 0.379 0.398 0.957zM12 8.187v4.437h-2.57v-4.141q0-0.82-0.316-1.285t-0.988-0.465q-0.492 0-0.824 0.27t-0.496 0.668q-0.086 0.234-0.086 0.633v4.32h-2.57q0.016-3.117 0.016-5.055t-0.008-2.313l-0.008-0.375h2.57v1.125h-0.016q0.156-0.25 0.32-0.438t0.441-0.406 0.68-0.34 0.895-0.121q1.336 0 2.148 0.887t0.813 2.598z"></path></symbol><symbol id="icon-heart" viewBox="0 0 34 30"><path d="M17,29.7 L16.4,29.2 C3.5,18.7 0,15 0,9 C0,4 4,0 9,0 C13.1,0 15.4,2.3 17,4.1 C18.6,2.3 20.9,0 25,0 C30,0 34,4 34,9 C34,15 30.5,18.7 17.6,29.2 L17,29.7 Z M9,2 C5.1,2 2,5.1 2,9 C2,14.1 5.2,17.5 17,27.1 C28.8,17.5 32,14.1 32,9 C32,5.1 28.9,2 25,2 C21.5,2 19.6,4.1 18.1,5.8 L17,7.1 L15.9,5.8 C14.4,4.1 12.5,2 9,2 Z" id="Shape"></path></symbol><symbol id="icon-arrow-right" viewBox="0 0 25.452 25.452"><path d="M4.471,24.929v-2.004l12.409-9.788c0.122-0.101,0.195-0.251,0.195-0.411c0-0.156-0.073-0.31-0.195-0.409L4.471,2.526V0.522c0-0.2,0.115-0.384,0.293-0.469c0.18-0.087,0.396-0.066,0.552,0.061l15.47,12.202c0.123,0.1,0.195,0.253,0.195,0.409c0,0.16-0.072,0.311-0.195,0.411L5.316,25.34c-0.155,0.125-0.372,0.147-0.552,0.061C4.586,25.315,4.471,25.13,4.471,24.929z"/></symbol><symbol id="icon-star" viewBox="0 0 48 48"><path fill="currentColor" d="M44,24c0,11.045-8.955,20-20,20S4,35.045,4,24S12.955,4,24,4S44,12.955,44,24z"/><path fill="#ffffff" d="M24,11l3.898,7.898l8.703,1.301l-6.301,6.102l1.5,8.699L24,30.898L16.199,35l1.5-8.699l-6.301-6.102  l8.703-1.301L24,11z"/></symbol><symbol id="icon-read" viewBox="0 0 32 32"><path fill="currentColor" d="M29,4H3C1.343,4,0,5.343,0,7v18c0,1.657,1.343,3,3,3h10c0,0.552,0.448,1,1,1h4c0.552,0,1-0.448,1-1h10  c1.657,0,3-1.343,3-3V7C32,5.343,30.657,4,29,4z M29,5v20H18.708c-0.618,0-1.236,0.146-1.789,0.422l-0.419,0.21V5H29z M15.5,5  v20.632l-0.419-0.21C14.528,25.146,13.91,25,13.292,25H3V5H15.5z M31,25c0,1.103-0.897,2-2,2H18v1h-4v-1H3c-1.103,0-2-0.897-2-2V7  c0-0.737,0.405-1.375,1-1.722V25c0,0.552,0.448,1,1,1h10.292c0.466,0,0.925,0.108,1.342,0.317l0.919,0.46  c0.141,0.07,0.294,0.106,0.447,0.106c0.153,0,0.306-0.035,0.447-0.106l0.919-0.46C17.783,26.108,18.242,26,18.708,26H29  c0.552,0,1-0.448,1-1V5.278C30.595,5.625,31,6.263,31,7V25z M6,12.5C6,12.224,6.224,12,6.5,12h5c0.276,0,0.5,0.224,0.5,0.5  S11.776,13,11.5,13h-5C6.224,13,6,12.776,6,12.5z M6,14.5C6,14.224,6.224,14,6.5,14h5c0.276,0,0.5,0.224,0.5,0.5S11.776,15,11.5,15  h-5C6.224,15,6,14.776,6,14.5z M6,16.5C6,16.224,6.224,16,6.5,16h5c0.276,0,0.5,0.224,0.5,0.5S11.776,17,11.5,17h-5  C6.224,17,6,16.776,6,16.5z M20,12.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,13,25.5,13h-5  C20.224,13,20,12.776,20,12.5z M20,14.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,15,25.5,15h-5  C20.224,15,20,14.776,20,14.5z M20,16.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,17,25.5,17h-5  C20.224,17,20,16.776,20,16.5z"></path></symbol><symbol id="icon-tistory" viewBox="0 0 24 24"><path d="M4 4h16v3h-6v13h-4V7H4V4z"/></symbol></defs></svg>

        <header class="bar-header">
    <a id="menu" role="button">
        <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    </a>
    <h1 class="logo">
        <a href="/">
            
                somaz <span class="version">v3.1.2</span>
            
        </a>
    </h1>
    <a id="search" class="dosearch" role="button">
        <svg class="icon-search"><use xlink:href="#icon-search"></use></svg>
    </a>
    
        <a href="https://github.com/thiagorossener/jekflix-template" class="get-theme" role="button">
            Get this theme!
        </a>
    
</header>

<div id="mask" class="overlay"></div>

<aside class="sidebar" id="sidebar">
    <nav id="navigation">
      <h2>Menu</h2>
      <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>

    </nav>
</aside>

<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>



        <section class="post two-columns">
            <article role="article" class="post-content">
                <p class="post-info">
                    
                        <svg class="icon-calendar" id="date"><use xlink:href="#icon-calendar"></use></svg>
                        <time class="date" datetime="2025-01-15T09:00:00+00:00">
                            


January 15, 2025

                        </time>
                    
                    <svg id="clock" class="icon-clock"><use xlink:href="#icon-clock"></use></svg>
                    <span>14 min to read</span>
                </p>
                <h1 class="post-title">Setting up GPU Server on Google Cloud Platform</h1>
                <p class="post-subtitle">A complete guide to deploying and configuring NVIDIA A100 GPU instances on GCP</p>

                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1737358848/gpu_keixqz.png" alt="Featured image" class="post-cover">
                

                <!-- Pagination links -->



                <!-- Add your table of contents here -->


                <p><br /></p>

<hr />

<h2 id="overview">Overview</h2>

<p>Google Cloud Platform (GCP) offers powerful GPU-accelerated computing instances that are ideal for AI/ML workloads, scientific computing, and rendering applications. This guide focuses on setting up and optimizing NVIDIA A100 GPU servers, which represent some of the most powerful GPU resources currently available in the cloud.</p>

<p>The NVIDIA A100 offers exceptional performance for deep learning training and inference, with variants including the 40GB and 80GB memory configurations. This guide walks through the entire process from checking GPU availability, requesting quota, deploying via Terraform, installing drivers and frameworks, to running popular AI applications.</p>

<div class="info-box info-box-default-not-check">
  <strong>NVIDIA A100 Specifications</strong>
  <table class="table-modern" style="margin-top:10px">
    <tr>
      <th>Specification</th>
      <th>A100 40GB</th>
      <th>A100 80GB</th>
    </tr>
    <tr>
      <td>GPU Memory</td>
      <td>40GB HBM2</td>
      <td>80GB HBM2e</td>
    </tr>
    <tr>
      <td>Memory Bandwidth</td>
      <td>1.6 TB/s</td>
      <td>2.0 TB/s</td>
    </tr>
    <tr>
      <td>FP32 Performance</td>
      <td>19.5 TFLOPS</td>
      <td>19.5 TFLOPS</td>
    </tr>
    <tr>
      <td>Tensor Cores</td>
      <td>432</td>
      <td>432</td>
    </tr>
    <tr>
      <td>CUDA Cores</td>
      <td>6,912</td>
      <td>6,912</td>
    </tr>
    <tr>
      <td>TensorFloat-32 (TF32)</td>
      <td>156 TFLOPS</td>
      <td>156 TFLOPS</td>
    </tr>
    <tr>
      <td>GCP Machine Types</td>
      <td>a2-highgpu-*</td>
      <td>a2-ultragpu-*</td>
    </tr>
  </table>
</div>

<p><br /></p>

<hr />

<h2 id="gpu-availability-and-quota">GPU Availability and Quota</h2>

<h3 id="checking-gpu-availability-by-region">Checking GPU Availability by Region</h3>

<p>Before deploying a GPU instance, you need to verify which GPU types are available in your preferred region and zones. The availability varies by region, and not all zones support all GPU types.</p>

<h4 id="official-documentation">Official Documentation</h4>
<ul>
  <li><a href="https://cloud.google.com/compute/docs/gpus?hl=en">English Documentation</a></li>
  <li><a href="https://cloud.google.com/compute/docs/gpus?hl=ko">Korean Documentation</a></li>
</ul>

<h4 id="using-gcloud-cli-to-check-availability">Using gcloud CLI to Check Availability</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check A100 availability in Seoul region</span>
gcloud compute accelerator-types list <span class="nt">--filter</span><span class="o">=</span><span class="s2">"name:nvidia-tesla-a100 AND zone:asia-northeast3"</span>
NAME               ZONE               DESCRIPTION
nvidia-tesla-a100  asia-northeast3-a  NVIDIA A100 40GB
nvidia-tesla-a100  asia-northeast3-b  NVIDIA A100 40GB

<span class="c"># Check A100 availability in all regions</span>
gcloud compute accelerator-types list | <span class="nb">grep </span>a100
nvidia-a100-80gb       us-central1-a              NVIDIA A100 80GB
nvidia-tesla-a100      us-central1-a              NVIDIA A100 40GB
nvidia-tesla-a100      us-central1-b              NVIDIA A100 40GB
<span class="c"># ... more output ...</span>
</code></pre></div></div>

<div style="width: 100%; margin: auto; margin-top: 20px;">
  <div class="mermaid">
    graph TD
      A[GPU Selection Process] --&gt; B{Determine Memory Needs}
      B --&gt;|High Memory Requirements| C[A100 80GB]
      B --&gt;|Standard Requirements| D[A100 40GB]
      C --&gt; E{Check Zone Availability}
      D --&gt; E
      E --&gt; F[Request Quota Increase]
      F --&gt; G[Deploy GPU Instance]
  </div>
</div>

<p><br /></p>

<h3 id="requesting-gpu-quota-increase">Requesting GPU Quota Increase</h3>

<p>By default, GCP accounts have limited or no GPU quota. To use GPUs, especially high-end models like the A100, you need to request a quota increase:</p>

<ol>
  <li>Navigate to <strong>IAM &amp; Admin &gt; Quotas &amp; System Limits</strong> in the GCP Console</li>
  <li>Filter for “NVIDIA A100” or specific quota names</li>
  <li>Select the region(s) where you need quota</li>
  <li>Click “EDIT QUOTAS” and enter your requested limit</li>
  <li>Submit and provide business justification if prompted</li>
</ol>

<p><strong>Important Resources:</strong></p>
<ul>
  <li><a href="https://cloud.google.com/compute/resource-usage?hl=en#gpu_quota">GPU Quota Documentation</a></li>
  <li><a href="https://cloud.google.com/docs/quotas/view-manage?hl=en#requesting_higher_quota">Requesting Higher Quota</a></li>
  <li><a href="https://cloud.google.com/compute/docs/gpus/add-remove-gpus?hl=en#accelerator-optimized_vms">Accelerator-optimized VMs</a></li>
  <li><a href="https://cloud.google.com/compute/docs/gpus/add-remove-gpus?hl=en#modify_the_gpu_count">Modifying GPU Count</a></li>
</ul>

<p><br /></p>

<div class="info-box info-box-warning-not-check">
  <strong>Quota Request Tips</strong>
  <ul>
    <li>Be specific about your use case (AI training, inference, scientific computing)</li>
    <li>Request quota in regions with known availability</li>
    <li>Specify timeline and duration of GPU usage</li>
    <li>For large quota requests, be prepared to provide business justification</li>
    <li>Allow 1-2 business days for quota approval</li>
  </ul>
</div>

<p><br /></p>

<hr />

<h2 id="gpu-pricing">GPU Pricing</h2>

<p>GPU pricing varies significantly by type, region, and commitment level. Always check the current pricing before deployment.</p>

<h3 id="official-pricing-documentation">Official Pricing Documentation</h3>
<ul>
  <li><a href="https://cloud.google.com/compute/all-pricing?hl=en">English</a></li>
  <li><a href="https://cloud.google.com/compute/all-pricing?hl=ko">Korean</a></li>
</ul>

<h3 id="nvidia-a100-pricing-example-as-of-2025">NVIDIA A100 Pricing Example (as of 2025)</h3>

<table>
  <thead>
    <tr>
      <th>Machine Type</th>
      <th>GPU Configuration</th>
      <th>Monthly Cost</th>
      <th>Daily Cost</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>a2-highgpu-1g</td>
      <td>1x A100 40GB</td>
      <td>~₩3.95M</td>
      <td>~₩120K</td>
    </tr>
    <tr>
      <td>a2-highgpu-2g</td>
      <td>2x A100 40GB</td>
      <td>~₩7.9M</td>
      <td>~₩240K</td>
    </tr>
    <tr>
      <td>a2-highgpu-4g</td>
      <td>4x A100 40GB</td>
      <td>~₩15.8M</td>
      <td>~₩480K</td>
    </tr>
    <tr>
      <td>a2-ultragpu-1g</td>
      <td>1x A100 80GB</td>
      <td>~₩6.2M</td>
      <td>~₩205K</td>
    </tr>
    <tr>
      <td>a2-ultragpu-2g</td>
      <td>2x A100 80GB</td>
      <td>~₩12.4M</td>
      <td>~₩410K</td>
    </tr>
    <tr>
      <td>a2-ultragpu-4g</td>
      <td>4x A100 80GB</td>
      <td>~₩24.8M</td>
      <td>~₩820K</td>
    </tr>
  </tbody>
</table>

<div class="info-box info-box-info">
  <h3><strong>Cost Optimization Strategies</strong></h3>
  <ul>
    <li><strong>Spot Instances:</strong> Use preemptible VMs for interruptible workloads (60-91% discount)</li>
    <li><strong>Committed Use Discounts:</strong> 1-year or 3-year commitments for predictable workloads (20-57% discount)</li>
    <li><strong>Right-sizing:</strong> Select appropriate GPU count based on actual needs</li>
    <li><strong>Resource Scheduling:</strong> Create startup/shutdown schedules for non-production environments</li>
    <li><strong>Monitoring:</strong> Set up budget alerts and utilization monitoring</li>
  </ul>
</div>

<p><br /></p>

<hr />

<h2 id="infrastructure-deployment-with-terraform">Infrastructure Deployment with Terraform</h2>

<p>Terraform allows for declarative, version-controlled deployment of GPU instances. Below is an example configuration for an A100 GPU server:</p>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ai_server ##</span>
<span class="nx">resource</span> <span class="s2">"google_compute_address"</span> <span class="s2">"ai_server_ip"</span> <span class="p">{</span>
  <span class="nx">name</span>   <span class="p">=</span> <span class="nx">var</span><span class="err">.</span><span class="nx">ai_server_ip</span>
  <span class="nx">region</span> <span class="p">=</span> <span class="nx">var</span><span class="err">.</span><span class="nx">region</span>
<span class="p">}</span>

<span class="nx">resource</span> <span class="s2">"google_compute_instance"</span> <span class="s2">"ai_server"</span> <span class="p">{</span>
  <span class="nx">name</span>                      <span class="p">=</span> <span class="nx">var</span><span class="err">.</span><span class="nx">ai_server</span>
  <span class="nx">machine_type</span>              <span class="p">=</span> <span class="s2">"a2-highgpu-2g"</span> <span class="c1"># a2-ultragpu-2g = A100 80G 2 / a2-highgpu-2g = A100 40G 2</span>
  <span class="nx">labels</span>                    <span class="p">=</span> <span class="nx">local</span><span class="err">.</span><span class="nx">default_labels</span>
  <span class="nx">zone</span>                      <span class="p">=</span> <span class="s2">"${var.region}-a"</span>
  <span class="nx">allow_stopping_for_update</span> <span class="p">=</span> <span class="kc">true</span>

  <span class="nx">tags</span> <span class="p">=</span> <span class="p">[</span><span class="nx">var</span><span class="err">.</span><span class="nx">nfs_client</span><span class="p">]</span>

  <span class="nx">boot_disk</span> <span class="p">{</span>
    <span class="nx">initialize_params</span> <span class="p">{</span>
      <span class="nx">image</span> <span class="p">=</span> <span class="s2">"ubuntu-os-cloud/ubuntu-2204-lts"</span>
      <span class="nx">size</span>  <span class="p">=</span> <span class="mi">100</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="nx">metadata</span> <span class="p">=</span> <span class="p">{</span>
    <span class="nx">ssh</span><span class="err">-</span><span class="nx">keys</span>              <span class="p">=</span> <span class="s2">"somaz:${file("</span><span class="err">../../</span><span class="nx">key</span><span class="err">/</span><span class="nx">ai</span><span class="err">-</span><span class="nx">server</span><span class="err">.</span><span class="nx">pub</span><span class="s2">")}"</span>
    <span class="nx">install</span><span class="err">-</span><span class="nx">nvidia</span><span class="err">-</span><span class="nx">driver</span> <span class="p">=</span> <span class="s2">"true"</span>
  <span class="p">}</span>

  <span class="nx">network_interface</span> <span class="p">{</span>
    <span class="nx">network</span>    <span class="p">=</span> <span class="s2">"projects/${var.host_project}/global/networks/${var.shared_vpc}"</span>
    <span class="nx">subnetwork</span> <span class="p">=</span> <span class="s2">"projects/${var.host_project}/regions/${var.region}/subnetworks/${var.subnet_share}-ai-b"</span>

    <span class="nx">access_config</span> <span class="p">{</span>
      <span class="c1">## Include this section to give the VM an external ip ##</span>
      <span class="nx">nat_ip</span> <span class="p">=</span> <span class="nx">google_compute_address</span><span class="err">.</span><span class="nx">ai_server_ip</span><span class="err">.</span><span class="nx">address</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="nx">scheduling</span> <span class="p">{</span>
    <span class="nx">on_host_maintenance</span> <span class="p">=</span> <span class="s2">"TERMINATE"</span> <span class="c1"># Required for GPU instances</span>
    <span class="nx">automatic_restart</span>   <span class="p">=</span> <span class="kc">true</span>
    <span class="nx">preemptible</span>         <span class="p">=</span> <span class="kc">false</span> <span class="c1"># Set to true for preemptible instances</span>
  <span class="p">}</span>

  <span class="nx">guest_accelerator</span> <span class="p">{</span>
    <span class="nx">type</span>  <span class="p">=</span> <span class="s2">"nvidia-tesla-a100"</span> <span class="c1"># nvidia-a100-80gb = A100 80G / nvidia-tesla-a100 = A100 40G</span>
    <span class="nx">count</span> <span class="p">=</span> <span class="mi">2</span>
  <span class="p">}</span>

  <span class="nx">depends_on</span> <span class="p">=</span> <span class="p">[</span><span class="nx">google_compute_address</span><span class="err">.</span><span class="nx">ai_server_ip</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="key-terraform-configuration-points">Key Terraform Configuration Points</h3>

<ol>
  <li><strong>Machine Type Selection</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">a2-highgpu-*g</code> for A100 40GB (where * is the GPU count)</li>
      <li><code class="language-plaintext highlighter-rouge">a2-ultragpu-*g</code> for A100 80GB (where * is the GPU count)</li>
    </ul>
  </li>
  <li><strong>Boot Disk</strong>:
    <ul>
      <li>Use Ubuntu 20.04 or 22.04 for best compatibility</li>
      <li>Allocate sufficient disk space (100GB+) for datasets and model weights</li>
    </ul>
  </li>
  <li><strong>GPU-specific Settings</strong>:
    <ul>
      <li>Set <code class="language-plaintext highlighter-rouge">on_host_maintenance = "TERMINATE"</code> (required for GPU VMs)</li>
      <li>Use <code class="language-plaintext highlighter-rouge">guest_accelerator</code> block to specify GPU type and count</li>
      <li>Consider setting <code class="language-plaintext highlighter-rouge">metadata.install-nvidia-driver = "true"</code> for automatic driver installation</li>
    </ul>
  </li>
  <li><strong>Networking</strong>:
    <ul>
      <li>Consider whether your workload needs an external IP</li>
      <li>Configure appropriate firewall rules for your services</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<hr />

<h2 id="gpu-server-setup-and-configuration">GPU Server Setup and Configuration</h2>

<p>After deploying the VM, you need to install drivers, CUDA toolkit, and other necessary software.</p>

<h3 id="verifying-gpu-availability">Verifying GPU Availability</h3>

<p>First, check if the GPUs are properly attached to the VM:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># OS Version</span>
lsb_release <span class="nt">-a</span>

<span class="c"># Confirm GPU hardware detection</span>
<span class="nb">sudo </span>lspci | <span class="nb">grep</span> <span class="nt">-i</span> nvidia
<span class="nb">sudo </span>lshw <span class="nt">-c</span> display
</code></pre></div></div>

<h3 id="installing-drivers-and-cuda-toolkit">Installing Drivers and CUDA Toolkit</h3>

<p>GCP can automatically install NVIDIA drivers if you set the <code class="language-plaintext highlighter-rouge">install-nvidia-driver = "true"</code> metadata. If you need to install manually:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install required packages</span>
<span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> ubuntu-drivers-common
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> nvidia-driver-535
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> nvidia-cuda-toolkit
</code></pre></div></div>

<h3 id="installing-cudnn">Installing cuDNN</h3>

<p>NVIDIA cuDNN is required for deep learning frameworks:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Download cuDNN (requires NVIDIA Developer account)</span>
<span class="c"># Example assumes you've downloaded the archive</span>
<span class="nb">tar </span>xvf cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz
<span class="nb">cd </span>cudnn-linux-x86_64-8.6.0.163_cuda11-archive

<span class="c"># Copy header files</span>
<span class="nb">sudo cp </span>include/cudnn<span class="k">*</span>.h /usr/include

<span class="c"># Copy library files</span>
<span class="nb">sudo cp </span>lib/libcudnn<span class="k">*</span> /usr/lib/x86_64-linux-gnu

<span class="c"># Set permissions and update library cache</span>
<span class="nb">sudo chmod </span>a+r /usr/include/cudnn<span class="k">*</span>.h /usr/lib/x86_64-linux-gnu/libcudnn<span class="k">*</span>
<span class="nb">sudo </span>ldconfig
</code></pre></div></div>

<h3 id="verifying-installation">Verifying Installation</h3>

<p>Verify that drivers, CUDA, and cuDNN are properly installed:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check NVIDIA driver</span>
nvidia-smi

<span class="c"># Expected output example:</span>
<span class="c"># +-----------------------------------------------------------------------------+</span>
<span class="c"># | NVIDIA-SMI 535.104.05   Driver Version: 535.104.05   CUDA Version: 12.2     |</span>
<span class="c"># |-------------------------------+----------------------+----------------------+</span>
<span class="c"># | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>
<span class="c"># | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span>
<span class="c"># |                               |                      |               MIG M. |</span>
<span class="c"># |===============================+======================+======================|</span>
<span class="c"># |   0  NVIDIA A100-SXM...  On   | 00000000:00:04.0 Off |                    0 |</span>
<span class="c"># | N/A   32C    P0    39W / 400W |      0MiB / 40960MiB |      0%      Default |</span>
<span class="c"># |                               |                      |             Disabled |</span>
<span class="c"># +-------------------------------+----------------------+----------------------+</span>
<span class="c"># |   1  NVIDIA A100-SXM...  On   | 00000000:00:05.0 Off |                    0 |</span>
<span class="c"># | N/A   34C    P0    41W / 400W |      0MiB / 40960MiB |      0%      Default |</span>
<span class="c"># |                               |                      |             Disabled |</span>
<span class="c"># +-------------------------------+----------------------+----------------------+</span>

<span class="c"># Check CUDA version</span>
nvcc <span class="nt">--version</span>

<span class="c"># Test cuDNN installation</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt; cudnn_test.cpp
#include &lt;cudnn.h&gt;
#include &lt;iostream&gt;

int main() {
    cudnnHandle_t cudnn;
    cudnnCreate(&amp;cudnn);
    std::cout &lt;&lt; "CuDNN version: " &lt;&lt; CUDNN_VERSION &lt;&lt; std::endl;
    cudnnDestroy(cudnn);
    return 0;
}
</span><span class="no">EOF

</span><span class="c"># Compile and run the cuDNN test</span>
nvcc <span class="nt">-o</span> cudnn_test cudnn_test.cpp <span class="nt">-lcudnn</span>
./cudnn_test
<span class="c"># Expected output: CuDNN version: 8600</span>
</code></pre></div></div>

<h3 id="setting-up-nvidia-docker-optional">Setting Up NVIDIA Docker (Optional)</h3>

<p>For containerized workloads, NVIDIA Docker allows GPU access from containers:</p>

<p><br /></p>

<script src="https://gist.github.com/somaz94/c48897a5bd73d87a1e31aef27f4e0ce2.js"></script>

<p><br /></p>

<hr />

<h2 id="aiml-application-setup">AI/ML Application Setup</h2>

<p>With the GPU environment configured, you can now set up popular AI/ML applications.</p>

<h3 id="stable-diffusion-webui">Stable Diffusion WebUI</h3>

<p>A popular interface for text-to-image generation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Clone repository and run installation script</span>
wget <span class="nt">-q</span> https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
<span class="nb">chmod</span> +x webui.sh

<span class="c"># Start the WebUI with remote access enabled</span>
./webui.sh <span class="nt">--listen</span> <span class="nt">--xformers</span>
</code></pre></div></div>

<p>Access the web UI at <code class="language-plaintext highlighter-rouge">http://&lt;your-vm-ip&gt;:7860</code></p>

<h3 id="kohya_ss">Kohya_ss</h3>

<p>A tool for training and fine-tuning Stable Diffusion models:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Clone repository</span>
git clone https://github.com/bmaltais/kohya_ss.git
<span class="nb">cd </span>kohya_ss

<span class="c"># Run installation script</span>
./setup.sh

<span class="c"># Start the UI with remote access</span>
./gui.sh <span class="nt">--listen</span><span class="o">=</span>0.0.0.0 <span class="nt">--headless</span>
</code></pre></div></div>

<p>Access the web UI at <code class="language-plaintext highlighter-rouge">http://&lt;your-vm-ip&gt;:7860</code></p>

<h3 id="comfyui">ComfyUI</h3>

<p>A node-based UI for Stable Diffusion:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Clone repository</span>
git clone https://github.com/comfyanonymous/ComfyUI.git
<span class="nb">cd </span>ComfyUI

<span class="c"># Install dependencies</span>
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt

<span class="c"># Start the UI with remote access</span>
python3 main.py <span class="nt">--listen</span> 0.0.0.0
</code></pre></div></div>

<p>Access the web UI at <code class="language-plaintext highlighter-rouge">http://&lt;your-vm-ip&gt;:8188</code></p>

<h3 id="pytorch-and-tensorflow-performance-testing">PyTorch and TensorFlow Performance Testing</h3>

<p>Test the GPU performance with standard benchmarks:</p>

<p><br /></p>

<script src="https://gist.github.com/somaz94/fed19230e716428d8cba1abb4d4be139.js"></script>

<p><br /></p>

<hr />

<h2 id="performance-optimization">Performance Optimization</h2>

<h3 id="nvidia-a100-performance-tuning">NVIDIA A100 Performance Tuning</h3>

<ol>
  <li><strong>GPU Compute Mode</strong>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="nb">sudo </span>nvidia-smi <span class="nt">-i</span> 0 <span class="nt">-c</span> EXCLUSIVE_PROCESS  <span class="c"># For dedicated single process</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Memory Clock Optimization</strong>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="nb">sudo </span>nvidia-smi <span class="nt">-i</span> 0 <span class="nt">-ac</span> 1215,1410  <span class="c"># Set memory and graphics clocks</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Persistent Mode</strong> (reduces initialization overhead):
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="nb">sudo </span>nvidia-smi <span class="nt">-i</span> 0 <span class="nt">-pm</span> 1  <span class="c"># Enable persistent mode</span>
</code></pre></div>    </div>
  </li>
</ol>

<h3 id="system-level-optimizations">System-level Optimizations</h3>

<ol>
  <li><strong>Disable CPU Power Management</strong>:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="nb">sudo </span>apt <span class="nb">install </span>cpufrequtils
   <span class="nb">sudo </span>cpufreq-set <span class="nt">-g</span> performance
</code></pre></div>    </div>
  </li>
  <li><strong>I/O Optimization</strong> for dataset loading:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="nb">echo</span> <span class="s1">'vm.dirty_ratio = 10'</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/sysctl.conf
   <span class="nb">echo</span> <span class="s1">'vm.dirty_background_ratio = 5'</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/sysctl.conf
   <span class="nb">sudo </span>sysctl <span class="nt">-p</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Memory Configuration</strong> for AI workloads:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="nb">echo</span> <span class="s1">'vm.overcommit_memory = 1'</span> | <span class="nb">sudo tee</span> <span class="nt">-a</span> /etc/sysctl.conf
   <span class="nb">sudo </span>sysctl <span class="nt">-p</span>
</code></pre></div>    </div>
  </li>
</ol>

<h3 id="framework-specific-optimizations">Framework-specific Optimizations</h3>

<ol>
  <li><strong>PyTorch</strong>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="bp">True</span>  <span class="c1"># Auto-tuner for convolutions
</span>   <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">matmul</span><span class="p">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="bp">True</span>  <span class="c1"># Enable TF32 for faster training
</span></code></pre></div>    </div>
  </li>
  <li><strong>TensorFlow</strong>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
   <span class="n">tf</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">set_jit</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># Enable XLA JIT compilation
</span>   <span class="n">mixed_precision</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">mixed_precision</span><span class="p">.</span><span class="n">Policy</span><span class="p">(</span><span class="s">'mixed_float16'</span><span class="p">)</span>
   <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">mixed_precision</span><span class="p">.</span><span class="n">set_global_policy</span><span class="p">(</span><span class="n">mixed_precision</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ol>

<p><br /></p>

<hr />

<h2 id="monitoring-gpu-usage">Monitoring GPU Usage</h2>

<h3 id="nvidia-system-management-interface">NVIDIA System Management Interface</h3>

<p>The primary tool for monitoring NVIDIA GPUs is <code class="language-plaintext highlighter-rouge">nvidia-smi</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Basic GPU status</span>
nvidia-smi

<span class="c"># Continuous monitoring (updates every 1s)</span>
nvidia-smi <span class="nt">-l</span> 1

<span class="c"># Show memory usage by process</span>
nvidia-smi <span class="nt">--query-compute-apps</span><span class="o">=</span>pid,process_name,used_memory <span class="nt">--format</span><span class="o">=</span>csv

<span class="c"># Monitor specific metrics</span>
nvidia-smi <span class="nt">--query-gpu</span><span class="o">=</span>timestamp,name,pci.bus_id,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used <span class="nt">--format</span><span class="o">=</span>csv <span class="nt">-l</span> 5
</code></pre></div></div>

<h3 id="dcgm-data-center-gpu-manager">DCGM (Data Center GPU Manager)</h3>

<p>For production environments, NVIDIA DCGM provides more advanced monitoring:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install DCGM</span>
<span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> datacenter-gpu-manager

<span class="c"># Start the service</span>
<span class="nb">sudo </span>systemctl start dcgm

<span class="c"># Run DCGM diagnostics</span>
<span class="nb">sudo </span>dcgmi diag <span class="nt">-r</span> 3

<span class="c"># Monitor GPU stats</span>
<span class="nb">sudo </span>dcgmi dmon
</code></pre></div></div>

<h3 id="integrating-with-prometheus-and-grafana">Integrating with Prometheus and Grafana</h3>

<p>For long-term monitoring and visualization:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install Prometheus NVIDIA exporter</span>
git clone https://github.com/NVIDIA/dcgm-exporter.git
<span class="nb">cd </span>dcgm-exporter
make binary
<span class="nb">sudo </span>make <span class="nb">install</span>

<span class="c"># Start the exporter</span>
<span class="nb">sudo </span>dcgm-exporter

<span class="c"># Configure Prometheus to scrape the exporter</span>
<span class="c"># Add to prometheus.yml:</span>
<span class="c"># - job_name: 'dcgm_exporter'</span>
<span class="c">#   static_configs:</span>
<span class="c">#   - targets: ['localhost:9400']</span>
</code></pre></div></div>

<div style="width: 100%; margin: auto; margin-top: 20px;">
  <div class="mermaid">
    graph LR
      A[NVIDIA GPU] --&gt; B[DCGM Exporter]
      B --&gt; C[Prometheus]
      C --&gt; D[Grafana]
      D --&gt; E[Alerts]
      D --&gt; F[Dashboards]
  </div>
</div>

<p><br /></p>

<hr />

<h2 id="best-practices-and-security">Best Practices and Security</h2>

<div class="info-box info-box-success-not-check">
  <strong>GPU Server Best Practices</strong>
  
  <strong>1. GPU Selection:</strong>
  <ul>
    <li>Choose appropriate GPU type based on memory requirements and computational needs</li>
    <li>Consider scalability - start small and scale up as needed</li>
    <li>Check zone availability before planning deployments</li>
  </ul>
  
  <strong>2. Cost Optimization:</strong>
  <ul>
    <li>Monitor usage patterns and adjust instance types accordingly</li>
    <li>Use preemptible instances for non-critical, fault-tolerant workloads</li>
    <li>Implement auto-shutdown for development environments</li>
    <li>Use committed use discounts for predictable workloads</li>
  </ul>
  
  <strong>3. Security:</strong>
  <ul>
    <li>Configure firewall rules to restrict access to necessary ports only</li>
    <li>Use service accounts with minimum required permissions</li>
    <li>Enable OS Login for centralized SSH access management</li>
    <li>Apply security patches promptly</li>
    <li>Enable Virtual Private Cloud (VPC) Service Controls for sensitive workloads</li>
  </ul>
  
  <strong>4. Performance:</strong>
  <ul>
    <li>Install the latest NVIDIA drivers compatible with your workload</li>
    <li>Configure appropriate CUDA and cuDNN versions</li>
    <li>Monitor GPU utilization and memory usage</li>
    <li>Implement proper cooling and power policies</li>
    <li>Use mixed precision training where possible</li>
  </ul>
  
  <strong>5. Data Management:</strong>
  <ul>
    <li>Use high-performance storage options for datasets (SSD, Local SSD)</li>
    <li>Implement data caching mechanisms</li>
    <li>Consider Cloud Storage with parallel transfers for large datasets</li>
    <li>Implement proper backup strategies for trained models</li>
  </ul>
</div>

<p><br /></p>

<hr />

<h2 id="references">References</h2>

<ul>
  <li><a href="https://cloud.google.com/compute/docs/gpus">GCP GPU Documentation</a></li>
  <li><a href="https://www.nvidia.com/en-us/data-center/a100/">NVIDIA A100 Tensor Core GPU</a></li>
  <li><a href="https://docs.nvidia.com/cuda/">CUDA Toolkit Documentation</a></li>
  <li><a href="https://github.com/somaz94/terraform-infra-gcp/tree/main/project/somaz-ai-project">Terraform Infrastructure Example</a></li>
  <li><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Stable Diffusion WebUI</a></li>
  <li><a href="https://github.com/bmaltais/kohya_ss">Kohya_ss</a></li>
  <li><a href="https://github.com/comfyanonymous/ComfyUI">ComfyUI</a></li>
  <li><a href="https://docs.nvidia.com/datacenter/dcgm/latest/">NVIDIA DCGM Documentation</a></li>
</ul>


                <!-- Pagination links -->


            </article>

            
                <aside class="see-also">
                    <h2>See also</h2>
                    <ul>
                        
                        
                        
                            <li>
                                <a href="/category/openstack/openstack-mistral/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1742267532/openstack_t2tmoc.png">
                                    
                                    <h3>Deep Dive into OpenStack Mistral</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/cicd/argocd-notifications-slack/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1737365011/argocd_x79gn0.png">
                                    
                                    <h3>Setting up ArgoCD Notifications with Slack</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/openstack/openstack-ceilometer/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1742267532/openstack_t2tmoc.png">
                                    
                                    <h3>Deep Dive into OpenStack Ceilometer</h3>
                                </a>
                            </li>
                        
                    </ul>
                </aside>
            

        </section>

        <!-- Add time bar only for pages without pagination -->
        
            <div class="time-bar" data-minutes="14">
    <span class="time-completed"></span>
    <span class="time-remaining"></span>
    <div class="bar">
        <span class="completed" style="width:0%;"></span>
        <span class="remaining" style="width:100%;"></span>
    </div>
</div>

            <button class="toggle-preview" onclick="togglePreview()">
    <span>Hide Preview ▼</span>
</button>

<div id="recommendationSection" class="recommendation">
    <div class="message">
        <strong>Why don't you read something next?</strong>
        <div>
            <button>
                <svg><use xlink:href="#icon-arrow-right"></use></svg>
                <span>Go back to top</span>
            </button>
        </div>
    </div>
    <div id="previewSection" class="preview-section">
        
        <a href="/category/aws/aws-ingress-annotations/" class="post-preview">
            <div class="image">
                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1737362570/ingress-nginx-aws_hxoiqe.png">
                
            </div>
            <h3 class="title">What is AWS Ingress Annotations?</h3>
        </a>
    </div>
</div>

<style>
.toggle-preview {
    position: fixed;
    bottom: 20px;
    right: 20px;
    background: #333;
    color: white;
    border: none;
    padding: 8px 15px;
    border-radius: 4px;
    cursor: pointer;
    z-index: 1000;
    opacity: 0;
    transition: opacity 0.3s ease;
}

.toggle-preview:hover {
    background: #444;
}

.toggle-preview.visible {
    opacity: 1;
}

.recommendation {
    margin-top: 1000px;
    display: block;
    transition: all 0.3s ease;
}

.recommendation.hidden {
    display: none;
}

.hide-preview {
    margin-left: 10px;
    background: none;
    border: 1px solid #666;
    color: #666;
    padding: 5px 10px;
    border-radius: 4px;
    cursor: pointer;
}

.hide-preview:hover {
    background: #f0f0f0;
}

.preview-section {
    max-height: 1000px;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
}

.preview-section.hidden {
    max-height: 0;
}
</style>

<script>
function togglePreview() {
    const recommendation = document.getElementById('recommendationSection');
    const button = document.querySelector('.toggle-preview span');
    
    if (recommendation.classList.contains('hidden')) {
        recommendation.classList.remove('hidden');
        button.textContent = 'Hide Preview ▼';
    } else {
        recommendation.classList.add('hidden');
        button.textContent = 'Show Preview ▲';
    }
}

window.addEventListener('scroll', function() {
    const toggleButton = document.querySelector('.toggle-preview');
    const recommendation = document.getElementById('recommendationSection');
    const rect = recommendation.getBoundingClientRect();
    
    if (rect.top <= window.innerHeight) {
        toggleButton.classList.add('visible');
    } else {
        toggleButton.classList.remove('visible');
    }
});
</script>

        

        <!-- Show modal if the post is the last one -->
        

        <!-- Show modal before user leaves the page -->
        

        <!-- Add your newsletter subscription form here -->

        <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;A comprehensive guide to setting up and configuring NVIDIA A100 GPU servers on GCP&quot;%20https://somaz.blog/category/gcp/gcp-gpu-server-setup/%20via%20&#64;twitter_username&hashtags=gcp,gpu,nvidia,cloud-computing,machine-learning,a100"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://somaz.blog/category/gcp/gcp-gpu-server-setup/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

        

  <section class="author">
    <div class="details">
      
        <img class="img-rounded" src="/assets/img/uploads/profile.png" alt="Somaz">
      
      <p class="def">Author</p>
      <h3 class="name">
        <a href="/authors/somaz/">Somaz</a>
      </h3>
      <p class="desc">DevOps engineer focused on cloud infrastructure and automation</p>
      <p>
        
          <a href="https://github.com/somaz94" title="Github">
            <svg><use xlink:href="#icon-github"></use></svg>
          </a>
        
        
        
        
        
        
          <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
            <svg><use xlink:href="#icon-linkedin"></use></svg>
          </a>
        
        
          <a href="https://somaz.tistory.com" title="Tistory">
            <svg><use xlink:href="#icon-tistory"></use></svg>
          </a>
        
      </p>
    </div>
  </section>

  
  
  
  
  
  
  
  

  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Somaz",
      
      "image": "/assets/img/uploads/profile.png",
      
      "jobTitle": "DevOps Engineer",
      "url": "https://somaz.blog/authors/somaz/",
      "sameAs": [
        "https://github.com/somaz94","https://www.linkedin.com/in/somaz","https://{{ author.tistory_username }}.tistory.com"
      ]
  }
  </script>


        

<section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'https-somaz94-github-io';
        var disqus_title = '';
        var disqus_url = '/category/gcp/gcp-gpu-server-setup/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>



        <footer>
    <p>
      
        <a href="https://github.com/somaz94" title="Github">
          <svg><use xlink:href="#icon-github"></use></svg>
        </a>
      
      
        <a href="https://www.facebook.com/facebook_username" title="Facebook">
          <svg><use xlink:href="#icon-facebook"></use></svg>
        </a>
      
      
        <a href="https://twitter.com/twitter_username" title="Twitter">
          <svg><use xlink:href="#icon-twitter"></use></svg>
        </a>
      
      
        <a href="https://medium.com/@medium_username" title="Medium">
          <svg><use xlink:href="#icon-medium"></use></svg>
        </a>
      
      
        <a href="https://www.instagram.com/instagram_username" title="Instagram">
          <svg><use xlink:href="#icon-instagram"></use></svg>
        </a>
      
      
        <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
          <svg><use xlink:href="#icon-linkedin"></use></svg>
        </a>
      
      
        <a href="https://somaz.tistory.com" title="Tistory">
          <svg><use xlink:href="#icon-tistory"></use></svg>
        </a>
      
    </p>

    <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>


    <p>
      <a href="https://somaz.blog/sitemap.xml" title="sitemap">Sitemap</a> |
      <a href="https://somaz.blog/privacy-policy.html" title="Privacy Policy">Privacy Policy</a>
    </p>

    <p>
      <span>Somaz Tech Blog</span> <svg class="love"><use xlink:href="#icon-heart"></use></svg>
    </p>
</footer>










<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "somaz",
  "description": "DevOps engineer's tech blog.",
  "url": "https://somaz.blog/",
  "logo": {
      "@type": "ImageObject",
      "url": "https://somaz.blog/assets/img/icons/mediumtile.png",
      "width": "600",
      "height": "315"
  },
  "sameAs": [
    "https://github.com/somaz94","https://www.facebook.com/facebook_username","https://twitter.com/twitter_username","https://medium.com/@medium_username","https://www.instagram.com/instagram_username","https://www.linkedin.com/in/somaz","https://{{ site.tistory_username }}.tistory.com"
  ]
}
</script>

<!-- Include the script that allows Netlify CMS login -->
<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>

<!-- Include the website scripts -->
<script src="/assets/js/scripts.min.js"></script>

<!-- Include Google Analytics script -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  var host = window.location.hostname;
  if (host != 'localhost') {
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-XXXXXXXX-X');
  }
</script>
  


<!-- Include extra scripts -->



        

        
        
        
        
        
        
        
        
        <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "BlogPosting",
            "name": "Setting up GPU Server on Google Cloud Platform",
            "headline": "A complete guide to deploying and configuring NVIDIA A100 GPU instances on GCP",
            "description": "A comprehensive guide to setting up and configuring NVIDIA A100 GPU servers on GCP",
            "image": "https://res.cloudinary.com/dkcm26aem/image/upload/v1737358848/gpu_keixqz.png",
            "url": "https://somaz.blog/category/gcp/gcp-gpu-server-setup/",
            "articleBody": "



Overview

Google Cloud Platform (GCP) offers powerful GPU-accelerated computing instances that are ideal for AI/ML workloads, scientific computing, and rendering applications. This guide focuses on setting up and optimizing NVIDIA A100 GPU servers, which represent some of the most powerful GPU resources currently available in the cloud.

The NVIDIA A100 offers exceptional performance for deep learning training and inference, with variants including the 40GB and 80GB memory configurations. This guide walks through the entire process from checking GPU availability, requesting quota, deploying via Terraform, installing drivers and frameworks, to running popular AI applications.


  NVIDIA A100 Specifications
  
    
      Specification
      A100 40GB
      A100 80GB
    
    
      GPU Memory
      40GB HBM2
      80GB HBM2e
    
    
      Memory Bandwidth
      1.6 TB/s
      2.0 TB/s
    
    
      FP32 Performance
      19.5 TFLOPS
      19.5 TFLOPS
    
    
      Tensor Cores
      432
      432
    
    
      CUDA Cores
      6,912
      6,912
    
    
      TensorFloat-32 (TF32)
      156 TFLOPS
      156 TFLOPS
    
    
      GCP Machine Types
      a2-highgpu-*
      a2-ultragpu-*
    
  






GPU Availability and Quota

Checking GPU Availability by Region

Before deploying a GPU instance, you need to verify which GPU types are available in your preferred region and zones. The availability varies by region, and not all zones support all GPU types.

Official Documentation

  English Documentation
  Korean Documentation


Using gcloud CLI to Check Availability

# Check A100 availability in Seoul region
gcloud compute accelerator-types list --filter=&quot;name:nvidia-tesla-a100 AND zone:asia-northeast3&quot;
NAME               ZONE               DESCRIPTION
nvidia-tesla-a100  asia-northeast3-a  NVIDIA A100 40GB
nvidia-tesla-a100  asia-northeast3-b  NVIDIA A100 40GB

# Check A100 availability in all regions
gcloud compute accelerator-types list | grep a100
nvidia-a100-80gb       us-central1-a              NVIDIA A100 80GB
nvidia-tesla-a100      us-central1-a              NVIDIA A100 40GB
nvidia-tesla-a100      us-central1-b              NVIDIA A100 40GB
# ... more output ...



  
    graph TD
      A[GPU Selection Process] --&amp;gt; B{Determine Memory Needs}
      B --&amp;gt;|High Memory Requirements| C[A100 80GB]
      B --&amp;gt;|Standard Requirements| D[A100 40GB]
      C --&amp;gt; E{Check Zone Availability}
      D --&amp;gt; E
      E --&amp;gt; F[Request Quota Increase]
      F --&amp;gt; G[Deploy GPU Instance]
  




Requesting GPU Quota Increase

By default, GCP accounts have limited or no GPU quota. To use GPUs, especially high-end models like the A100, you need to request a quota increase:


  Navigate to IAM &amp;amp; Admin &amp;gt; Quotas &amp;amp; System Limits in the GCP Console
  Filter for “NVIDIA A100” or specific quota names
  Select the region(s) where you need quota
  Click “EDIT QUOTAS” and enter your requested limit
  Submit and provide business justification if prompted


Important Resources:

  GPU Quota Documentation
  Requesting Higher Quota
  Accelerator-optimized VMs
  Modifying GPU Count





  Quota Request Tips
  
    Be specific about your use case (AI training, inference, scientific computing)
    Request quota in regions with known availability
    Specify timeline and duration of GPU usage
    For large quota requests, be prepared to provide business justification
    Allow 1-2 business days for quota approval
  






GPU Pricing

GPU pricing varies significantly by type, region, and commitment level. Always check the current pricing before deployment.

Official Pricing Documentation

  English
  Korean


NVIDIA A100 Pricing Example (as of 2025)


  
    
      Machine Type
      GPU Configuration
      Monthly Cost
      Daily Cost
    
  
  
    
      a2-highgpu-1g
      1x A100 40GB
      ~₩3.95M
      ~₩120K
    
    
      a2-highgpu-2g
      2x A100 40GB
      ~₩7.9M
      ~₩240K
    
    
      a2-highgpu-4g
      4x A100 40GB
      ~₩15.8M
      ~₩480K
    
    
      a2-ultragpu-1g
      1x A100 80GB
      ~₩6.2M
      ~₩205K
    
    
      a2-ultragpu-2g
      2x A100 80GB
      ~₩12.4M
      ~₩410K
    
    
      a2-ultragpu-4g
      4x A100 80GB
      ~₩24.8M
      ~₩820K
    
  



  Cost Optimization Strategies
  
    Spot Instances: Use preemptible VMs for interruptible workloads (60-91% discount)
    Committed Use Discounts: 1-year or 3-year commitments for predictable workloads (20-57% discount)
    Right-sizing: Select appropriate GPU count based on actual needs
    Resource Scheduling: Create startup/shutdown schedules for non-production environments
    Monitoring: Set up budget alerts and utilization monitoring
  






Infrastructure Deployment with Terraform

Terraform allows for declarative, version-controlled deployment of GPU instances. Below is an example configuration for an A100 GPU server:

## ai_server ##
resource &quot;google_compute_address&quot; &quot;ai_server_ip&quot; {
  name   = var.ai_server_ip
  region = var.region
}

resource &quot;google_compute_instance&quot; &quot;ai_server&quot; {
  name                      = var.ai_server
  machine_type              = &quot;a2-highgpu-2g&quot; # a2-ultragpu-2g = A100 80G 2 / a2-highgpu-2g = A100 40G 2
  labels                    = local.default_labels
  zone                      = &quot;${var.region}-a&quot;
  allow_stopping_for_update = true

  tags = [var.nfs_client]

  boot_disk {
    initialize_params {
      image = &quot;ubuntu-os-cloud/ubuntu-2204-lts&quot;
      size  = 100
    }
  }

  metadata = {
    ssh-keys              = &quot;somaz:${file(&quot;../../key/ai-server.pub&quot;)}&quot;
    install-nvidia-driver = &quot;true&quot;
  }

  network_interface {
    network    = &quot;projects/${var.host_project}/global/networks/${var.shared_vpc}&quot;
    subnetwork = &quot;projects/${var.host_project}/regions/${var.region}/subnetworks/${var.subnet_share}-ai-b&quot;

    access_config {
      ## Include this section to give the VM an external ip ##
      nat_ip = google_compute_address.ai_server_ip.address
    }
  }

  scheduling {
    on_host_maintenance = &quot;TERMINATE&quot; # Required for GPU instances
    automatic_restart   = true
    preemptible         = false # Set to true for preemptible instances
  }

  guest_accelerator {
    type  = &quot;nvidia-tesla-a100&quot; # nvidia-a100-80gb = A100 80G / nvidia-tesla-a100 = A100 40G
    count = 2
  }

  depends_on = [google_compute_address.ai_server_ip]
}


Key Terraform Configuration Points


  Machine Type Selection:
    
      a2-highgpu-*g for A100 40GB (where * is the GPU count)
      a2-ultragpu-*g for A100 80GB (where * is the GPU count)
    
  
  Boot Disk:
    
      Use Ubuntu 20.04 or 22.04 for best compatibility
      Allocate sufficient disk space (100GB+) for datasets and model weights
    
  
  GPU-specific Settings:
    
      Set on_host_maintenance = &quot;TERMINATE&quot; (required for GPU VMs)
      Use guest_accelerator block to specify GPU type and count
      Consider setting metadata.install-nvidia-driver = &quot;true&quot; for automatic driver installation
    
  
  Networking:
    
      Consider whether your workload needs an external IP
      Configure appropriate firewall rules for your services
    
  






GPU Server Setup and Configuration

After deploying the VM, you need to install drivers, CUDA toolkit, and other necessary software.

Verifying GPU Availability

First, check if the GPUs are properly attached to the VM:

# OS Version
lsb_release -a

# Confirm GPU hardware detection
sudo lspci | grep -i nvidia
sudo lshw -c display


Installing Drivers and CUDA Toolkit

GCP can automatically install NVIDIA drivers if you set the install-nvidia-driver = &quot;true&quot; metadata. If you need to install manually:

# Install required packages
sudo apt update
sudo apt install -y ubuntu-drivers-common
sudo apt install -y nvidia-driver-535
sudo apt install -y nvidia-cuda-toolkit


Installing cuDNN

NVIDIA cuDNN is required for deep learning frameworks:

# Download cuDNN (requires NVIDIA Developer account)
# Example assumes you&apos;ve downloaded the archive
tar xvf cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz
cd cudnn-linux-x86_64-8.6.0.163_cuda11-archive

# Copy header files
sudo cp include/cudnn*.h /usr/include

# Copy library files
sudo cp lib/libcudnn* /usr/lib/x86_64-linux-gnu

# Set permissions and update library cache
sudo chmod a+r /usr/include/cudnn*.h /usr/lib/x86_64-linux-gnu/libcudnn*
sudo ldconfig


Verifying Installation

Verify that drivers, CUDA, and cuDNN are properly installed:

# Check NVIDIA driver
nvidia-smi

# Expected output example:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 535.104.05   Driver Version: 535.104.05   CUDA Version: 12.2     |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
# |                               |                      |               MIG M. |
# |===============================+======================+======================|
# |   0  NVIDIA A100-SXM...  On   | 00000000:00:04.0 Off |                    0 |
# | N/A   32C    P0    39W / 400W |      0MiB / 40960MiB |      0%      Default |
# |                               |                      |             Disabled |
# +-------------------------------+----------------------+----------------------+
# |   1  NVIDIA A100-SXM...  On   | 00000000:00:05.0 Off |                    0 |
# | N/A   34C    P0    41W / 400W |      0MiB / 40960MiB |      0%      Default |
# |                               |                      |             Disabled |
# +-------------------------------+----------------------+----------------------+

# Check CUDA version
nvcc --version

# Test cuDNN installation
cat &amp;lt;&amp;lt;EOF &amp;gt; cudnn_test.cpp
#include &amp;lt;cudnn.h&amp;gt;
#include &amp;lt;iostream&amp;gt;

int main() {
    cudnnHandle_t cudnn;
    cudnnCreate(&amp;amp;cudnn);
    std::cout &amp;lt;&amp;lt; &quot;CuDNN version: &quot; &amp;lt;&amp;lt; CUDNN_VERSION &amp;lt;&amp;lt; std::endl;
    cudnnDestroy(cudnn);
    return 0;
}
EOF

# Compile and run the cuDNN test
nvcc -o cudnn_test cudnn_test.cpp -lcudnn
./cudnn_test
# Expected output: CuDNN version: 8600


Setting Up NVIDIA Docker (Optional)

For containerized workloads, NVIDIA Docker allows GPU access from containers:









AI/ML Application Setup

With the GPU environment configured, you can now set up popular AI/ML applications.

Stable Diffusion WebUI

A popular interface for text-to-image generation:

# Clone repository and run installation script
wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
chmod +x webui.sh

# Start the WebUI with remote access enabled
./webui.sh --listen --xformers


Access the web UI at http://&amp;lt;your-vm-ip&amp;gt;:7860

Kohya_ss

A tool for training and fine-tuning Stable Diffusion models:

# Clone repository
git clone https://github.com/bmaltais/kohya_ss.git
cd kohya_ss

# Run installation script
./setup.sh

# Start the UI with remote access
./gui.sh --listen=0.0.0.0 --headless


Access the web UI at http://&amp;lt;your-vm-ip&amp;gt;:7860

ComfyUI

A node-based UI for Stable Diffusion:

# Clone repository
git clone https://github.com/comfyanonymous/ComfyUI.git
cd ComfyUI

# Install dependencies
pip install -r requirements.txt

# Start the UI with remote access
python3 main.py --listen 0.0.0.0


Access the web UI at http://&amp;lt;your-vm-ip&amp;gt;:8188

PyTorch and TensorFlow Performance Testing

Test the GPU performance with standard benchmarks:









Performance Optimization

NVIDIA A100 Performance Tuning


  GPU Compute Mode:
       sudo nvidia-smi -i 0 -c EXCLUSIVE_PROCESS  # For dedicated single process
    
  
  Memory Clock Optimization:
       sudo nvidia-smi -i 0 -ac 1215,1410  # Set memory and graphics clocks
    
  
  Persistent Mode (reduces initialization overhead):
       sudo nvidia-smi -i 0 -pm 1  # Enable persistent mode
    
  


System-level Optimizations


  Disable CPU Power Management:
       sudo apt install cpufrequtils
   sudo cpufreq-set -g performance
    
  
  I/O Optimization for dataset loading:
       echo &apos;vm.dirty_ratio = 10&apos; | sudo tee -a /etc/sysctl.conf
   echo &apos;vm.dirty_background_ratio = 5&apos; | sudo tee -a /etc/sysctl.conf
   sudo sysctl -p
    
  
  Memory Configuration for AI workloads:
       echo &apos;vm.overcommit_memory = 1&apos; | sudo tee -a /etc/sysctl.conf
   sudo sysctl -p
    
  


Framework-specific Optimizations


  PyTorch:
       torch.backends.cudnn.benchmark = True  # Auto-tuner for convolutions
   torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 for faster training
    
  
  TensorFlow:
       import tensorflow as tf
   tf.config.optimizer.set_jit(True)  # Enable XLA JIT compilation
   mixed_precision = tf.keras.mixed_precision.Policy(&apos;mixed_float16&apos;)
   tf.keras.mixed_precision.set_global_policy(mixed_precision)
    
  






Monitoring GPU Usage

NVIDIA System Management Interface

The primary tool for monitoring NVIDIA GPUs is nvidia-smi:

# Basic GPU status
nvidia-smi

# Continuous monitoring (updates every 1s)
nvidia-smi -l 1

# Show memory usage by process
nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv

# Monitor specific metrics
nvidia-smi --query-gpu=timestamp,name,pci.bus_id,utilization.gpu,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 5


DCGM (Data Center GPU Manager)

For production environments, NVIDIA DCGM provides more advanced monitoring:

# Install DCGM
sudo apt-get install -y datacenter-gpu-manager

# Start the service
sudo systemctl start dcgm

# Run DCGM diagnostics
sudo dcgmi diag -r 3

# Monitor GPU stats
sudo dcgmi dmon


Integrating with Prometheus and Grafana

For long-term monitoring and visualization:

# Install Prometheus NVIDIA exporter
git clone https://github.com/NVIDIA/dcgm-exporter.git
cd dcgm-exporter
make binary
sudo make install

# Start the exporter
sudo dcgm-exporter

# Configure Prometheus to scrape the exporter
# Add to prometheus.yml:
# - job_name: &apos;dcgm_exporter&apos;
#   static_configs:
#   - targets: [&apos;localhost:9400&apos;]



  
    graph LR
      A[NVIDIA GPU] --&amp;gt; B[DCGM Exporter]
      B --&amp;gt; C[Prometheus]
      C --&amp;gt; D[Grafana]
      D --&amp;gt; E[Alerts]
      D --&amp;gt; F[Dashboards]
  






Best Practices and Security


  GPU Server Best Practices
  
  1. GPU Selection:
  
    Choose appropriate GPU type based on memory requirements and computational needs
    Consider scalability - start small and scale up as needed
    Check zone availability before planning deployments
  
  
  2. Cost Optimization:
  
    Monitor usage patterns and adjust instance types accordingly
    Use preemptible instances for non-critical, fault-tolerant workloads
    Implement auto-shutdown for development environments
    Use committed use discounts for predictable workloads
  
  
  3. Security:
  
    Configure firewall rules to restrict access to necessary ports only
    Use service accounts with minimum required permissions
    Enable OS Login for centralized SSH access management
    Apply security patches promptly
    Enable Virtual Private Cloud (VPC) Service Controls for sensitive workloads
  
  
  4. Performance:
  
    Install the latest NVIDIA drivers compatible with your workload
    Configure appropriate CUDA and cuDNN versions
    Monitor GPU utilization and memory usage
    Implement proper cooling and power policies
    Use mixed precision training where possible
  
  
  5. Data Management:
  
    Use high-performance storage options for datasets (SSD, Local SSD)
    Implement data caching mechanisms
    Consider Cloud Storage with parallel transfers for large datasets
    Implement proper backup strategies for trained models
  






References


  GCP GPU Documentation
  NVIDIA A100 Tensor Core GPU
  CUDA Toolkit Documentation
  Terraform Infrastructure Example
  Stable Diffusion WebUI
  Kohya_ss
  ComfyUI
  NVIDIA DCGM Documentation

",
            "wordcount": "2549",
            "inLanguage": "en",
            "dateCreated": "2025-01-15/",
            "datePublished": "2025-01-15/",
            "dateModified": "2025-01-15/",
            "author": {
                "@type": "Person",
                "name": "Somaz",
                
                "image": "/assets/img/uploads/profile.png",
                
                "jobTitle": "DevOps Engineer",
                "url": "https://somaz.blog/authors/somaz/",
                "sameAs": [
                    "https://github.com/somaz94","https://www.linkedin.com/in/somaz"
                ]
            },
            "publisher": {
                "@type": "Organization",
                "name": "somaz",
                "url": "https://somaz.blog/",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://somaz.blog/assets/img/blog-image.png",
                    "width": "600",
                    "height": "315"
                }
            },
            "mainEntityOfPage": "True",
            "genre": "GCP",
            "articleSection": "GCP",
            "keywords": ["gcp","gpu","nvidia","cloud-computing","machine-learning","a100"]
        }
        </script>
    </body>
</html>
