<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    

    
    

    
    

    
    

    <!-- ✅ Google Tag Manager 추가 -->
    <script>
        (function(w,d,s,l,i){
            w[l]=w[l]||[];
            w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});
            var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';
            j.async=true;
            j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;
            f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer','GTM-MBP83N4Q');
    </script>
      <!-- ✅ End Google Tag Manager -->

    <!-- Mermaid.js 직접 로드 -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>

    <title>Building ETL Pipeline with BigQuery and Dataflow | somaz</title>
    <meta name="description" content="Learn how to build an ETL pipeline using MongoDB, BigQuery, and Google Sheets with Google Cloud Platform services">
    
        <meta name="keywords" content="bigquery, dataflow, etl, gcp, mongodb">
    

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Building ETL Pipeline with BigQuery and Dataflow | somaz">
    <meta name="twitter:description" content="Learn how to build an ETL pipeline using MongoDB, BigQuery, and Google Sheets with Google Cloud Platform services">

    
        <meta property="twitter:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1738719521/gcp-bigquery-dataflow_ikgarw.png">
    
    
    
        <meta name="twitter:site" content="@twitter_username">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="https://somaz.blog/category/gcp/bigquery-dataflow/">
    <meta property="og:title" content="Building ETL Pipeline with BigQuery and Dataflow | somaz">
    <meta property="og:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1738719521/gcp-bigquery-dataflow_ikgarw.png">
    <meta property="og:description" content="Learn how to build an ETL pipeline using MongoDB, BigQuery, and Google Sheets with Google Cloud Platform services">
    <meta property="og:site_name" content="Somaz Tech Blog">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="somaz">
    <meta name="msapplication-TileColor" content="#141414">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#141414">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:300,400,700" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="canonical" href="https://somaz.blog/category/gcp/bigquery-dataflow/">
    <link rel="alternate" type="application/rss+xml" title="Somaz Tech Blog" href="https://somaz.blog/feed.xml" />

    <!-- Include extra styles -->
    

    <!-- JavaScript enabled/disabled -->
    <script>
        document.querySelector('html').classList.remove('no-js');
    </script>

    <!-- Google Adsense -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8725590811736154"
        crossorigin="anonymous"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet"> -->
    <!-- <link href="https://cdn.jsdelivr.net/gh/sunn-us/SUIT/fonts/variable/woff2/SUIT-Variable.css" rel="stylesheet"> -->
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css2?family=Albert+Sans:wght@400;500;700&display=swap" rel="stylesheet"> -->

    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml" />

</head>
<!-- ✅ Google Tag Manager (noscript) -->
<noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MBP83N4Q"
            height="0" width="0" style="display:none;visibility:hidden">
    </iframe>
</noscript>
<!-- ✅ End Google Tag Manager (noscript) -->
    <body class="has-push-menu">
        





        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 1000 1000"><path d="M969.8,870.3c27,27.7,27,71.8,0,99.1C955.7,983,937.9,990,920,990c-17.9,0-35.7-7-49.7-20.7L500,599L129.6,969.4C115.6,983,97.8,990,79.9,990s-35.7-7-49.7-20.7c-27-27.3-27-71.4,0-99.1L400.9,500L30.3,129.3c-27-27.3-27-71.4,0-99.1c27.3-27,71.8-27,99.4,0L500,400.9L870.4,30.2c27.7-27,71.8-27,99.4,0c27,27.7,27,71.8,0,99.1L599.1,500L969.8,870.3z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-clock" viewBox="0 0 1000 1000"><path d="M500,10C229.8,10,10,229.8,10,500c0,270.2,219.8,490,490,490c270.2,0,490-219.8,490-490C990,229.8,770.2,10,500,10z M500,910.2c-226.2,0-410.2-184-410.2-410.2c0-226.2,184-410.2,410.2-410.2c226.2,0,410.2,184,410.2,410.2C910.2,726.1,726.2,910.2,500,910.2z M753.1,374c8.2,11.9,5.2,28.1-6.6,36.3L509.9,573.7c-4.4,3.1-9.6,4.6-14.8,4.6c-4.1,0-8.3-1-12.1-3c-8.6-4.5-14-13.4-14-23.1V202.5c0-14.4,11.7-26.1,26.1-26.1c14.4,0,26.1,11.7,26.1,26.1v300l195.6-135.1C728.7,359.2,744.9,362.1,753.1,374z"/></symbol><symbol id="icon-calendar" viewBox="0 0 1000 1000"><path d="M920,500v420H80V500H920 M990,430H10v490c0,38.7,31.3,70,70,70h840c38.7,0,70-31.3,70-70V430L990,430z"/><path d="M850,80v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80H360v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80C72.8,80,10,142.7,10,220v140h980V220C990,142.7,927.2,80,850,80z"/><path d="M255,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C290,25.8,274.3,10,255,10z"/><path d="M745,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C780,25.8,764.3,10,745,10z"/></symbol><symbol id="icon-github" viewBox="0 0 12 14"><path d="M6 1q1.633 0 3.012 0.805t2.184 2.184 0.805 3.012q0 1.961-1.145 3.527t-2.957 2.168q-0.211 0.039-0.312-0.055t-0.102-0.234q0-0.023 0.004-0.598t0.004-1.051q0-0.758-0.406-1.109 0.445-0.047 0.801-0.141t0.734-0.305 0.633-0.52 0.414-0.82 0.16-1.176q0-0.93-0.617-1.609 0.289-0.711-0.062-1.594-0.219-0.070-0.633 0.086t-0.719 0.344l-0.297 0.187q-0.727-0.203-1.5-0.203t-1.5 0.203q-0.125-0.086-0.332-0.211t-0.652-0.301-0.664-0.105q-0.352 0.883-0.062 1.594-0.617 0.68-0.617 1.609 0 0.664 0.16 1.172t0.41 0.82 0.629 0.523 0.734 0.305 0.801 0.141q-0.305 0.281-0.383 0.805-0.164 0.078-0.352 0.117t-0.445 0.039-0.512-0.168-0.434-0.488q-0.148-0.25-0.379-0.406t-0.387-0.187l-0.156-0.023q-0.164 0-0.227 0.035t-0.039 0.090 0.070 0.109 0.102 0.094l0.055 0.039q0.172 0.078 0.34 0.297t0.246 0.398l0.078 0.18q0.102 0.297 0.344 0.48t0.523 0.234 0.543 0.055 0.434-0.027l0.18-0.031q0 0.297 0.004 0.691t0.004 0.426q0 0.141-0.102 0.234t-0.312 0.055q-1.812-0.602-2.957-2.168t-1.145-3.527q0-1.633 0.805-3.012t2.184-2.184 3.012-0.805zM2.273 9.617q0.023-0.055-0.055-0.094-0.078-0.023-0.102 0.016-0.023 0.055 0.055 0.094 0.070 0.047 0.102-0.016zM2.516 9.883q0.055-0.039-0.016-0.125-0.078-0.070-0.125-0.023-0.055 0.039 0.016 0.125 0.078 0.078 0.125 0.023zM2.75 10.234q0.070-0.055 0-0.148-0.062-0.102-0.133-0.047-0.070 0.039 0 0.141t0.133 0.055zM3.078 10.562q0.062-0.062-0.031-0.148-0.094-0.094-0.156-0.023-0.070 0.062 0.031 0.148 0.094 0.094 0.156 0.023zM3.523 10.758q0.023-0.086-0.102-0.125-0.117-0.031-0.148 0.055t0.102 0.117q0.117 0.047 0.148-0.047zM4.016 10.797q0-0.102-0.133-0.086-0.125 0-0.125 0.086 0 0.102 0.133 0.086 0.125 0 0.125-0.086zM4.469 10.719q-0.016-0.086-0.141-0.070-0.125 0.023-0.109 0.117t0.141 0.062 0.109-0.109z"></path></symbol><symbol id="icon-medium" viewBox="0 0 1000 1000"><path d="M336.5,240.2v641.5c0,9.1-2.3,16.9-6.8,23.2s-11.2,9.6-20,9.6c-6.2,0-12.2-1.5-18-4.4L37.3,782.7c-7.7-3.6-14.1-9.8-19.4-18.3S10,747.4,10,739V115.5c0-7.3,1.8-13.5,5.5-18.6c3.6-5.1,8.9-7.7,15.9-7.7c5.1,0,13.1,2.7,24.1,8.2l279.5,140C335.9,238.6,336.5,239.5,336.5,240.2L336.5,240.2z M371.5,295.5l292,473.6l-292-145.5V295.5z M990,305.3v576.4c0,9.1-2.6,16.5-7.7,22.1c-5.1,5.7-12,8.5-20.8,8.5s-17.3-2.4-25.7-7.1L694.7,784.9L990,305.3z M988.4,239.7c0,1.1-46.8,77.6-140.3,229.4C754.6,621,699.8,709.8,683.8,735.7L470.5,389l177.2-288.2c6.2-10.2,15.7-15.3,28.4-15.3c5.1,0,9.8,1.1,14.2,3.3l295.9,147.7C987.6,237.1,988.4,238.2,988.4,239.7L988.4,239.7z"/></symbol><symbol id="icon-instagram" viewBox="0 0 489.84 489.84"><path d="M249.62,50.46c65.4,0,73.14.25,99,1.43C372.47,53,385.44,57,394.07,60.32a75.88,75.88,0,0,1,28.16,18.32,75.88,75.88,0,0,1,18.32,28.16c3.35,8.63,7.34,21.6,8.43,45.48,1.18,25.83,1.43,33.57,1.43,99s-0.25,73.14-1.43,99c-1.09,23.88-5.08,36.85-8.43,45.48a81.11,81.11,0,0,1-46.48,46.48c-8.63,3.35-21.6,7.34-45.48,8.43-25.82,1.18-33.57,1.43-99,1.43s-73.15-.25-99-1.43c-23.88-1.09-36.85-5.08-45.48-8.43A75.88,75.88,0,0,1,77,423.86,75.88,75.88,0,0,1,58.69,395.7c-3.35-8.63-7.34-21.6-8.43-45.48-1.18-25.83-1.43-33.57-1.43-99s0.25-73.14,1.43-99c1.09-23.88,5.08-36.85,8.43-45.48A75.88,75.88,0,0,1,77,78.64a75.88,75.88,0,0,1,28.16-18.32c8.63-3.35,21.6-7.34,45.48-8.43,25.83-1.18,33.57-1.43,99-1.43m0-44.13c-66.52,0-74.86.28-101,1.47s-43.87,5.33-59.45,11.38A120.06,120.06,0,0,0,45.81,47.44,120.06,120.06,0,0,0,17.56,90.82C11.5,106.4,7.36,124.2,6.17,150.27s-1.47,34.46-1.47,101,0.28,74.86,1.47,101,5.33,43.87,11.38,59.45a120.06,120.06,0,0,0,28.25,43.38,120.06,120.06,0,0,0,43.38,28.25c15.58,6.05,33.38,10.19,59.45,11.38s34.46,1.47,101,1.47,74.86-.28,101-1.47,43.87-5.33,59.45-11.38a125.24,125.24,0,0,0,71.63-71.63c6.05-15.58,10.19-33.38,11.38-59.45s1.47-34.46,1.47-101-0.28-74.86-1.47-101-5.33-43.87-11.38-59.45a120.06,120.06,0,0,0-28.25-43.38,120.06,120.06,0,0,0-43.38-28.25C394.47,13.13,376.67,9,350.6,7.8s-34.46-1.47-101-1.47h0Z" transform="translate(-4.7 -6.33)" /><path d="M249.62,125.48A125.77,125.77,0,1,0,375.39,251.25,125.77,125.77,0,0,0,249.62,125.48Zm0,207.41a81.64,81.64,0,1,1,81.64-81.64A81.64,81.64,0,0,1,249.62,332.89Z" transform="translate(-4.7 -6.33)"/><circle cx="375.66" cy="114.18" r="29.39" /></symbol><symbol id="icon-linkedin" viewBox="0 0 12 14"><path d="M2.727 4.883v7.742h-2.578v-7.742h2.578zM2.891 2.492q0.008 0.57-0.395 0.953t-1.059 0.383h-0.016q-0.641 0-1.031-0.383t-0.391-0.953q0-0.578 0.402-0.957t1.051-0.379 1.039 0.379 0.398 0.957zM12 8.187v4.437h-2.57v-4.141q0-0.82-0.316-1.285t-0.988-0.465q-0.492 0-0.824 0.27t-0.496 0.668q-0.086 0.234-0.086 0.633v4.32h-2.57q0.016-3.117 0.016-5.055t-0.008-2.313l-0.008-0.375h2.57v1.125h-0.016q0.156-0.25 0.32-0.438t0.441-0.406 0.68-0.34 0.895-0.121q1.336 0 2.148 0.887t0.813 2.598z"></path></symbol><symbol id="icon-heart" viewBox="0 0 34 30"><path d="M17,29.7 L16.4,29.2 C3.5,18.7 0,15 0,9 C0,4 4,0 9,0 C13.1,0 15.4,2.3 17,4.1 C18.6,2.3 20.9,0 25,0 C30,0 34,4 34,9 C34,15 30.5,18.7 17.6,29.2 L17,29.7 Z M9,2 C5.1,2 2,5.1 2,9 C2,14.1 5.2,17.5 17,27.1 C28.8,17.5 32,14.1 32,9 C32,5.1 28.9,2 25,2 C21.5,2 19.6,4.1 18.1,5.8 L17,7.1 L15.9,5.8 C14.4,4.1 12.5,2 9,2 Z" id="Shape"></path></symbol><symbol id="icon-arrow-right" viewBox="0 0 25.452 25.452"><path d="M4.471,24.929v-2.004l12.409-9.788c0.122-0.101,0.195-0.251,0.195-0.411c0-0.156-0.073-0.31-0.195-0.409L4.471,2.526V0.522c0-0.2,0.115-0.384,0.293-0.469c0.18-0.087,0.396-0.066,0.552,0.061l15.47,12.202c0.123,0.1,0.195,0.253,0.195,0.409c0,0.16-0.072,0.311-0.195,0.411L5.316,25.34c-0.155,0.125-0.372,0.147-0.552,0.061C4.586,25.315,4.471,25.13,4.471,24.929z"/></symbol><symbol id="icon-star" viewBox="0 0 48 48"><path fill="currentColor" d="M44,24c0,11.045-8.955,20-20,20S4,35.045,4,24S12.955,4,24,4S44,12.955,44,24z"/><path fill="#ffffff" d="M24,11l3.898,7.898l8.703,1.301l-6.301,6.102l1.5,8.699L24,30.898L16.199,35l1.5-8.699l-6.301-6.102  l8.703-1.301L24,11z"/></symbol><symbol id="icon-read" viewBox="0 0 32 32"><path fill="currentColor" d="M29,4H3C1.343,4,0,5.343,0,7v18c0,1.657,1.343,3,3,3h10c0,0.552,0.448,1,1,1h4c0.552,0,1-0.448,1-1h10  c1.657,0,3-1.343,3-3V7C32,5.343,30.657,4,29,4z M29,5v20H18.708c-0.618,0-1.236,0.146-1.789,0.422l-0.419,0.21V5H29z M15.5,5  v20.632l-0.419-0.21C14.528,25.146,13.91,25,13.292,25H3V5H15.5z M31,25c0,1.103-0.897,2-2,2H18v1h-4v-1H3c-1.103,0-2-0.897-2-2V7  c0-0.737,0.405-1.375,1-1.722V25c0,0.552,0.448,1,1,1h10.292c0.466,0,0.925,0.108,1.342,0.317l0.919,0.46  c0.141,0.07,0.294,0.106,0.447,0.106c0.153,0,0.306-0.035,0.447-0.106l0.919-0.46C17.783,26.108,18.242,26,18.708,26H29  c0.552,0,1-0.448,1-1V5.278C30.595,5.625,31,6.263,31,7V25z M6,12.5C6,12.224,6.224,12,6.5,12h5c0.276,0,0.5,0.224,0.5,0.5  S11.776,13,11.5,13h-5C6.224,13,6,12.776,6,12.5z M6,14.5C6,14.224,6.224,14,6.5,14h5c0.276,0,0.5,0.224,0.5,0.5S11.776,15,11.5,15  h-5C6.224,15,6,14.776,6,14.5z M6,16.5C6,16.224,6.224,16,6.5,16h5c0.276,0,0.5,0.224,0.5,0.5S11.776,17,11.5,17h-5  C6.224,17,6,16.776,6,16.5z M20,12.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,13,25.5,13h-5  C20.224,13,20,12.776,20,12.5z M20,14.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,15,25.5,15h-5  C20.224,15,20,14.776,20,14.5z M20,16.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,17,25.5,17h-5  C20.224,17,20,16.776,20,16.5z"></path></symbol><symbol id="icon-tistory" viewBox="0 0 24 24"><path d="M4 4h16v3h-6v13h-4V7H4V4z"/></symbol></defs></svg>

        <header class="bar-header">
    <a id="menu" role="button">
        <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    </a>
    <h1 class="logo">
        <a href="/">
            
                somaz <span class="version">v3.1.2</span>
            
        </a>
    </h1>
    <a id="search" class="dosearch" role="button">
        <svg class="icon-search"><use xlink:href="#icon-search"></use></svg>
    </a>
    
        <a href="https://github.com/thiagorossener/jekflix-template" class="get-theme" role="button">
            Get this theme!
        </a>
    
</header>

<div id="mask" class="overlay"></div>

<aside class="sidebar" id="sidebar">
    <nav id="navigation">
      <h2>Menu</h2>
      <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>

    </nav>
</aside>

<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>



        <section class="post two-columns">
            <article role="article" class="post-content">
                <p class="post-info">
                    
                        <svg class="icon-calendar" id="date"><use xlink:href="#icon-calendar"></use></svg>
                        <time class="date" datetime="2025-02-11T05:00:00+00:00">
                            


February 11, 2025

                        </time>
                    
                    <svg id="clock" class="icon-clock"><use xlink:href="#icon-clock"></use></svg>
                    <span>17 min to read</span>
                </p>
                <h1 class="post-title">Building ETL Pipeline with BigQuery and Dataflow</h1>
                <p class="post-subtitle">A comprehensive guide to data ETL process using GCP services</p>

                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1738719521/gcp-bigquery-dataflow_ikgarw.png" alt="Featured image" class="post-cover">
                

                <!-- Pagination links -->



                <!-- Add your table of contents here -->


                <p><a href="https://medium.com/analytics-vidhya/etl-with-dataflow-bigquery-bfaf22fbd0d0">Image Reference</a></p>

<p><br /></p>

<hr />

<h2 id="overview">Overview</h2>

<p>Modern data pipelines are essential for organizations to transform raw data into actionable insights. Following our previous post about <a href="https://somaz94.github.io/category/gcp/bigquery/">BigQuery</a>, this guide explores how to build a robust ETL (Extract, Transform, Load) pipeline using Google Cloud Platform’s managed services, focusing on moving data from MongoDB to BigQuery and finally to Google Sheets for analysis and visualization.</p>

<p>This comprehensive tutorial will guide you through building a fully automated data pipeline with the following flow:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MongoDB → BigQuery → Google Sheets
</code></pre></div></div>

<p><br /></p>

<div class="info-box info-box-default-not-check">
  <strong>Understanding ETL</strong>
  <p>ETL (Extract, Transform, Load) is a three-phase process for integrating data from multiple sources into a single data repository:</p>
  <ul>
    <li><strong>Extract</strong>: Retrieving data from source systems (databases, APIs, files)</li>
    <li><strong>Transform</strong>: Converting data to appropriate format with operations like filtering, sorting, aggregating, joining, cleaning, and validation</li>
    <li><strong>Load</strong>: Writing the transformed data to a target destination (data warehouse, database, file)</li>
  </ul>
  <p>Cloud-based ETL solutions like the one we're building offer advantages including scalability, reduced maintenance, cost-effectiveness, and integration with other cloud services.</p>
</div>

<p><br /></p>

<hr />

<h2 id="gcp-services-overview">GCP Services Overview</h2>

<p>Our ETL pipeline leverages several key Google Cloud Platform services:</p>

<div class="table-container">
  <table class="table-beauty">
    <tr>
      <th>Service</th>
      <th>Role in Pipeline</th>
      <th>Key Capabilities</th>
    </tr>
    <tr>
      <td>Cloud Dataflow</td>
      <td>Data processing engine</td>
      <td>Batch/stream processing, auto-scaling, managed infrastructure</td>
    </tr>
    <tr>
      <td>BigQuery</td>
      <td>Data warehouse</td>
      <td>Serverless storage and analytics, SQL interface, high-performance</td>
    </tr>
    <tr>
      <td>Cloud Functions</td>
      <td>Serverless code execution</td>
      <td>Event-driven processing, automatic scaling, pay-per-use</td>
    </tr>
    <tr>
      <td>Cloud Scheduler</td>
      <td>Job scheduling</td>
      <td>Managed cron jobs, reliable triggering, timezone support</td>
    </tr>
    <tr>
      <td>Cloud Storage</td>
      <td>Function code storage</td>
      <td>Highly durable object storage, global access, integration with other services</td>
    </tr>
  </table>
</div>

<p><br /></p>

<div style="width: 100%; margin: auto; margin-top: 20px;">
  <div class="mermaid">
    graph LR
      A[MongoDB] --&gt;|Extract| B[Cloud Functions]
      B --&gt;|Transform| C[BigQuery]
      C --&gt;|Extract| D[Cloud Functions]
      D --&gt;|Transform &amp; Load| E[Google Sheets]
      F[Cloud Scheduler] --&gt;|Trigger| B
      F --&gt;|Trigger| D
      G[Cloud Storage] --&gt;|Store Code| B
      G --&gt;|Store Code| D
  </div>
</div>

<p><br /></p>

<hr />

<h2 id="cloud-dataflow-deep-dive">Cloud Dataflow Deep Dive</h2>

<p><br /></p>

<h3 id="what-is-google-cloud-dataflow">What is Google Cloud Dataflow?</h3>

<p>Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines within the Google Cloud Platform ecosystem. It handles both batch and streaming data processing scenarios with automatic resource management and dynamic work rebalancing.</p>

<p><br /></p>

<h3 id="dataflow-architecture">Dataflow Architecture</h3>

<p>When a Dataflow job is launched, the service:</p>

<ol>
  <li>Takes your pipeline code (Apache Beam)</li>
  <li>Optimizes it into an execution graph</li>
  <li>Splits the workload into multiple steps</li>
  <li>Distributes these steps across Compute Engine instances</li>
  <li>Manages scaling, fault tolerance, and worker communication</li>
</ol>

<p><strong>This abstraction allows developers to focus on pipeline logic rather than infrastructure management.</strong></p>

<p><br /></p>

<h3 id="key-features-and-benefits">Key Features and Benefits</h3>

<ol>
  <li><strong>Unified Programming Model</strong>
    <ul>
      <li>Apache Beam SDK provides a single programming model for both batch and streaming</li>
      <li>Same code can run on multiple execution engines</li>
      <li>Supported languages include Java, Python, Go, and more</li>
    </ul>
  </li>
  <li><strong>No-Ops Data Processing</strong>
    <ul>
      <li>Fully managed service with automatic provisioning and scaling</li>
      <li>Hands-off infrastructure management</li>
      <li>Dynamic work rebalancing for optimal throughput</li>
      <li>Built-in monitoring and logging</li>
    </ul>
  </li>
  <li><strong>Advanced Processing Capabilities</strong>
    <ul>
      <li>Native handling of late and out-of-order data</li>
      <li>Exactly-once processing guarantees</li>
      <li>Windowing for time-based aggregations</li>
      <li>Watermarking for tracking event time progress</li>
    </ul>
  </li>
  <li><strong>Performance and Scalability</strong>
    <ul>
      <li>Horizontal scaling to handle varying workloads</li>
      <li>Fusion optimization to minimize overhead</li>
      <li>Auto-tuning of worker resources</li>
      <li>Support for persistent disks and SSDs for performance-sensitive operations</li>
    </ul>
  </li>
</ol>

<div style="width: 100%; margin: auto; margin-top: 20px;">
  <div class="mermaid">
    graph TD
      A[Beam SDK Code] --&gt; B[Dataflow Service]
      B --&gt; C{Job Execution}
      C --&gt; D[Worker Pool]
      D --&gt; E[Worker 1]
      D --&gt; F[Worker 2]
      D --&gt; G[Worker N]
      B &lt;--&gt;|Metrics/Logs| H[Monitoring &amp; Logging]
      B &lt;--&gt;|Status/Control| I[Job Management]
  </div>
</div>

<p><br /></p>

<h3 id="sample-dataflow-pipeline-code">Sample Dataflow Pipeline Code</h3>

<p>Here’s a simplified example of a Python Dataflow pipeline that reads from MongoDB, performs transformations, and writes to BigQuery:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">apache_beam</span> <span class="k">as</span> <span class="n">beam</span>
<span class="kn">from</span> <span class="nn">apache_beam.options.pipeline_options</span> <span class="kn">import</span> <span class="n">PipelineOptions</span>
<span class="kn">from</span> <span class="nn">apache_beam.io.mongodbio</span> <span class="kn">import</span> <span class="n">ReadFromMongoDB</span>
<span class="kn">from</span> <span class="nn">apache_beam.io.gcp.bigquery</span> <span class="kn">import</span> <span class="n">WriteToBigQuery</span>

<span class="c1"># Define pipeline options including GCP project and Dataflow runner
</span><span class="n">options</span> <span class="o">=</span> <span class="n">PipelineOptions</span><span class="p">([</span>
    <span class="s">'--project=your-gcp-project'</span><span class="p">,</span>
    <span class="s">'--region=us-central1'</span><span class="p">,</span>
    <span class="s">'--runner=DataflowRunner'</span><span class="p">,</span>
    <span class="s">'--temp_location=gs://your-bucket/temp'</span><span class="p">,</span>
    <span class="s">'--staging_location=gs://your-bucket/staging'</span>
<span class="p">])</span>

<span class="c1"># Schema for BigQuery table
</span><span class="n">schema</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'fields'</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'id'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'STRING'</span><span class="p">},</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'name'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'STRING'</span><span class="p">},</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'value'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'FLOAT'</span><span class="p">},</span>
        <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'timestamp'</span><span class="p">,</span> <span class="s">'type'</span><span class="p">:</span> <span class="s">'TIMESTAMP'</span><span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>

<span class="c1"># MongoDB connection URI
</span><span class="n">uri</span> <span class="o">=</span> <span class="s">"mongodb+srv://username:password@cluster.mongodb.net"</span>

<span class="c1"># Create and run the pipeline
</span><span class="k">with</span> <span class="n">beam</span><span class="p">.</span><span class="n">Pipeline</span><span class="p">(</span><span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">)</span> <span class="k">as</span> <span class="n">p</span><span class="p">:</span>
    <span class="p">(</span><span class="n">p</span> 
     <span class="o">|</span> <span class="s">'ReadFromMongoDB'</span> <span class="o">&gt;&gt;</span> <span class="n">ReadFromMongoDB</span><span class="p">(</span>
         <span class="n">uri</span><span class="o">=</span><span class="n">uri</span><span class="p">,</span>
         <span class="n">db</span><span class="o">=</span><span class="s">'your_database'</span><span class="p">,</span>
         <span class="n">coll</span><span class="o">=</span><span class="s">'your_collection'</span><span class="p">)</span>
     <span class="o">|</span> <span class="s">'TransformData'</span> <span class="o">&gt;&gt;</span> <span class="n">beam</span><span class="p">.</span><span class="n">Map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">doc</span><span class="p">:</span> <span class="p">{</span>
         <span class="s">'id'</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">doc</span><span class="p">[</span><span class="s">'_id'</span><span class="p">]),</span>
         <span class="s">'name'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="s">''</span><span class="p">),</span>
         <span class="s">'value'</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'value'</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
         <span class="s">'timestamp'</span><span class="p">:</span> <span class="n">doc</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'created_at'</span><span class="p">).</span><span class="n">timestamp</span><span class="p">()</span>
     <span class="p">})</span>
     <span class="o">|</span> <span class="s">'WriteToBigQuery'</span> <span class="o">&gt;&gt;</span> <span class="n">WriteToBigQuery</span><span class="p">(</span>
         <span class="n">table</span><span class="o">=</span><span class="s">'your-project:your_dataset.your_table'</span><span class="p">,</span>
         <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">,</span>
         <span class="n">create_disposition</span><span class="o">=</span><span class="n">beam</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="n">BigQueryDisposition</span><span class="p">.</span><span class="n">CREATE_IF_NEEDED</span><span class="p">,</span>
         <span class="n">write_disposition</span><span class="o">=</span><span class="n">beam</span><span class="p">.</span><span class="n">io</span><span class="p">.</span><span class="n">BigQueryDisposition</span><span class="p">.</span><span class="n">WRITE_TRUNCATE</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="cloud-functions-and-scheduler">Cloud Functions and Scheduler</h2>

<p><br /></p>

<h3 id="cloud-functions">Cloud Functions</h3>

<p>Google Cloud Functions is a lightweight, event-driven, serverless compute platform that allows you to create single-purpose, stand-alone functions that respond to cloud events without managing the underlying infrastructure.</p>

<div class="info-box info-box-default-not-check">
  <h3>When to Use Cloud Functions vs. Dataflow</h3>
  <p>In our ETL pipeline, we're using Cloud Functions for orchestration and simpler data movement tasks, while Dataflow would be better suited for:</p>
  <ul>
    <li>Processing very large datasets (TB+)</li>
    <li>Complex data transformations requiring parallel processing</li>
    <li>Streaming data scenarios with windowing requirements</li>
    <li>Pipelines that need sophisticated fault tolerance</li>
  </ul>
  <p>Cloud Functions is ideal for event-triggered actions, lightweight processing, and gluing together different GCP services, as we're using it in this workflow.</p>
</div>

<p><br /></p>

<h4 id="key-features">Key Features</h4>

<ul>
  <li><strong>Event-Driven</strong>: Triggers include HTTP requests, Cloud Storage events, Pub/Sub messages, and Firestore events</li>
  <li><strong>Stateless</strong>: Each invocation is independent, with no persistent state between executions</li>
  <li><strong>Automatic Scaling</strong>: Scales from zero to many instances based on load</li>
  <li><strong>Runtime Support</strong>: Node.js, Python, Go, Java, .NET, Ruby, and PHP</li>
  <li><strong>Pay-per-use</strong>: Billed by compute time in 100ms increments</li>
</ul>

<p><br /></p>

<h3 id="cloud-scheduler">Cloud Scheduler</h3>

<p>Google Cloud Scheduler is a fully managed enterprise-grade cron job scheduler that allows you to schedule virtually any job, including batch jobs, big data jobs, cloud infrastructure operations, and more.</p>

<p><br /></p>

<h4 id="key-features-1">Key Features</h4>

<ul>
  <li><strong>Reliability</strong>: Google-grade reliability with SLA</li>
  <li><strong>Cron Syntax</strong>: Familiar unix-cron format for job definitions</li>
  <li><strong>Flexible Targets</strong>: HTTP/S endpoints, Pub/Sub topics, App Engine applications</li>
  <li><strong>Authentication</strong>: Support for OIDC tokens, basic auth, and OAuth</li>
  <li><strong>Monitoring</strong>: Integrated with Cloud Monitoring for observability</li>
  <li><strong>Retry Logic</strong>: Configurable retry on failed jobs</li>
</ul>

<p><br /></p>

<h3 id="example-code-mongodb-to-bigquery-function">Example Code: MongoDB to BigQuery Function</h3>

<p>Here’s a sample Cloud Function that extracts data from MongoDB and loads it into BigQuery:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pymongo</span>
<span class="kn">from</span> <span class="nn">google.cloud</span> <span class="kn">import</span> <span class="n">bigquery</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="k">def</span> <span class="nf">mongodb_to_bigquery</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
    <span class="c1"># MongoDB connection
</span>    <span class="n">client</span> <span class="o">=</span> <span class="n">pymongo</span><span class="p">.</span><span class="n">MongoClient</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'MONGODB_URI'</span><span class="p">))</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">client</span><span class="p">[</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'MONGODB_DB'</span><span class="p">)]</span>
    <span class="n">collection</span> <span class="o">=</span> <span class="n">db</span><span class="p">[</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'MONGODB_COLLECTION'</span><span class="p">)]</span>
    
    <span class="c1"># Get data from MongoDB
</span>    <span class="n">cursor</span> <span class="o">=</span> <span class="n">collection</span><span class="p">.</span><span class="n">find</span><span class="p">({})</span>
    
    <span class="c1"># Transform data for BigQuery
</span>    <span class="n">rows</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">cursor</span><span class="p">:</span>
        <span class="c1"># Convert MongoDB ObjectId to string
</span>        <span class="n">document</span><span class="p">[</span><span class="s">'_id'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">document</span><span class="p">[</span><span class="s">'_id'</span><span class="p">])</span>
        
        <span class="c1"># Convert any datetime objects to string
</span>        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">document</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">datetime</span><span class="p">):</span>
                <span class="n">document</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="p">.</span><span class="n">isoformat</span><span class="p">()</span>
        
        <span class="n">rows</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
    
    <span class="c1"># BigQuery client
</span>    <span class="n">client</span> <span class="o">=</span> <span class="n">bigquery</span><span class="p">.</span><span class="n">Client</span><span class="p">()</span>
    <span class="n">table_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'BIGQUERY_PROJECT'</span><span class="p">)</span><span class="si">}</span><span class="s">.</span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'BIGQUERY_DATASET'</span><span class="p">)</span><span class="si">}</span><span class="s">.</span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'BIGQUERY_TABLE'</span><span class="p">)</span><span class="si">}</span><span class="s">"</span>
    
    <span class="c1"># Load data into BigQuery
</span>    <span class="n">errors</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">insert_rows_json</span><span class="p">(</span><span class="n">table_id</span><span class="p">,</span> <span class="n">rows</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">errors</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s">"Encountered errors: </span><span class="si">{</span><span class="n">errors</span><span class="si">}</span><span class="s">"</span><span class="p">,</span> <span class="mi">500</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s">"Successfully loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span><span class="si">}</span><span class="s"> rows to </span><span class="si">{</span><span class="n">table_id</span><span class="si">}</span><span class="s">"</span><span class="p">,</span> <span class="mi">200</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="example-code-bigquery-to-google-sheets-function">Example Code: BigQuery to Google Sheets Function</h3>

<p>Here’s a sample Cloud Function that extracts data from BigQuery and loads it into Google Sheets:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">gspread</span>
<span class="kn">from</span> <span class="nn">google.cloud</span> <span class="kn">import</span> <span class="n">bigquery</span>
<span class="kn">from</span> <span class="nn">google.oauth2</span> <span class="kn">import</span> <span class="n">service_account</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="k">def</span> <span class="nf">bigquery_to_sheets</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
    <span class="c1"># BigQuery setup
</span>    <span class="n">bq_client</span> <span class="o">=</span> <span class="n">bigquery</span><span class="p">.</span><span class="n">Client</span><span class="p">()</span>
    
    <span class="c1"># Query data from BigQuery
</span>    <span class="n">query</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"""
    SELECT *
    FROM `</span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'BIGQUERY_PROJECT'</span><span class="p">)</span><span class="si">}</span><span class="s">.</span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'BIGQUERY_DATASET'</span><span class="p">)</span><span class="si">}</span><span class="s">.</span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'BIGQUERY_TABLE'</span><span class="p">)</span><span class="si">}</span><span class="s">`
    LIMIT 1000
    """</span>
    
    <span class="n">query_job</span> <span class="o">=</span> <span class="n">bq_client</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="n">query_job</span><span class="p">.</span><span class="n">result</span><span class="p">()</span>
    
    <span class="c1"># Transform data for Google Sheets
</span>    <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="n">field</span><span class="p">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">query_job</span><span class="p">.</span><span class="n">schema</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">headers</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span>
        <span class="n">data</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">field</span><span class="p">])</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">headers</span><span class="p">])</span>
    
    <span class="c1"># Google Sheets setup
</span>    <span class="n">credentials</span> <span class="o">=</span> <span class="n">service_account</span><span class="p">.</span><span class="n">Credentials</span><span class="p">.</span><span class="n">from_service_account_file</span><span class="p">(</span>
        <span class="s">'bigquery.json'</span><span class="p">,</span>
        <span class="n">scopes</span><span class="o">=</span><span class="p">[</span><span class="s">'https://www.googleapis.com/auth/spreadsheets'</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">gc</span> <span class="o">=</span> <span class="n">gspread</span><span class="p">.</span><span class="n">authorize</span><span class="p">(</span><span class="n">credentials</span><span class="p">)</span>
    
    <span class="c1"># Open the spreadsheet and write data
</span>    <span class="n">sheet</span> <span class="o">=</span> <span class="n">gc</span><span class="p">.</span><span class="n">open_by_key</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'SHEET_ID'</span><span class="p">)).</span><span class="n">worksheet</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'SHEET_NAME'</span><span class="p">))</span>
    <span class="n">sheet</span><span class="p">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="n">sheet</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="s">'A1'</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="sa">f</span><span class="s">"Successfully updated Google Sheet with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s"> rows of data"</span><span class="p">,</span> <span class="mi">200</span>
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="complete-etl-workflow-implementation">Complete ETL Workflow Implementation</h2>

<p>Our complete ETL workflow follows this sequence:</p>

<div style="width: 100%; margin: auto; margin-top: 20px;">
  <div class="mermaid">
    flowchart TD
        Z["Enable API Service"] --&gt; A["Create BigQuery Service Account and Grant Permission"]
        A --&gt; B["Create Google Sheet and Grant Permission"]
        B --&gt; C["Create BigQuery Dataset"]
        C --&gt; D["Create Storage for Cloud-function"]
        D --&gt; E1["mongodb → bigquery table workflow"]
        E1 --&gt; F1["Compress Zip file (source code)"]
        E1 --&gt; G1["Upload Zip file (source code) to bucket"]
        E1 --&gt; H1["Create cloud function"]
        E1 --&gt; I1["Create cloud scheduler"]
        D --&gt; E2["bigquery table → googlesheet workflow"]
        E2 --&gt; F2["Compress Zip file (source code)"]
        E2 --&gt; G2["Upload Zip file (source code) to bucket"]
        E2 --&gt; H2["Create cloud function"]
        E2 --&gt; I2["Create cloud scheduler"]
  </div>
</div>

<p><br /></p>

<h3 id="step-by-step-implementation">Step-by-Step Implementation</h3>

<h4 id="1-enable-required-api-services">1. Enable Required API Services</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Enable required APIs</span>
gcloud services <span class="nb">enable </span>dataflow.googleapis.com
gcloud services <span class="nb">enable </span>bigquery.googleapis.com
gcloud services <span class="nb">enable </span>cloudfunctions.googleapis.com
gcloud services <span class="nb">enable </span>cloudscheduler.googleapis.com
gcloud services <span class="nb">enable </span>storage.googleapis.com
gcloud services <span class="nb">enable </span>sheets.googleapis.com
</code></pre></div></div>

<h4 id="2-set-up-service-accounts-and-permissions">2. Set Up Service Accounts and Permissions</h4>

<script src="https://gist.github.com/somaz94/32601974b92d4731068b44b8a82622e6.js"></script>

<p><br /></p>

<h4 id="3-create-bigquery-dataset-and-tables">3. Create BigQuery Dataset and Tables</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create dataset</span>
bq mk <span class="nt">--dataset</span> <span class="nv">$PROJECT_ID</span>:analytics_data

<span class="c"># Create table with schema</span>
bq mk <span class="nt">--table</span> <span class="nv">$PROJECT_ID</span>:analytics_data.mongodb_data _id:STRING,name:STRING,value:FLOAT,timestamp:TIMESTAMP
</code></pre></div></div>

<h4 id="4-set-up-cloud-storage-for-function-code">4. Set Up Cloud Storage for Function Code</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create buckets for function code</span>
gsutil mb <span class="nt">-l</span> us-central1 gs://<span class="nv">$PROJECT_ID</span><span class="nt">-functions</span>
</code></pre></div></div>

<h4 id="5-prepare-and-deploy-mongodb-to-bigquery-function">5. Prepare and Deploy MongoDB to BigQuery Function</h4>

<script src="https://gist.github.com/somaz94/d721800187c601d94862ce2d004a5d4e.js"></script>

<p><br /></p>

<h4 id="6-prepare-and-deploy-bigquery-to-google-sheets-function">6. Prepare and Deploy BigQuery to Google Sheets Function</h4>

<script src="https://gist.github.com/somaz94/f9d4f7bf20eeba87983cc7b0d478097b.js"></script>

<p><br /></p>

<h4 id="7-set-up-cloud-scheduler-jobs">7. Set Up Cloud Scheduler Jobs</h4>

<script src="https://gist.github.com/somaz94/a65b24be24b41572d6471e499aaf7791.js"></script>

<p><br /></p>

<hr />

<h2 id="infrastructure-as-code-iac-implementation">Infrastructure as Code (IaC) Implementation</h2>

<p>For production environments, it’s recommended to use Infrastructure as Code tools to automate and version your ETL pipeline setup. Below are two approaches using popular IaC tools.</p>

<p><br /></p>

<h3 id="terraform-implementation">Terraform Implementation</h3>

<h4 id="terraform-benefits-for-etl-pipelines">Terraform Benefits for ETL Pipelines</h4>
<ul>
  <li>Version Control: Track changes to your infrastructure over time</li>
  <li>Repeatability: Consistently deploy the same environment across stages</li>
  <li>Modularity: Reuse components across different pipelines</li>
  <li>Dependency Management: Automatically handle resource dependencies</li>
  <li>State Management: Track the actual state of resources vs. desired state</li>
</ul>

<p><br /></p>

<p>The complete implementation is available at: <a href="https://github.com/somaz94/terraform-infra-gcp/tree/main/project/somaz-bigquery-project">terraform-infra-gcp</a></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>somaz-bigquery-project/
├── README.md
├── bigquery-to-google-sheet
│   ├── bigquery.json
│   ├── main.py
│   ├── main.py.all_data
│   ├── main.py.date
│   ├── main.py.single_db
│   ├── main.py.time_UTC
│   └── requirements.txt
├── bigquery.tfvars
├── cloud-storage.tf
├── locals.tf
├── mongodb-bigquery-googlesheet-workflow.tf
├── mongodb-to-bigquery
│   ├── MongoDB_to_BigQuery
│   ├── main.py
│   ├── main.py.local
│   ├── main.py.single_db
│   └── requirement.txt
├── provider.tf
├── terraform-backend.tf
└── variables.tf
</code></pre></div></div>

<h4 id="key-terraform-resource-types-used">Key Terraform resource types used:</h4>
<ul>
  <li><code class="language-plaintext highlighter-rouge">google_storage_bucket</code>: For function code storage</li>
  <li><code class="language-plaintext highlighter-rouge">google_cloudfunctions_function</code>: For serverless ETL functions</li>
  <li><code class="language-plaintext highlighter-rouge">google_cloud_scheduler_job</code>: For scheduling ETL tasks</li>
  <li><code class="language-plaintext highlighter-rouge">google_bigquery_dataset</code>: For data storage</li>
  <li><code class="language-plaintext highlighter-rouge">google_bigquery_table</code>: For structured data tables</li>
</ul>

<p><br /></p>

<h3 id="pulumi-implementation">Pulumi Implementation</h3>

<p>Pulumi offers a more programmatic approach to infrastructure as code, allowing you to use familiar languages like Python:</p>

<p>The complete implementation is available at: <a href="https://github.com/somaz94/pulumi-study/tree/main/gcp/python/bigdata-flow-functions">pulumi-study</a></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bigdata-flow-functions/
├── Pulumi.yaml
├── README.md
├── __main__.py
├── bigquery-to-google-sheet
│   ├── bigquery.json
│   ├── main.py
│   ├── main.py.all_data
│   ├── main.py.date
│   ├── main.py.single_db
│   ├── main.py.time_UTC
│   └── requirements.txt
├── bq_dataset.py
├── bq_sheet_archive.py
├── bq_sheet_function.py
├── config.py
├── mdb_bq_archive.py
├── mdb_bq_function.py
├── mongodb-to-bigquery
│   ├── main.py
│   ├── main.py.local
│   ├── main.py.single_db
│   └── requirements.txt
├── requirements.txt
├── scheduler_manager.py
├── storage.py
└── utils.py
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="advanced-customization-and-optimizations">Advanced Customization and Optimizations</h2>

<p><br /></p>

<h3 id="handling-different-data-types">Handling Different Data Types</h3>

<p>The provided code examples can be customized for different MongoDB collections and data structures:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">main.py.all_data</code>: Processes complete collections</li>
  <li><code class="language-plaintext highlighter-rouge">main.py.date</code>: Filters data by date ranges</li>
  <li><code class="language-plaintext highlighter-rouge">main.py.single_db</code>: Works with single MongoDB databases</li>
  <li><code class="language-plaintext highlighter-rouge">main.py.time_UTC</code>: Handles timezone conversions</li>
</ul>

<p><br /></p>

<h3 id="scaling-considerations">Scaling Considerations</h3>

<p>As your data volumes grow, consider these optimizations:</p>

<ol>
  <li><strong>BigQuery Partitioning</strong>: Partition tables by date for improved query performance and cost
    <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="k">CREATE</span> <span class="k">TABLE</span> <span class="n">analytics_data</span><span class="p">.</span><span class="n">mongodb_data</span>
   <span class="k">PARTITION</span> <span class="k">BY</span> <span class="nb">DATE</span><span class="p">(</span><span class="nb">timestamp</span><span class="p">)</span>
   <span class="k">AS</span> <span class="k">SELECT</span> <span class="o">*</span> <span class="k">FROM</span> <span class="n">analytics_data</span><span class="p">.</span><span class="n">mongodb_data_temp</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Batch Processing</strong>: Process data in manageable chunks
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="c1"># Process in batches of 1000 documents
</span>   <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1000</span>
   <span class="n">total_docs</span> <span class="o">=</span> <span class="n">collection</span><span class="p">.</span><span class="n">count_documents</span><span class="p">({})</span>
   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_docs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
       <span class="n">cursor</span> <span class="o">=</span> <span class="n">collection</span><span class="p">.</span><span class="n">find</span><span class="p">({}).</span><span class="n">skip</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">limit</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
       <span class="c1"># Process batch
</span></code></pre></div>    </div>
  </li>
  <li><strong>Error Handling</strong>: Implement robust error handling and retries
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">from</span> <span class="nn">google.api_core.retry</span> <span class="kn">import</span> <span class="n">Retry</span>
      
   <span class="o">@</span><span class="n">Retry</span><span class="p">(</span><span class="n">predicate</span><span class="o">=</span><span class="nb">Exception</span><span class="p">,</span> <span class="n">deadline</span><span class="o">=</span><span class="mf">60.0</span><span class="p">)</span>
   <span class="k">def</span> <span class="nf">insert_with_retry</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">table_id</span><span class="p">,</span> <span class="n">rows</span><span class="p">):</span>
       <span class="n">errors</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">insert_rows_json</span><span class="p">(</span><span class="n">table_id</span><span class="p">,</span> <span class="n">rows</span><span class="p">)</span>
       <span class="k">if</span> <span class="n">errors</span><span class="p">:</span>
           <span class="k">raise</span> <span class="nb">Exception</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error inserting rows: </span><span class="si">{</span><span class="n">errors</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
       <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Monitoring</strong>: Set up alerts for pipeline failures
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="kn">from</span> <span class="nn">google.cloud</span> <span class="kn">import</span> <span class="n">monitoring_v3</span>
      
   <span class="k">def</span> <span class="nf">create_failure_alert</span><span class="p">(</span><span class="n">project_id</span><span class="p">):</span>
       <span class="n">client</span> <span class="o">=</span> <span class="n">monitoring_v3</span><span class="p">.</span><span class="n">AlertPolicyServiceClient</span><span class="p">()</span>
       <span class="c1"># Create alert policy configuration
</span>       <span class="c1"># ...
</span></code></pre></div>    </div>
  </li>
</ol>

<p><br /></p>

<hr />

<h2 id="best-practices-and-lessons-learned">Best Practices and Lessons Learned</h2>

<p><br /></p>

<h3 id="security-best-practices">Security Best Practices</h3>

<ol>
  <li><strong>Principle of Least Privilege</strong>: Grant only required permissions to service accounts</li>
  <li><strong>Secret Management</strong>: Store sensitive credentials securely using Secret Manager</li>
  <li><strong>VPC Service Controls</strong>: Implement network security boundaries for sensitive data</li>
  <li><strong>Audit Logging</strong>: Enable comprehensive logging for all data access</li>
</ol>

<p><br /></p>

<h3 id="performance-optimization">Performance Optimization</h3>

<ol>
  <li><strong>BigQuery Best Practices</strong>:
    <ul>
      <li>Use partitioning and clustering</li>
      <li>Filter on partitioned columns</li>
      <li>Select only needed columns</li>
      <li>Use approximate aggregation functions when possible</li>
    </ul>
  </li>
  <li><strong>Cloud Functions Performance</strong>:
    <ul>
      <li>Increase memory allocation for CPU-intensive tasks</li>
      <li>Keep functions stateless and idempotent</li>
      <li>Reuse connections to external services</li>
    </ul>
  </li>
  <li><strong>Cost Optimization</strong>:
    <ul>
      <li>Set up budget alerts</li>
      <li>Use BigQuery flat-rate pricing for predictable workloads</li>
      <li>Schedule less frequent updates for non-critical data</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<hr />

<h2 id="next-steps-and-future-improvements">Next Steps and Future Improvements</h2>

<p>This ETL pipeline can be extended in several ways:</p>

<ol>
  <li><strong>Real-time Processing</strong>: Implement streaming data ingestion using Pub/Sub and Dataflow</li>
  <li><strong>Data Quality Checks</strong>: Add validation steps to ensure data integrity</li>
  <li><strong>Advanced Analytics</strong>: Implement BigQuery ML for predictive analytics</li>
  <li><strong>Visualization</strong>: Connect to Data Studio or Looker for rich dashboards</li>
  <li><strong>CI/CD Pipeline</strong>: Set up automated testing and deployment</li>
</ol>

<p>In future articles, we’ll explore these advanced topics in greater detail.</p>

<p><br /></p>

<hr />

<h2 id="references">References</h2>

<ul>
  <li><a href="https://medium.com/analytics-vidhya/etl-with-dataflow-bigquery-bfaf22fbd0d0">ETL with Dataflow BigQuery</a></li>
  <li><a href="https://beam.apache.org/documentation/programming-guide/">Apache Beam Programming Guide</a></li>
  <li><a href="https://cloud.google.com/dataflow/docs">Google Cloud Dataflow Documentation</a></li>
  <li><a href="https://cloud.google.com/bigquery/docs/best-practices-performance-overview">BigQuery Best Practices</a></li>
  <li><a href="https://cloud.google.com/functions/docs">Cloud Functions Documentation</a></li>
  <li><a href="https://cloud.google.com/scheduler/docs">Cloud Scheduler Documentation</a></li>
  <li><a href="https://console.cloud.google.com/storage/browser/dataflow-templates/latest">Dataflow Templates</a></li>
  <li><a href="https://github.com/somaz94/terraform-infra-gcp/tree/main/project/somaz-bigquery-project">Terraform Implementation</a></li>
  <li><a href="https://github.com/somaz94/pulumi-study/tree/main/gcp/python/bigdata-flow-functions">Pulumi Implementation</a></li>
</ul>


                <!-- Pagination links -->


            </article>

            
                <aside class="see-also">
                    <h2>See also</h2>
                    <ul>
                        
                        
                        
                            <li>
                                <a href="/category/troubleshooting/fluent-bit-opensearch-duplicate-logs/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1754458041/fluenbit-opensearch_lmr5hf.png">
                                    
                                    <h3>Resolving Duplicate Log Issues in Fluent Bit to OpenSearch Integration</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/kubernetes/kubernetes-install-kubespray-new/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1737697968/kubespray_uyug5r.png">
                                    
                                    <h3>Installing Kubernetes with Kubespray and Adding Worker Nodes (2025V.)</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/openstack/openstack-kuryr/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1742267532/openstack_t2tmoc.png">
                                    
                                    <h3>Deep Dive into OpenStack Kuryr</h3>
                                </a>
                            </li>
                        
                    </ul>
                </aside>
            

        </section>

        <!-- Add time bar only for pages without pagination -->
        
            <div class="time-bar" data-minutes="17">
    <span class="time-completed"></span>
    <span class="time-remaining"></span>
    <div class="bar">
        <span class="completed" style="width:0%;"></span>
        <span class="remaining" style="width:100%;"></span>
    </div>
</div>

            <button class="toggle-preview" onclick="togglePreview()">
    <span>Hide Preview ▼</span>
</button>

<div id="recommendationSection" class="recommendation">
    <div class="message">
        <strong>Why don't you read something next?</strong>
        <div>
            <button>
                <svg><use xlink:href="#icon-arrow-right"></use></svg>
                <span>Go back to top</span>
            </button>
        </div>
    </div>
    <div id="previewSection" class="preview-section">
        
        <a href="/category/linux/dev-null/" class="post-preview">
            <div class="image">
                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1738917615/dev-null_yogt4x.png">
                
            </div>
            <h3 class="title">Understanding Linux File Descriptors and /dev/null</h3>
        </a>
    </div>
</div>

<style>
.toggle-preview {
    position: fixed;
    bottom: 20px;
    right: 20px;
    background: #333;
    color: white;
    border: none;
    padding: 8px 15px;
    border-radius: 4px;
    cursor: pointer;
    z-index: 1000;
    opacity: 0;
    transition: opacity 0.3s ease;
}

.toggle-preview:hover {
    background: #444;
}

.toggle-preview.visible {
    opacity: 1;
}

.recommendation {
    margin-top: 1000px;
    display: block;
    transition: all 0.3s ease;
}

.recommendation.hidden {
    display: none;
}

.hide-preview {
    margin-left: 10px;
    background: none;
    border: 1px solid #666;
    color: #666;
    padding: 5px 10px;
    border-radius: 4px;
    cursor: pointer;
}

.hide-preview:hover {
    background: #f0f0f0;
}

.preview-section {
    max-height: 1000px;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
}

.preview-section.hidden {
    max-height: 0;
}
</style>

<script>
function togglePreview() {
    const recommendation = document.getElementById('recommendationSection');
    const button = document.querySelector('.toggle-preview span');
    
    if (recommendation.classList.contains('hidden')) {
        recommendation.classList.remove('hidden');
        button.textContent = 'Hide Preview ▼';
    } else {
        recommendation.classList.add('hidden');
        button.textContent = 'Show Preview ▲';
    }
}

window.addEventListener('scroll', function() {
    const toggleButton = document.querySelector('.toggle-preview');
    const recommendation = document.getElementById('recommendationSection');
    const rect = recommendation.getBoundingClientRect();
    
    if (rect.top <= window.innerHeight) {
        toggleButton.classList.add('visible');
    } else {
        toggleButton.classList.remove('visible');
    }
});
</script>

        

        <!-- Show modal if the post is the last one -->
        

        <!-- Show modal before user leaves the page -->
        

        <!-- Add your newsletter subscription form here -->

        <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;Learn how to build an ETL pipeline using MongoDB, BigQuery, and Google Sheets with Google Cloud Platform services&quot;%20https://somaz.blog/category/gcp/bigquery-dataflow/%20via%20&#64;twitter_username&hashtags=bigquery,dataflow,etl,gcp,mongodb"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://somaz.blog/category/gcp/bigquery-dataflow/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

        

  <section class="author">
    <div class="details">
      
        <img class="img-rounded" src="/assets/img/uploads/profile.png" alt="Somaz">
      
      <p class="def">Author</p>
      <h3 class="name">
        <a href="/authors/somaz/">Somaz</a>
      </h3>
      <p class="desc">DevOps engineer focused on cloud infrastructure and automation</p>
      <p>
        
          <a href="https://github.com/somaz94" title="Github">
            <svg><use xlink:href="#icon-github"></use></svg>
          </a>
        
        
        
        
        
        
          <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
            <svg><use xlink:href="#icon-linkedin"></use></svg>
          </a>
        
        
          <a href="https://somaz.tistory.com" title="Tistory">
            <svg><use xlink:href="#icon-tistory"></use></svg>
          </a>
        
      </p>
    </div>
  </section>

  
  
  
  
  
  
  
  

  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Somaz",
      
      "image": "/assets/img/uploads/profile.png",
      
      "jobTitle": "DevOps Engineer",
      "url": "https://somaz.blog/authors/somaz/",
      "sameAs": [
        "https://github.com/somaz94","https://www.linkedin.com/in/somaz","https://{{ author.tistory_username }}.tistory.com"
      ]
  }
  </script>


        

<section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'https-somaz94-github-io';
        var disqus_title = '';
        var disqus_url = '/category/gcp/bigquery-dataflow/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>



        <footer>
    <p>
      
        <a href="https://github.com/somaz94" title="Github">
          <svg><use xlink:href="#icon-github"></use></svg>
        </a>
      
      
        <a href="https://www.facebook.com/facebook_username" title="Facebook">
          <svg><use xlink:href="#icon-facebook"></use></svg>
        </a>
      
      
        <a href="https://twitter.com/twitter_username" title="Twitter">
          <svg><use xlink:href="#icon-twitter"></use></svg>
        </a>
      
      
        <a href="https://medium.com/@medium_username" title="Medium">
          <svg><use xlink:href="#icon-medium"></use></svg>
        </a>
      
      
        <a href="https://www.instagram.com/instagram_username" title="Instagram">
          <svg><use xlink:href="#icon-instagram"></use></svg>
        </a>
      
      
        <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
          <svg><use xlink:href="#icon-linkedin"></use></svg>
        </a>
      
      
        <a href="https://somaz.tistory.com" title="Tistory">
          <svg><use xlink:href="#icon-tistory"></use></svg>
        </a>
      
    </p>

    <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>


    <p>
      <a href="https://somaz.blog/sitemap.xml" title="sitemap">Sitemap</a> |
      <a href="https://somaz.blog/privacy-policy.html" title="Privacy Policy">Privacy Policy</a>
    </p>

    <p>
      <span>Somaz Tech Blog</span> <svg class="love"><use xlink:href="#icon-heart"></use></svg>
    </p>
</footer>










<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "somaz",
  "description": "DevOps engineer's tech blog.",
  "url": "https://somaz.blog/",
  "logo": {
      "@type": "ImageObject",
      "url": "https://somaz.blog/assets/img/icons/mediumtile.png",
      "width": "600",
      "height": "315"
  },
  "sameAs": [
    "https://github.com/somaz94","https://www.facebook.com/facebook_username","https://twitter.com/twitter_username","https://medium.com/@medium_username","https://www.instagram.com/instagram_username","https://www.linkedin.com/in/somaz","https://{{ site.tistory_username }}.tistory.com"
  ]
}
</script>

<!-- Include the script that allows Netlify CMS login -->
<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>

<!-- Include the website scripts -->
<script src="/assets/js/scripts.min.js"></script>

<!-- Include Google Analytics script -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  var host = window.location.hostname;
  if (host != 'localhost') {
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-XXXXXXXX-X');
  }
</script>
  


<!-- Include extra scripts -->



        

        
        
        
        
        
        
        
        
        <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "BlogPosting",
            "name": "Building ETL Pipeline with BigQuery and Dataflow",
            "headline": "A comprehensive guide to data ETL process using GCP services",
            "description": "Learn how to build an ETL pipeline using MongoDB, BigQuery, and Google Sheets with Google Cloud Platform services",
            "image": "https://res.cloudinary.com/dkcm26aem/image/upload/v1738719521/gcp-bigquery-dataflow_ikgarw.png",
            "url": "https://somaz.blog/category/gcp/bigquery-dataflow/",
            "articleBody": "Image Reference





Overview

Modern data pipelines are essential for organizations to transform raw data into actionable insights. Following our previous post about BigQuery, this guide explores how to build a robust ETL (Extract, Transform, Load) pipeline using Google Cloud Platform’s managed services, focusing on moving data from MongoDB to BigQuery and finally to Google Sheets for analysis and visualization.

This comprehensive tutorial will guide you through building a fully automated data pipeline with the following flow:

MongoDB → BigQuery → Google Sheets





  Understanding ETL
  ETL (Extract, Transform, Load) is a three-phase process for integrating data from multiple sources into a single data repository:
  
    Extract: Retrieving data from source systems (databases, APIs, files)
    Transform: Converting data to appropriate format with operations like filtering, sorting, aggregating, joining, cleaning, and validation
    Load: Writing the transformed data to a target destination (data warehouse, database, file)
  
  Cloud-based ETL solutions like the one we&apos;re building offer advantages including scalability, reduced maintenance, cost-effectiveness, and integration with other cloud services.






GCP Services Overview

Our ETL pipeline leverages several key Google Cloud Platform services:


  
    
      Service
      Role in Pipeline
      Key Capabilities
    
    
      Cloud Dataflow
      Data processing engine
      Batch/stream processing, auto-scaling, managed infrastructure
    
    
      BigQuery
      Data warehouse
      Serverless storage and analytics, SQL interface, high-performance
    
    
      Cloud Functions
      Serverless code execution
      Event-driven processing, automatic scaling, pay-per-use
    
    
      Cloud Scheduler
      Job scheduling
      Managed cron jobs, reliable triggering, timezone support
    
    
      Cloud Storage
      Function code storage
      Highly durable object storage, global access, integration with other services
    
  





  
    graph LR
      A[MongoDB] --&amp;gt;|Extract| B[Cloud Functions]
      B --&amp;gt;|Transform| C[BigQuery]
      C --&amp;gt;|Extract| D[Cloud Functions]
      D --&amp;gt;|Transform &amp;amp; Load| E[Google Sheets]
      F[Cloud Scheduler] --&amp;gt;|Trigger| B
      F --&amp;gt;|Trigger| D
      G[Cloud Storage] --&amp;gt;|Store Code| B
      G --&amp;gt;|Store Code| D
  






Cloud Dataflow Deep Dive



What is Google Cloud Dataflow?

Google Cloud Dataflow is a fully managed service for executing Apache Beam pipelines within the Google Cloud Platform ecosystem. It handles both batch and streaming data processing scenarios with automatic resource management and dynamic work rebalancing.



Dataflow Architecture

When a Dataflow job is launched, the service:


  Takes your pipeline code (Apache Beam)
  Optimizes it into an execution graph
  Splits the workload into multiple steps
  Distributes these steps across Compute Engine instances
  Manages scaling, fault tolerance, and worker communication


This abstraction allows developers to focus on pipeline logic rather than infrastructure management.



Key Features and Benefits


  Unified Programming Model
    
      Apache Beam SDK provides a single programming model for both batch and streaming
      Same code can run on multiple execution engines
      Supported languages include Java, Python, Go, and more
    
  
  No-Ops Data Processing
    
      Fully managed service with automatic provisioning and scaling
      Hands-off infrastructure management
      Dynamic work rebalancing for optimal throughput
      Built-in monitoring and logging
    
  
  Advanced Processing Capabilities
    
      Native handling of late and out-of-order data
      Exactly-once processing guarantees
      Windowing for time-based aggregations
      Watermarking for tracking event time progress
    
  
  Performance and Scalability
    
      Horizontal scaling to handle varying workloads
      Fusion optimization to minimize overhead
      Auto-tuning of worker resources
      Support for persistent disks and SSDs for performance-sensitive operations
    
  



  
    graph TD
      A[Beam SDK Code] --&amp;gt; B[Dataflow Service]
      B --&amp;gt; C{Job Execution}
      C --&amp;gt; D[Worker Pool]
      D --&amp;gt; E[Worker 1]
      D --&amp;gt; F[Worker 2]
      D --&amp;gt; G[Worker N]
      B &amp;lt;--&amp;gt;|Metrics/Logs| H[Monitoring &amp;amp; Logging]
      B &amp;lt;--&amp;gt;|Status/Control| I[Job Management]
  




Sample Dataflow Pipeline Code

Here’s a simplified example of a Python Dataflow pipeline that reads from MongoDB, performs transformations, and writes to BigQuery:

import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.mongodbio import ReadFromMongoDB
from apache_beam.io.gcp.bigquery import WriteToBigQuery

# Define pipeline options including GCP project and Dataflow runner
options = PipelineOptions([
    &apos;--project=your-gcp-project&apos;,
    &apos;--region=us-central1&apos;,
    &apos;--runner=DataflowRunner&apos;,
    &apos;--temp_location=gs://your-bucket/temp&apos;,
    &apos;--staging_location=gs://your-bucket/staging&apos;
])

# Schema for BigQuery table
schema = {
    &apos;fields&apos;: [
        {&apos;name&apos;: &apos;id&apos;, &apos;type&apos;: &apos;STRING&apos;},
        {&apos;name&apos;: &apos;name&apos;, &apos;type&apos;: &apos;STRING&apos;},
        {&apos;name&apos;: &apos;value&apos;, &apos;type&apos;: &apos;FLOAT&apos;},
        {&apos;name&apos;: &apos;timestamp&apos;, &apos;type&apos;: &apos;TIMESTAMP&apos;}
    ]
}

# MongoDB connection URI
uri = &quot;mongodb+srv://username:password@cluster.mongodb.net&quot;

# Create and run the pipeline
with beam.Pipeline(options=options) as p:
    (p 
     | &apos;ReadFromMongoDB&apos; &amp;gt;&amp;gt; ReadFromMongoDB(
         uri=uri,
         db=&apos;your_database&apos;,
         coll=&apos;your_collection&apos;)
     | &apos;TransformData&apos; &amp;gt;&amp;gt; beam.Map(lambda doc: {
         &apos;id&apos;: str(doc[&apos;_id&apos;]),
         &apos;name&apos;: doc.get(&apos;name&apos;, &apos;&apos;),
         &apos;value&apos;: float(doc.get(&apos;value&apos;, 0)),
         &apos;timestamp&apos;: doc.get(&apos;created_at&apos;).timestamp()
     })
     | &apos;WriteToBigQuery&apos; &amp;gt;&amp;gt; WriteToBigQuery(
         table=&apos;your-project:your_dataset.your_table&apos;,
         schema=schema,
         create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
         write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)
    )






Cloud Functions and Scheduler



Cloud Functions

Google Cloud Functions is a lightweight, event-driven, serverless compute platform that allows you to create single-purpose, stand-alone functions that respond to cloud events without managing the underlying infrastructure.


  When to Use Cloud Functions vs. Dataflow
  In our ETL pipeline, we&apos;re using Cloud Functions for orchestration and simpler data movement tasks, while Dataflow would be better suited for:
  
    Processing very large datasets (TB+)
    Complex data transformations requiring parallel processing
    Streaming data scenarios with windowing requirements
    Pipelines that need sophisticated fault tolerance
  
  Cloud Functions is ideal for event-triggered actions, lightweight processing, and gluing together different GCP services, as we&apos;re using it in this workflow.




Key Features


  Event-Driven: Triggers include HTTP requests, Cloud Storage events, Pub/Sub messages, and Firestore events
  Stateless: Each invocation is independent, with no persistent state between executions
  Automatic Scaling: Scales from zero to many instances based on load
  Runtime Support: Node.js, Python, Go, Java, .NET, Ruby, and PHP
  Pay-per-use: Billed by compute time in 100ms increments




Cloud Scheduler

Google Cloud Scheduler is a fully managed enterprise-grade cron job scheduler that allows you to schedule virtually any job, including batch jobs, big data jobs, cloud infrastructure operations, and more.



Key Features


  Reliability: Google-grade reliability with SLA
  Cron Syntax: Familiar unix-cron format for job definitions
  Flexible Targets: HTTP/S endpoints, Pub/Sub topics, App Engine applications
  Authentication: Support for OIDC tokens, basic auth, and OAuth
  Monitoring: Integrated with Cloud Monitoring for observability
  Retry Logic: Configurable retry on failed jobs




Example Code: MongoDB to BigQuery Function

Here’s a sample Cloud Function that extracts data from MongoDB and loads it into BigQuery:

import os
import pymongo
from google.cloud import bigquery
from datetime import datetime

def mongodb_to_bigquery(request):
    # MongoDB connection
    client = pymongo.MongoClient(os.environ.get(&apos;MONGODB_URI&apos;))
    db = client[os.environ.get(&apos;MONGODB_DB&apos;)]
    collection = db[os.environ.get(&apos;MONGODB_COLLECTION&apos;)]
    
    # Get data from MongoDB
    cursor = collection.find({})
    
    # Transform data for BigQuery
    rows = []
    for document in cursor:
        # Convert MongoDB ObjectId to string
        document[&apos;_id&apos;] = str(document[&apos;_id&apos;])
        
        # Convert any datetime objects to string
        for key, value in document.items():
            if isinstance(value, datetime):
                document[key] = value.isoformat()
        
        rows.append(document)
    
    # BigQuery client
    client = bigquery.Client()
    table_id = f&quot;{os.environ.get(&apos;BIGQUERY_PROJECT&apos;)}.{os.environ.get(&apos;BIGQUERY_DATASET&apos;)}.{os.environ.get(&apos;BIGQUERY_TABLE&apos;)}&quot;
    
    # Load data into BigQuery
    errors = client.insert_rows_json(table_id, rows)
    
    if errors:
        return f&quot;Encountered errors: {errors}&quot;, 500
    else:
        return f&quot;Successfully loaded {len(rows)} rows to {table_id}&quot;, 200




Example Code: BigQuery to Google Sheets Function

Here’s a sample Cloud Function that extracts data from BigQuery and loads it into Google Sheets:

import os
import gspread
from google.cloud import bigquery
from google.oauth2 import service_account
from datetime import datetime

def bigquery_to_sheets(request):
    # BigQuery setup
    bq_client = bigquery.Client()
    
    # Query data from BigQuery
    query = f&quot;&quot;&quot;
    SELECT *
    FROM `{os.environ.get(&apos;BIGQUERY_PROJECT&apos;)}.{os.environ.get(&apos;BIGQUERY_DATASET&apos;)}.{os.environ.get(&apos;BIGQUERY_TABLE&apos;)}`
    LIMIT 1000
    &quot;&quot;&quot;
    
    query_job = bq_client.query(query)
    rows = query_job.result()
    
    # Transform data for Google Sheets
    headers = [field.name for field in query_job.schema]
    data = [headers]
    for row in rows:
        data.append([str(row[field]) for field in headers])
    
    # Google Sheets setup
    credentials = service_account.Credentials.from_service_account_file(
        &apos;bigquery.json&apos;,
        scopes=[&apos;https://www.googleapis.com/auth/spreadsheets&apos;]
    )
    gc = gspread.authorize(credentials)
    
    # Open the spreadsheet and write data
    sheet = gc.open_by_key(os.environ.get(&apos;SHEET_ID&apos;)).worksheet(os.environ.get(&apos;SHEET_NAME&apos;))
    sheet.clear()
    sheet.update(&apos;A1&apos;, data)
    
    return f&quot;Successfully updated Google Sheet with {len(data)-1} rows of data&quot;, 200






Complete ETL Workflow Implementation

Our complete ETL workflow follows this sequence:


  
    flowchart TD
        Z[&quot;Enable API Service&quot;] --&amp;gt; A[&quot;Create BigQuery Service Account and Grant Permission&quot;]
        A --&amp;gt; B[&quot;Create Google Sheet and Grant Permission&quot;]
        B --&amp;gt; C[&quot;Create BigQuery Dataset&quot;]
        C --&amp;gt; D[&quot;Create Storage for Cloud-function&quot;]
        D --&amp;gt; E1[&quot;mongodb → bigquery table workflow&quot;]
        E1 --&amp;gt; F1[&quot;Compress Zip file (source code)&quot;]
        E1 --&amp;gt; G1[&quot;Upload Zip file (source code) to bucket&quot;]
        E1 --&amp;gt; H1[&quot;Create cloud function&quot;]
        E1 --&amp;gt; I1[&quot;Create cloud scheduler&quot;]
        D --&amp;gt; E2[&quot;bigquery table → googlesheet workflow&quot;]
        E2 --&amp;gt; F2[&quot;Compress Zip file (source code)&quot;]
        E2 --&amp;gt; G2[&quot;Upload Zip file (source code) to bucket&quot;]
        E2 --&amp;gt; H2[&quot;Create cloud function&quot;]
        E2 --&amp;gt; I2[&quot;Create cloud scheduler&quot;]
  




Step-by-Step Implementation

1. Enable Required API Services

# Enable required APIs
gcloud services enable dataflow.googleapis.com
gcloud services enable bigquery.googleapis.com
gcloud services enable cloudfunctions.googleapis.com
gcloud services enable cloudscheduler.googleapis.com
gcloud services enable storage.googleapis.com
gcloud services enable sheets.googleapis.com


2. Set Up Service Accounts and Permissions





3. Create BigQuery Dataset and Tables

# Create dataset
bq mk --dataset $PROJECT_ID:analytics_data

# Create table with schema
bq mk --table $PROJECT_ID:analytics_data.mongodb_data _id:STRING,name:STRING,value:FLOAT,timestamp:TIMESTAMP


4. Set Up Cloud Storage for Function Code

# Create buckets for function code
gsutil mb -l us-central1 gs://$PROJECT_ID-functions


5. Prepare and Deploy MongoDB to BigQuery Function





6. Prepare and Deploy BigQuery to Google Sheets Function





7. Set Up Cloud Scheduler Jobs







Infrastructure as Code (IaC) Implementation

For production environments, it’s recommended to use Infrastructure as Code tools to automate and version your ETL pipeline setup. Below are two approaches using popular IaC tools.



Terraform Implementation

Terraform Benefits for ETL Pipelines

  Version Control: Track changes to your infrastructure over time
  Repeatability: Consistently deploy the same environment across stages
  Modularity: Reuse components across different pipelines
  Dependency Management: Automatically handle resource dependencies
  State Management: Track the actual state of resources vs. desired state




The complete implementation is available at: terraform-infra-gcp

somaz-bigquery-project/
├── README.md
├── bigquery-to-google-sheet
│   ├── bigquery.json
│   ├── main.py
│   ├── main.py.all_data
│   ├── main.py.date
│   ├── main.py.single_db
│   ├── main.py.time_UTC
│   └── requirements.txt
├── bigquery.tfvars
├── cloud-storage.tf
├── locals.tf
├── mongodb-bigquery-googlesheet-workflow.tf
├── mongodb-to-bigquery
│   ├── MongoDB_to_BigQuery
│   ├── main.py
│   ├── main.py.local
│   ├── main.py.single_db
│   └── requirement.txt
├── provider.tf
├── terraform-backend.tf
└── variables.tf


Key Terraform resource types used:

  google_storage_bucket: For function code storage
  google_cloudfunctions_function: For serverless ETL functions
  google_cloud_scheduler_job: For scheduling ETL tasks
  google_bigquery_dataset: For data storage
  google_bigquery_table: For structured data tables




Pulumi Implementation

Pulumi offers a more programmatic approach to infrastructure as code, allowing you to use familiar languages like Python:

The complete implementation is available at: pulumi-study

bigdata-flow-functions/
├── Pulumi.yaml
├── README.md
├── __main__.py
├── bigquery-to-google-sheet
│   ├── bigquery.json
│   ├── main.py
│   ├── main.py.all_data
│   ├── main.py.date
│   ├── main.py.single_db
│   ├── main.py.time_UTC
│   └── requirements.txt
├── bq_dataset.py
├── bq_sheet_archive.py
├── bq_sheet_function.py
├── config.py
├── mdb_bq_archive.py
├── mdb_bq_function.py
├── mongodb-to-bigquery
│   ├── main.py
│   ├── main.py.local
│   ├── main.py.single_db
│   └── requirements.txt
├── requirements.txt
├── scheduler_manager.py
├── storage.py
└── utils.py






Advanced Customization and Optimizations



Handling Different Data Types

The provided code examples can be customized for different MongoDB collections and data structures:


  main.py.all_data: Processes complete collections
  main.py.date: Filters data by date ranges
  main.py.single_db: Works with single MongoDB databases
  main.py.time_UTC: Handles timezone conversions




Scaling Considerations

As your data volumes grow, consider these optimizations:


  BigQuery Partitioning: Partition tables by date for improved query performance and cost
       CREATE TABLE analytics_data.mongodb_data
   PARTITION BY DATE(timestamp)
   AS SELECT * FROM analytics_data.mongodb_data_temp
    
  
  Batch Processing: Process data in manageable chunks
       # Process in batches of 1000 documents
   batch_size = 1000
   total_docs = collection.count_documents({})
   for i in range(0, total_docs, batch_size):
       cursor = collection.find({}).skip(i).limit(batch_size)
       # Process batch
    
  
  Error Handling: Implement robust error handling and retries
       from google.api_core.retry import Retry
      
   @Retry(predicate=Exception, deadline=60.0)
   def insert_with_retry(client, table_id, rows):
       errors = client.insert_rows_json(table_id, rows)
       if errors:
           raise Exception(f&quot;Error inserting rows: {errors}&quot;)
       return len(rows)
    
  
  Monitoring: Set up alerts for pipeline failures
       from google.cloud import monitoring_v3
      
   def create_failure_alert(project_id):
       client = monitoring_v3.AlertPolicyServiceClient()
       # Create alert policy configuration
       # ...
    
  






Best Practices and Lessons Learned



Security Best Practices


  Principle of Least Privilege: Grant only required permissions to service accounts
  Secret Management: Store sensitive credentials securely using Secret Manager
  VPC Service Controls: Implement network security boundaries for sensitive data
  Audit Logging: Enable comprehensive logging for all data access




Performance Optimization


  BigQuery Best Practices:
    
      Use partitioning and clustering
      Filter on partitioned columns
      Select only needed columns
      Use approximate aggregation functions when possible
    
  
  Cloud Functions Performance:
    
      Increase memory allocation for CPU-intensive tasks
      Keep functions stateless and idempotent
      Reuse connections to external services
    
  
  Cost Optimization:
    
      Set up budget alerts
      Use BigQuery flat-rate pricing for predictable workloads
      Schedule less frequent updates for non-critical data
    
  






Next Steps and Future Improvements

This ETL pipeline can be extended in several ways:


  Real-time Processing: Implement streaming data ingestion using Pub/Sub and Dataflow
  Data Quality Checks: Add validation steps to ensure data integrity
  Advanced Analytics: Implement BigQuery ML for predictive analytics
  Visualization: Connect to Data Studio or Looker for rich dashboards
  CI/CD Pipeline: Set up automated testing and deployment


In future articles, we’ll explore these advanced topics in greater detail.





References


  ETL with Dataflow BigQuery
  Apache Beam Programming Guide
  Google Cloud Dataflow Documentation
  BigQuery Best Practices
  Cloud Functions Documentation
  Cloud Scheduler Documentation
  Dataflow Templates
  Terraform Implementation
  Pulumi Implementation

",
            "wordcount": "3099",
            "inLanguage": "en",
            "dateCreated": "2025-02-11/",
            "datePublished": "2025-02-11/",
            "dateModified": "2025-02-11/",
            "author": {
                "@type": "Person",
                "name": "Somaz",
                
                "image": "/assets/img/uploads/profile.png",
                
                "jobTitle": "DevOps Engineer",
                "url": "https://somaz.blog/authors/somaz/",
                "sameAs": [
                    "https://github.com/somaz94","https://www.linkedin.com/in/somaz"
                ]
            },
            "publisher": {
                "@type": "Organization",
                "name": "somaz",
                "url": "https://somaz.blog/",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://somaz.blog/assets/img/blog-image.png",
                    "width": "600",
                    "height": "315"
                }
            },
            "mainEntityOfPage": "True",
            "genre": "GCP",
            "articleSection": "GCP",
            "keywords": ["bigquery","dataflow","etl","gcp","mongodb"]
        }
        </script>
    </body>
</html>
