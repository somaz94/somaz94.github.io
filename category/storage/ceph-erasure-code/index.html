<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    

    
    

    
    

    
    

    <!-- ✅ Google Tag Manager 추가 -->
    <script>
        (function(w,d,s,l,i){
            w[l]=w[l]||[];
            w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});
            var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';
            j.async=true;
            j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;
            f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer','GTM-MBP83N4Q');
    </script>
      <!-- ✅ End Google Tag Manager -->

    <!-- Mermaid.js 직접 로드 -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>

    <title>Advanced Ceph Erasure Code Configuration and Performance Optimization | somaz</title>
    <meta name="description" content="Comprehensive walkthrough of Ceph Erasure Code setup, configuration, and optimization including comparison with replica methods, practical implementation ste...">
    
        <meta name="keywords" content="ceph, erasure-code, storage-efficiency, data-protection, performance-optimization, distributed-storage, infrastructure, automation">
    

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Advanced Ceph Erasure Code Configuration and Performance Optimization | somaz">
    <meta name="twitter:description" content="Comprehensive walkthrough of Ceph Erasure Code setup, configuration, and optimization including comparison with replica methods, practical implementation ste...">

    
        <meta property="twitter:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1755133935/ceph-erasure-code_kheagu.png">
    
    
    
        <meta name="twitter:site" content="@twitter_username">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="https://somaz.blog/category/storage/ceph-erasure-code/">
    <meta property="og:title" content="Advanced Ceph Erasure Code Configuration and Performance Optimization | somaz">
    <meta property="og:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1755133935/ceph-erasure-code_kheagu.png">
    <meta property="og:description" content="Comprehensive walkthrough of Ceph Erasure Code setup, configuration, and optimization including comparison with replica methods, practical implementation ste...">
    <meta property="og:site_name" content="Somaz Tech Blog">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="somaz">
    <meta name="msapplication-TileColor" content="#141414">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#141414">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:300,400,700" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="canonical" href="https://somaz.blog/category/storage/ceph-erasure-code/">
    <link rel="alternate" type="application/rss+xml" title="Somaz Tech Blog" href="https://somaz.blog/feed.xml" />

    <!-- Include extra styles -->
    

    <!-- JavaScript enabled/disabled -->
    <script>
        document.querySelector('html').classList.remove('no-js');
    </script>

    <!-- Google Adsense -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8725590811736154"
        crossorigin="anonymous"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet"> -->
    <!-- <link href="https://cdn.jsdelivr.net/gh/sunn-us/SUIT/fonts/variable/woff2/SUIT-Variable.css" rel="stylesheet"> -->
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css2?family=Albert+Sans:wght@400;500;700&display=swap" rel="stylesheet"> -->

    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml" />

</head>
<!-- ✅ Google Tag Manager (noscript) -->
<noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MBP83N4Q"
            height="0" width="0" style="display:none;visibility:hidden">
    </iframe>
</noscript>
<!-- ✅ End Google Tag Manager (noscript) -->
    <body class="has-push-menu">
        





        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 1000 1000"><path d="M969.8,870.3c27,27.7,27,71.8,0,99.1C955.7,983,937.9,990,920,990c-17.9,0-35.7-7-49.7-20.7L500,599L129.6,969.4C115.6,983,97.8,990,79.9,990s-35.7-7-49.7-20.7c-27-27.3-27-71.4,0-99.1L400.9,500L30.3,129.3c-27-27.3-27-71.4,0-99.1c27.3-27,71.8-27,99.4,0L500,400.9L870.4,30.2c27.7-27,71.8-27,99.4,0c27,27.7,27,71.8,0,99.1L599.1,500L969.8,870.3z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-clock" viewBox="0 0 1000 1000"><path d="M500,10C229.8,10,10,229.8,10,500c0,270.2,219.8,490,490,490c270.2,0,490-219.8,490-490C990,229.8,770.2,10,500,10z M500,910.2c-226.2,0-410.2-184-410.2-410.2c0-226.2,184-410.2,410.2-410.2c226.2,0,410.2,184,410.2,410.2C910.2,726.1,726.2,910.2,500,910.2z M753.1,374c8.2,11.9,5.2,28.1-6.6,36.3L509.9,573.7c-4.4,3.1-9.6,4.6-14.8,4.6c-4.1,0-8.3-1-12.1-3c-8.6-4.5-14-13.4-14-23.1V202.5c0-14.4,11.7-26.1,26.1-26.1c14.4,0,26.1,11.7,26.1,26.1v300l195.6-135.1C728.7,359.2,744.9,362.1,753.1,374z"/></symbol><symbol id="icon-calendar" viewBox="0 0 1000 1000"><path d="M920,500v420H80V500H920 M990,430H10v490c0,38.7,31.3,70,70,70h840c38.7,0,70-31.3,70-70V430L990,430z"/><path d="M850,80v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80H360v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80C72.8,80,10,142.7,10,220v140h980V220C990,142.7,927.2,80,850,80z"/><path d="M255,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C290,25.8,274.3,10,255,10z"/><path d="M745,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C780,25.8,764.3,10,745,10z"/></symbol><symbol id="icon-github" viewBox="0 0 12 14"><path d="M6 1q1.633 0 3.012 0.805t2.184 2.184 0.805 3.012q0 1.961-1.145 3.527t-2.957 2.168q-0.211 0.039-0.312-0.055t-0.102-0.234q0-0.023 0.004-0.598t0.004-1.051q0-0.758-0.406-1.109 0.445-0.047 0.801-0.141t0.734-0.305 0.633-0.52 0.414-0.82 0.16-1.176q0-0.93-0.617-1.609 0.289-0.711-0.062-1.594-0.219-0.070-0.633 0.086t-0.719 0.344l-0.297 0.187q-0.727-0.203-1.5-0.203t-1.5 0.203q-0.125-0.086-0.332-0.211t-0.652-0.301-0.664-0.105q-0.352 0.883-0.062 1.594-0.617 0.68-0.617 1.609 0 0.664 0.16 1.172t0.41 0.82 0.629 0.523 0.734 0.305 0.801 0.141q-0.305 0.281-0.383 0.805-0.164 0.078-0.352 0.117t-0.445 0.039-0.512-0.168-0.434-0.488q-0.148-0.25-0.379-0.406t-0.387-0.187l-0.156-0.023q-0.164 0-0.227 0.035t-0.039 0.090 0.070 0.109 0.102 0.094l0.055 0.039q0.172 0.078 0.34 0.297t0.246 0.398l0.078 0.18q0.102 0.297 0.344 0.48t0.523 0.234 0.543 0.055 0.434-0.027l0.18-0.031q0 0.297 0.004 0.691t0.004 0.426q0 0.141-0.102 0.234t-0.312 0.055q-1.812-0.602-2.957-2.168t-1.145-3.527q0-1.633 0.805-3.012t2.184-2.184 3.012-0.805zM2.273 9.617q0.023-0.055-0.055-0.094-0.078-0.023-0.102 0.016-0.023 0.055 0.055 0.094 0.070 0.047 0.102-0.016zM2.516 9.883q0.055-0.039-0.016-0.125-0.078-0.070-0.125-0.023-0.055 0.039 0.016 0.125 0.078 0.078 0.125 0.023zM2.75 10.234q0.070-0.055 0-0.148-0.062-0.102-0.133-0.047-0.070 0.039 0 0.141t0.133 0.055zM3.078 10.562q0.062-0.062-0.031-0.148-0.094-0.094-0.156-0.023-0.070 0.062 0.031 0.148 0.094 0.094 0.156 0.023zM3.523 10.758q0.023-0.086-0.102-0.125-0.117-0.031-0.148 0.055t0.102 0.117q0.117 0.047 0.148-0.047zM4.016 10.797q0-0.102-0.133-0.086-0.125 0-0.125 0.086 0 0.102 0.133 0.086 0.125 0 0.125-0.086zM4.469 10.719q-0.016-0.086-0.141-0.070-0.125 0.023-0.109 0.117t0.141 0.062 0.109-0.109z"></path></symbol><symbol id="icon-medium" viewBox="0 0 1000 1000"><path d="M336.5,240.2v641.5c0,9.1-2.3,16.9-6.8,23.2s-11.2,9.6-20,9.6c-6.2,0-12.2-1.5-18-4.4L37.3,782.7c-7.7-3.6-14.1-9.8-19.4-18.3S10,747.4,10,739V115.5c0-7.3,1.8-13.5,5.5-18.6c3.6-5.1,8.9-7.7,15.9-7.7c5.1,0,13.1,2.7,24.1,8.2l279.5,140C335.9,238.6,336.5,239.5,336.5,240.2L336.5,240.2z M371.5,295.5l292,473.6l-292-145.5V295.5z M990,305.3v576.4c0,9.1-2.6,16.5-7.7,22.1c-5.1,5.7-12,8.5-20.8,8.5s-17.3-2.4-25.7-7.1L694.7,784.9L990,305.3z M988.4,239.7c0,1.1-46.8,77.6-140.3,229.4C754.6,621,699.8,709.8,683.8,735.7L470.5,389l177.2-288.2c6.2-10.2,15.7-15.3,28.4-15.3c5.1,0,9.8,1.1,14.2,3.3l295.9,147.7C987.6,237.1,988.4,238.2,988.4,239.7L988.4,239.7z"/></symbol><symbol id="icon-instagram" viewBox="0 0 489.84 489.84"><path d="M249.62,50.46c65.4,0,73.14.25,99,1.43C372.47,53,385.44,57,394.07,60.32a75.88,75.88,0,0,1,28.16,18.32,75.88,75.88,0,0,1,18.32,28.16c3.35,8.63,7.34,21.6,8.43,45.48,1.18,25.83,1.43,33.57,1.43,99s-0.25,73.14-1.43,99c-1.09,23.88-5.08,36.85-8.43,45.48a81.11,81.11,0,0,1-46.48,46.48c-8.63,3.35-21.6,7.34-45.48,8.43-25.82,1.18-33.57,1.43-99,1.43s-73.15-.25-99-1.43c-23.88-1.09-36.85-5.08-45.48-8.43A75.88,75.88,0,0,1,77,423.86,75.88,75.88,0,0,1,58.69,395.7c-3.35-8.63-7.34-21.6-8.43-45.48-1.18-25.83-1.43-33.57-1.43-99s0.25-73.14,1.43-99c1.09-23.88,5.08-36.85,8.43-45.48A75.88,75.88,0,0,1,77,78.64a75.88,75.88,0,0,1,28.16-18.32c8.63-3.35,21.6-7.34,45.48-8.43,25.83-1.18,33.57-1.43,99-1.43m0-44.13c-66.52,0-74.86.28-101,1.47s-43.87,5.33-59.45,11.38A120.06,120.06,0,0,0,45.81,47.44,120.06,120.06,0,0,0,17.56,90.82C11.5,106.4,7.36,124.2,6.17,150.27s-1.47,34.46-1.47,101,0.28,74.86,1.47,101,5.33,43.87,11.38,59.45a120.06,120.06,0,0,0,28.25,43.38,120.06,120.06,0,0,0,43.38,28.25c15.58,6.05,33.38,10.19,59.45,11.38s34.46,1.47,101,1.47,74.86-.28,101-1.47,43.87-5.33,59.45-11.38a125.24,125.24,0,0,0,71.63-71.63c6.05-15.58,10.19-33.38,11.38-59.45s1.47-34.46,1.47-101-0.28-74.86-1.47-101-5.33-43.87-11.38-59.45a120.06,120.06,0,0,0-28.25-43.38,120.06,120.06,0,0,0-43.38-28.25C394.47,13.13,376.67,9,350.6,7.8s-34.46-1.47-101-1.47h0Z" transform="translate(-4.7 -6.33)" /><path d="M249.62,125.48A125.77,125.77,0,1,0,375.39,251.25,125.77,125.77,0,0,0,249.62,125.48Zm0,207.41a81.64,81.64,0,1,1,81.64-81.64A81.64,81.64,0,0,1,249.62,332.89Z" transform="translate(-4.7 -6.33)"/><circle cx="375.66" cy="114.18" r="29.39" /></symbol><symbol id="icon-linkedin" viewBox="0 0 12 14"><path d="M2.727 4.883v7.742h-2.578v-7.742h2.578zM2.891 2.492q0.008 0.57-0.395 0.953t-1.059 0.383h-0.016q-0.641 0-1.031-0.383t-0.391-0.953q0-0.578 0.402-0.957t1.051-0.379 1.039 0.379 0.398 0.957zM12 8.187v4.437h-2.57v-4.141q0-0.82-0.316-1.285t-0.988-0.465q-0.492 0-0.824 0.27t-0.496 0.668q-0.086 0.234-0.086 0.633v4.32h-2.57q0.016-3.117 0.016-5.055t-0.008-2.313l-0.008-0.375h2.57v1.125h-0.016q0.156-0.25 0.32-0.438t0.441-0.406 0.68-0.34 0.895-0.121q1.336 0 2.148 0.887t0.813 2.598z"></path></symbol><symbol id="icon-heart" viewBox="0 0 34 30"><path d="M17,29.7 L16.4,29.2 C3.5,18.7 0,15 0,9 C0,4 4,0 9,0 C13.1,0 15.4,2.3 17,4.1 C18.6,2.3 20.9,0 25,0 C30,0 34,4 34,9 C34,15 30.5,18.7 17.6,29.2 L17,29.7 Z M9,2 C5.1,2 2,5.1 2,9 C2,14.1 5.2,17.5 17,27.1 C28.8,17.5 32,14.1 32,9 C32,5.1 28.9,2 25,2 C21.5,2 19.6,4.1 18.1,5.8 L17,7.1 L15.9,5.8 C14.4,4.1 12.5,2 9,2 Z" id="Shape"></path></symbol><symbol id="icon-arrow-right" viewBox="0 0 25.452 25.452"><path d="M4.471,24.929v-2.004l12.409-9.788c0.122-0.101,0.195-0.251,0.195-0.411c0-0.156-0.073-0.31-0.195-0.409L4.471,2.526V0.522c0-0.2,0.115-0.384,0.293-0.469c0.18-0.087,0.396-0.066,0.552,0.061l15.47,12.202c0.123,0.1,0.195,0.253,0.195,0.409c0,0.16-0.072,0.311-0.195,0.411L5.316,25.34c-0.155,0.125-0.372,0.147-0.552,0.061C4.586,25.315,4.471,25.13,4.471,24.929z"/></symbol><symbol id="icon-star" viewBox="0 0 48 48"><path fill="currentColor" d="M44,24c0,11.045-8.955,20-20,20S4,35.045,4,24S12.955,4,24,4S44,12.955,44,24z"/><path fill="#ffffff" d="M24,11l3.898,7.898l8.703,1.301l-6.301,6.102l1.5,8.699L24,30.898L16.199,35l1.5-8.699l-6.301-6.102  l8.703-1.301L24,11z"/></symbol><symbol id="icon-read" viewBox="0 0 32 32"><path fill="currentColor" d="M29,4H3C1.343,4,0,5.343,0,7v18c0,1.657,1.343,3,3,3h10c0,0.552,0.448,1,1,1h4c0.552,0,1-0.448,1-1h10  c1.657,0,3-1.343,3-3V7C32,5.343,30.657,4,29,4z M29,5v20H18.708c-0.618,0-1.236,0.146-1.789,0.422l-0.419,0.21V5H29z M15.5,5  v20.632l-0.419-0.21C14.528,25.146,13.91,25,13.292,25H3V5H15.5z M31,25c0,1.103-0.897,2-2,2H18v1h-4v-1H3c-1.103,0-2-0.897-2-2V7  c0-0.737,0.405-1.375,1-1.722V25c0,0.552,0.448,1,1,1h10.292c0.466,0,0.925,0.108,1.342,0.317l0.919,0.46  c0.141,0.07,0.294,0.106,0.447,0.106c0.153,0,0.306-0.035,0.447-0.106l0.919-0.46C17.783,26.108,18.242,26,18.708,26H29  c0.552,0,1-0.448,1-1V5.278C30.595,5.625,31,6.263,31,7V25z M6,12.5C6,12.224,6.224,12,6.5,12h5c0.276,0,0.5,0.224,0.5,0.5  S11.776,13,11.5,13h-5C6.224,13,6,12.776,6,12.5z M6,14.5C6,14.224,6.224,14,6.5,14h5c0.276,0,0.5,0.224,0.5,0.5S11.776,15,11.5,15  h-5C6.224,15,6,14.776,6,14.5z M6,16.5C6,16.224,6.224,16,6.5,16h5c0.276,0,0.5,0.224,0.5,0.5S11.776,17,11.5,17h-5  C6.224,17,6,16.776,6,16.5z M20,12.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,13,25.5,13h-5  C20.224,13,20,12.776,20,12.5z M20,14.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,15,25.5,15h-5  C20.224,15,20,14.776,20,14.5z M20,16.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,17,25.5,17h-5  C20.224,17,20,16.776,20,16.5z"></path></symbol><symbol id="icon-tistory" viewBox="0 0 24 24"><path d="M4 4h16v3h-6v13h-4V7H4V4z"/></symbol></defs></svg>

        <header class="bar-header">
    <a id="menu" role="button">
        <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    </a>
    <h1 class="logo">
        <a href="/">
            
                somaz <span class="version">v3.1.2</span>
            
        </a>
    </h1>
    <a id="search" class="dosearch" role="button">
        <svg class="icon-search"><use xlink:href="#icon-search"></use></svg>
    </a>
    
        <a href="https://github.com/thiagorossener/jekflix-template" class="get-theme" role="button">
            Get this theme!
        </a>
    
</header>

<div id="mask" class="overlay"></div>

<aside class="sidebar" id="sidebar">
    <nav id="navigation">
      <h2>Menu</h2>
      <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>

    </nav>
</aside>

<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>



        <section class="post two-columns">
            <article role="article" class="post-content">
                <p class="post-info">
                    
                        <svg class="icon-calendar" id="date"><use xlink:href="#icon-calendar"></use></svg>
                        <time class="date" datetime="2025-11-08T00:00:00+00:00">
                            


November 8, 2025

                        </time>
                    
                    <svg id="clock" class="icon-clock"><use xlink:href="#icon-clock"></use></svg>
                    <span>13 min to read</span>
                </p>
                <h1 class="post-title">Advanced Ceph Erasure Code Configuration and Performance Optimization</h1>
                <p class="post-subtitle">Complete guide to implementing Erasure Code in Ceph clusters for storage efficiency and data protection</p>

                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755133935/ceph-erasure-code_kheagu.png" alt="Featured image" class="post-cover">
                

                <!-- Pagination links -->



                <!-- Add your table of contents here -->


                <p><br /></p>

<hr />

<h2 id="overview">Overview</h2>

<p>In large-scale storage systems, achieving both <strong>reliability</strong> and <strong>efficiency</strong> simultaneously presents significant challenges. Ceph addresses this through two distinct data protection methods: <strong>Replica</strong> and <strong>Erasure Code</strong>.</p>

<p>This article focuses primarily on <strong>Erasure Code</strong> configuration and practical implementation, highlighting its superior storage efficiency. We’ll explore the fundamental differences between Replica and Erasure Code approaches, provide step-by-step configuration guides, and address common troubleshooting scenarios.</p>

<p>Erasure Code offers substantial storage savings while maintaining high availability, making it ideal for environments where storage cost optimization is critical without compromising data durability.</p>

<p><br /></p>

<hr />

<h2 id="what-is-erasure-code">What is Erasure Code?</h2>

<p>Erasure Code is a data protection method that divides data into multiple chunks (k) and adds recovery chunks (m), storing the complete dataset across <code class="language-plaintext highlighter-rouge">N = k + m</code> blocks.</p>

<p>This approach enables data recovery even when some blocks are lost, providing resilience through mathematical redundancy rather than simple replication.</p>

<p><br /></p>

<h3 id="key-characteristics">Key Characteristics:</h3>

<ul>
  <li><strong>Data Segmentation</strong>: Original data split into k data chunks</li>
  <li><strong>Parity Generation</strong>: m parity chunks calculated from data chunks</li>
  <li><strong>Fault Tolerance</strong>: Can survive up to m chunk failures</li>
  <li><strong>Storage Efficiency</strong>: Significantly better than traditional replication</li>
</ul>

<p><br /></p>

<h3 id="example-configuration">Example Configuration:</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># k=3, m=2 configuration</span>
<span class="c"># - 3 data chunks + 2 parity chunks = 5 total chunks</span>
<span class="c"># - Can survive up to 2 chunk failures</span>
<span class="c"># - Storage efficiency: 3/5 = 60% (vs 33% for 3-way replication)</span>
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="replica-vs-erasure-code-comparison">Replica vs Erasure Code Comparison</h2>

<div class="table-container">
  <table class="table-modern">
    <tr>
      <th>Aspect</th>
      <th>Replica Method</th>
      <th>Erasure Code Method</th>
    </tr>
    <tr>
      <td><strong>Data Protection</strong></td>
      <td>Identical data copies</td>
      <td>Data segmentation + parity</td>
    </tr>
    <tr>
      <td><strong>Storage Efficiency</strong></td>
      <td>Very Low (33% for 3-replica)</td>
      <td>High (60% for k=3,m=2)</td>
    </tr>
    <tr>
      <td><strong>Recovery Speed</strong></td>
      <td>Fast (simple copy)</td>
      <td>Slower (parity calculation)</td>
    </tr>
    <tr>
      <td><strong>CPU/Compute Load</strong></td>
      <td>Low</td>
      <td>High (parity operations)</td>
    </tr>
    <tr>
      <td><strong>Network Overhead</strong></td>
      <td>High (full data copies)</td>
      <td>Lower (distributed chunks)</td>
    </tr>
    <tr>
      <td><strong>Suitable Environment</strong></td>
      <td>Fast recovery critical</td>
      <td>Storage space optimization</td>
    </tr>
    <tr>
      <td><strong>Minimum OSDs</strong></td>
      <td>Equal to replica count</td>
      <td>k + m</td>
    </tr>
    <tr>
      <td><strong>Failure Tolerance</strong></td>
      <td>n-1 failures</td>
      <td>m failures</td>
    </tr>
  </table>
  <div class="caption">Comparison between Replica and Erasure Code strategies</div>
  
</div>

<p><br /></p>

<hr />

<h2 id="infrastructure-requirements">Infrastructure Requirements</h2>

<p><br /></p>

<h3 id="minimum-cluster-configuration">Minimum Cluster Configuration</h3>

<div class="table-container">
  <table class="table-modern">
    <tr>
      <th>Component</th>
      <th>Requirement</th>
      <th>Recommendation</th>
    </tr>
    <tr>
      <td><strong>OSDs</strong></td>
      <td>k + m (minimum)</td>
      <td>k + m + 2 (for maintenance)</td>
    </tr>
    <tr>
      <td><strong>Memory</strong></td>
      <td>4GB per OSD</td>
      <td>8GB per OSD</td>
    </tr>
    <tr>
      <td><strong>Network</strong></td>
      <td>1Gbps</td>
      <td>10Gbps</td>
    </tr>
    <tr>
      <td><strong>CPU</strong></td>
      <td>1 core per OSD</td>
      <td>2 cores per OSD</td>
    </tr>
    <tr>
      <td><strong>Failure Domains</strong></td>
      <td>k + m distinct domains</td>
      <td>Well-distributed topology</td>
    </tr>
  </table>
  <div class="caption">Minimum hardware and topology recommendations</div>
</div>

<p><br /></p>

<h3 id="example-cluster-topology">Example Cluster Topology</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 3-node cluster example for k=3, m=2</span>
Node 1: 2 OSDs <span class="o">(</span>rack-1<span class="o">)</span>
Node 2: 2 OSDs <span class="o">(</span>rack-2<span class="o">)</span>  
Node 3: 1 OSD <span class="o">(</span>rack-3<span class="o">)</span>
Total: 5 OSDs across 3 failure domains
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="erasure-code-profile-configuration">Erasure Code Profile Configuration</h2>

<p><br /></p>

<h3 id="step-1-verify-cluster-status">Step 1: Verify Cluster Status</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check Ceph version</span>
ceph <span class="nt">--version</span>

<span class="c"># Verify cluster health</span>
ceph <span class="nt">-s</span>

<span class="c"># Check OSD status</span>
ceph osd tree

<span class="c"># Verify available failure domains</span>
ceph osd crush tree
</code></pre></div></div>

<p><br /></p>

<h3 id="step-2-create-erasure-code-profile">Step 2: Create Erasure Code Profile</h3>

<script src="https://gist.github.com/somaz94/24cb13b9266c69cc0f02a3d231ab6676.js"></script>

<p><br /></p>

<h4 id="profile-parameters-explanation">Profile Parameters Explanation:</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Core Parameters</span>
<span class="nv">k</span><span class="o">=</span>3                        <span class="c"># Number of data chunks</span>
<span class="nv">m</span><span class="o">=</span>2                        <span class="c"># Number of parity chunks</span>
<span class="nv">plugin</span><span class="o">=</span>jerasure           <span class="c"># Erasure coding algorithm</span>

<span class="c"># Advanced Parameters</span>
<span class="nv">technique</span><span class="o">=</span>reed_sol_van    <span class="c"># Specific technique within plugin</span>
crush-failure-domain<span class="o">=</span>host <span class="c"># Failure domain for chunk placement</span>
crush-device-class<span class="o">=</span>ssd    <span class="c"># Restrict to specific device class</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="step-3-verify-profile-creation">Step 3: Verify Profile Creation</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List all erasure code profiles</span>
ceph osd erasure-code-profile <span class="nb">ls</span>

<span class="c"># Display profile details</span>
ceph osd erasure-code-profile get ec-profile-advanced

<span class="c"># Expected output:</span>
<span class="c"># crush-device-class=ssd</span>
<span class="c"># crush-failure-domain=host</span>
<span class="c"># k=4</span>
<span class="c"># m=2</span>
<span class="c"># plugin=jerasure</span>
<span class="c"># technique=reed_sol_van</span>
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="pool-creation-and-management">Pool Creation and Management</h2>

<p><br /></p>

<h3 id="step-1-calculate-placement-groups-pgs">Step 1: Calculate Placement Groups (PGs)</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Formula: (OSDs * 100) / (k + m) / pool_count</span>
<span class="c"># Example: (12 OSDs * 100) / 6 / 2 pools = 100 PGs</span>
<span class="c"># Round to nearest power of 2: 128 PGs</span>

<span class="c"># For production environments, use Ceph PG calculator:</span>
<span class="c"># https://ceph.io/pgcalc/</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="step-2-create-erasure-code-pool">Step 2: Create Erasure Code Pool</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create pool with calculated PGs</span>
ceph osd pool create ec-pool-storage 128 erasure ec-profile-advanced

<span class="c"># Verify pool creation</span>
ceph osd pool <span class="nb">ls </span>detail | <span class="nb">grep </span>ec-pool-storage

<span class="c"># Check pool status</span>
ceph osd pool get ec-pool-storage all
</code></pre></div></div>

<p><br /></p>

<h3 id="step-3-configure-pool-for-applications">Step 3: Configure Pool for Applications</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Enable for RGW (Object Storage)</span>
ceph osd pool application <span class="nb">enable </span>ec-pool-storage rgw

<span class="c"># Enable for RBD (Block Storage) - requires metadata pool</span>
ceph osd pool create ec-pool-metadata 32 replicated
ceph osd pool application <span class="nb">enable </span>ec-pool-metadata rbd
ceph osd pool application <span class="nb">enable </span>ec-pool-storage rbd

<span class="c"># Enable for CephFS (File Storage)</span>
ceph osd pool application <span class="nb">enable </span>ec-pool-storage cephfs
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="performance-optimization">Performance Optimization</h2>

<p><br /></p>

<h3 id="step-1-crush-map-optimization">Step 1: CRUSH Map Optimization</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Extract current CRUSH map</span>
ceph osd getcrushmap <span class="nt">-o</span> crushmap.bin
crushtool <span class="nt">-d</span> crushmap.bin <span class="nt">-o</span> crushmap.txt

<span class="c"># Edit crushmap.txt to optimize placement rules</span>
<span class="c"># Add custom rules for erasure code pools</span>

<span class="c"># Compile and inject updated CRUSH map</span>
crushtool <span class="nt">-c</span> crushmap.txt <span class="nt">-o</span> crushmap-new.bin
ceph osd setcrushmap <span class="nt">-i</span> crushmap-new.bin
</code></pre></div></div>

<h4 id="custom-crush-rule-example">Custom CRUSH Rule Example:</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Custom rule in crushmap.txt</span>
rule ec_ssd_rule <span class="o">{</span>
    <span class="nb">id </span>1
    <span class="nb">type </span>erasure
    min_size 3
    max_size 6
    step take default class ssd
    step chooseleaf firstn 0 <span class="nb">type </span>host
    step emit
<span class="o">}</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="step-2-configure-pool-parameters">Step 2: Configure Pool Parameters</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Set optimal parameters for erasure code pool</span>
ceph osd pool <span class="nb">set </span>ec-pool-storage pg_num 128
ceph osd pool <span class="nb">set </span>ec-pool-storage pgp_num 128

<span class="c"># Optimize for read performance</span>
ceph osd pool <span class="nb">set </span>ec-pool-storage fast_read <span class="nb">true</span>

<span class="c"># Configure compression (optional)</span>
ceph osd pool <span class="nb">set </span>ec-pool-storage compression_algorithm zstd
ceph osd pool <span class="nb">set </span>ec-pool-storage compression_mode force
</code></pre></div></div>

<p><br /></p>

<h3 id="step-3-monitor-performance-metrics">Step 3: Monitor Performance Metrics</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check pool statistics</span>
ceph <span class="nb">df </span>detail

<span class="c"># Monitor I/O performance</span>
ceph osd perf

<span class="c"># Check placement group distribution</span>
ceph pg dump | <span class="nb">grep </span>ec-pool-storage

<span class="c"># Monitor erasure code operations</span>
ceph daemon osd.0 perf dump | <span class="nb">grep </span>erasure
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="rbd-integration-with-erasure-code">RBD Integration with Erasure Code</h2>

<p><br /></p>

<h3 id="step-1-create-metadata-and-data-pools">Step 1: Create Metadata and Data Pools</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create replicated metadata pool</span>
ceph osd pool create rbd-metadata 32 replicated

<span class="c"># Create erasure code data pool</span>
ceph osd pool create rbd-ec-data 128 erasure ec-profile-advanced

<span class="c"># Enable RBD application</span>
ceph osd pool application <span class="nb">enable </span>rbd-metadata rbd
ceph osd pool application <span class="nb">enable </span>rbd-ec-data rbd
</code></pre></div></div>

<p><br /></p>

<h3 id="step-2-configure-rbd-with-erasure-code-backend">Step 2: Configure RBD with Erasure Code Backend</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create RBD image with erasure code data pool</span>
rbd create <span class="nt">--size</span> 10G <span class="nt">--data-pool</span> rbd-ec-data rbd-metadata/test-image

<span class="c"># Verify image configuration</span>
rbd info rbd-metadata/test-image

<span class="c"># Expected output:</span>
<span class="c"># rbd image 'test-image':</span>
<span class="c">#     size 10 GiB in 2560 objects</span>
<span class="c">#     order 22 (4 MiB objects)</span>
<span class="c">#     data_pool: rbd-ec-data</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="step-3-kubernetes-storageclass-configuration">Step 3: Kubernetes StorageClass Configuration</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># rbd-ec-storageclass.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">rbd-erasure-code</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="na">storageclass.beta.kubernetes.io/is-default-class</span><span class="pi">:</span> <span class="s2">"</span><span class="s">false"</span>
<span class="na">provisioner</span><span class="pi">:</span> <span class="s">rbd.csi.ceph.com</span>
<span class="na">parameters</span><span class="pi">:</span>
  <span class="na">clusterID</span><span class="pi">:</span> <span class="s2">"</span><span class="s">b9127830-b0cc-4e34-aa47-9d1a2e9949a8"</span>
  <span class="na">pool</span><span class="pi">:</span> <span class="s2">"</span><span class="s">rbd-metadata"</span>
  <span class="na">dataPool</span><span class="pi">:</span> <span class="s2">"</span><span class="s">rbd-ec-data"</span>
  <span class="na">imageFeatures</span><span class="pi">:</span> <span class="s">layering</span>
  <span class="na">csi.storage.k8s.io/provisioner-secret-name</span><span class="pi">:</span> <span class="s">csi-rbd-secret</span>
  <span class="na">csi.storage.k8s.io/provisioner-secret-namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
  <span class="na">csi.storage.k8s.io/controller-expand-secret-name</span><span class="pi">:</span> <span class="s">csi-rbd-secret</span>
  <span class="na">csi.storage.k8s.io/controller-expand-secret-namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
  <span class="na">csi.storage.k8s.io/node-stage-secret-name</span><span class="pi">:</span> <span class="s">csi-rbd-secret</span>
  <span class="na">csi.storage.k8s.io/node-stage-secret-namespace</span><span class="pi">:</span> <span class="s">kube-system</span>
  <span class="na">csi.storage.k8s.io/fstype</span><span class="pi">:</span> <span class="s">ext4</span>
<span class="na">reclaimPolicy</span><span class="pi">:</span> <span class="s">Delete</span>
<span class="na">allowVolumeExpansion</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">mountOptions</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">discard</span>
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="troubleshooting-common-issues">Troubleshooting Common Issues</h2>

<p><br /></p>

<h3 id="issue-1-degraded-data-redundancy">Issue 1: Degraded Data Redundancy</h3>

<h4 id="symptoms">Symptoms:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>HEALTH_WARN
Degraded data redundancy: 122 pgs undersized
</code></pre></div></div>

<h4 id="root-causes">Root Causes:</h4>
<ul>
  <li>Insufficient OSDs for k+m configuration</li>
  <li>OSDs not distributed across required failure domains</li>
  <li>OSD failures exceeding tolerance threshold</li>
</ul>

<h4 id="resolution-steps">Resolution Steps:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check OSD distribution</span>
ceph osd tree

<span class="c"># Verify failure domain configuration</span>
ceph osd crush tree

<span class="c"># Add more OSDs if needed</span>
ceph-volume lvm create <span class="nt">--data</span> /dev/sdX

<span class="c"># Check PG mapping</span>
ceph pg ls-by-pool ec-pool-storage | <span class="nb">head</span> <span class="nt">-10</span>

<span class="c"># Force PG repair if necessary</span>
ceph pg repair &lt;pg_id&gt;
</code></pre></div></div>

<p><br /></p>

<h3 id="issue-2-pool-application-not-enabled">Issue 2: Pool Application Not Enabled</h3>

<h4 id="symptoms-1">Symptoms:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>HEALTH_WARN
1 pool<span class="o">(</span>s<span class="o">)</span> <span class="k">do </span>not have an application enabled
</code></pre></div></div>

<h4 id="resolution">Resolution:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Enable appropriate application</span>
ceph osd pool application <span class="nb">enable </span>ec-pool-storage rgw

<span class="c"># Verify application status</span>
ceph osd pool application get ec-pool-storage
</code></pre></div></div>

<p><br /></p>

<h3 id="issue-3-slow-recovery-performance">Issue 3: Slow Recovery Performance</h3>

<h4 id="symptoms-2">Symptoms:</h4>
<ul>
  <li>Extended recovery times</li>
  <li>High CPU utilization during recovery</li>
  <li>Network bandwidth saturation</li>
</ul>

<h4 id="optimization-steps">Optimization Steps:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Adjust recovery parameters</span>
ceph config <span class="nb">set </span>osd osd_recovery_max_active 3
ceph config <span class="nb">set </span>osd osd_max_backfills 1
ceph config <span class="nb">set </span>osd osd_recovery_sleep 0.1

<span class="c"># Monitor recovery progress</span>
ceph <span class="nt">-s</span> | <span class="nb">grep </span>recovery

<span class="c"># Check individual OSD performance</span>
ceph daemon osd.0 perf dump | <span class="nb">grep </span>recovery
</code></pre></div></div>

<p><br /></p>

<h3 id="issue-4-crush-rule-conflicts">Issue 4: CRUSH Rule Conflicts</h3>

<h4 id="symptoms-3">Symptoms:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>HEALTH_ERR
Pool <span class="s1">'ec-pool-storage'</span> has no available OSDs <span class="k">for </span>pg X.Y
</code></pre></div></div>

<h4 id="resolution-1">Resolution:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check current CRUSH rules</span>
ceph osd crush rule <span class="nb">ls
</span>ceph osd crush rule dump

<span class="c"># Verify pool's CRUSH rule</span>
ceph osd pool get ec-pool-storage crush_rule

<span class="c"># Create appropriate CRUSH rule</span>
ceph osd crush rule create-erasure ec-rule ec-profile-advanced

<span class="c"># Apply rule to pool</span>
ceph osd pool <span class="nb">set </span>ec-pool-storage crush_rule ec-rule
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="monitoring-and-maintenance">Monitoring and Maintenance</h2>

<p><br /></p>

<h3 id="step-1-setup-monitoring-scripts">Step 1: Setup Monitoring Scripts</h3>

<script src="https://gist.github.com/somaz94/b16417e537dd93dc2d8e6586d27317bc.js"></script>

<p><br /></p>

<h3 id="step-2-automated-health-checks">Step 2: Automated Health Checks</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create cron job for regular monitoring</span>
<span class="nb">cat</span> <span class="o">&gt;</span> /etc/cron.d/ceph-monitoring <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">'
# Monitor Ceph erasure code pools every 15 minutes
*/15 * * * * root /usr/local/bin/ceph-ec-monitor.sh &gt;&gt; /var/log/ceph-monitoring.log 2&gt;&amp;1
</span><span class="no">EOF
</span></code></pre></div></div>

<p><br /></p>

<h3 id="step-3-performance-baselines">Step 3: Performance Baselines</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Establish performance baselines</span>
<span class="nb">echo</span> <span class="s2">"=== Storage Efficiency Comparison ==="</span> <span class="o">&gt;</span> /var/log/ceph-baseline.log

<span class="c"># Calculate storage efficiency</span>
<span class="nv">TOTAL_RAW</span><span class="o">=</span><span class="si">$(</span>ceph <span class="nb">df</span> | <span class="nb">grep </span>TOTAL | <span class="nb">awk</span> <span class="s1">'{print $2}'</span><span class="si">)</span>
<span class="nv">TOTAL_USED</span><span class="o">=</span><span class="si">$(</span>ceph <span class="nb">df</span> | <span class="nb">grep </span>TOTAL | <span class="nb">awk</span> <span class="s1">'{print $4}'</span><span class="si">)</span>
<span class="nv">EFFICIENCY</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="s2">"scale=2; </span><span class="nv">$TOTAL_USED</span><span class="s2"> / </span><span class="nv">$TOTAL_RAW</span><span class="s2"> * 100"</span> | bc<span class="si">)</span>

<span class="nb">echo</span> <span class="s2">"Raw Storage: </span><span class="nv">$TOTAL_RAW</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /var/log/ceph-baseline.log
<span class="nb">echo</span> <span class="s2">"Used Storage: </span><span class="nv">$TOTAL_USED</span><span class="s2">"</span> <span class="o">&gt;&gt;</span> /var/log/ceph-baseline.log
<span class="nb">echo</span> <span class="s2">"Efficiency: </span><span class="nv">$EFFICIENCY</span><span class="s2">%"</span> <span class="o">&gt;&gt;</span> /var/log/ceph-baseline.log

<span class="c"># I/O performance baseline</span>
rados bench <span class="nt">-p</span> ec-pool-storage 30 write <span class="o">&gt;</span> /var/log/ceph-write-baseline.log
rados bench <span class="nt">-p</span> ec-pool-storage 30 <span class="nb">seq</span> <span class="o">&gt;</span> /var/log/ceph-read-baseline.log
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="best-practices-and-recommendations">Best Practices and Recommendations</h2>

<p><br /></p>

<h3 id="production-deployment-guidelines">Production Deployment Guidelines</h3>

<ol>
  <li><strong>Capacity Planning</strong>
    <ul>
      <li>Plan for k+m+2 OSDs minimum for maintenance windows</li>
      <li>Consider network bandwidth for erasure code operations</li>
      <li>Account for increased CPU requirements</li>
    </ul>
  </li>
  <li><strong>Failure Domain Design</strong>
    <ul>
      <li>Distribute OSDs across physical racks or availability zones</li>
      <li>Ensure each failure domain can handle chunk placement</li>
      <li>Plan for correlated failures (power, network)</li>
    </ul>
  </li>
  <li><strong>Performance Optimization</strong>
    <ul>
      <li>Use fast SSDs for metadata pools</li>
      <li>Optimize CRUSH rules for data locality</li>
      <li>Monitor and tune recovery parameters</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<h3 id="when-to-choose-erasure-code">When to Choose Erasure Code</h3>

<div class="table-container">
  <table class="table-modern">
    <tr>
      <th>Scenario</th>
      <th>Recommendation</th>
      <th>Rationale</th>
    </tr>
    <tr>
      <td><strong>Cold Storage</strong></td>
      <td>Erasure Code</td>
      <td>Storage efficiency priority</td>
    </tr>
    <tr>
      <td><strong>Backup Systems</strong></td>
      <td>Erasure Code</td>
      <td>Cost optimization critical</td>
    </tr>
    <tr>
      <td><strong>Archive Data</strong></td>
      <td>Erasure Code</td>
      <td>Infrequent access patterns</td>
    </tr>
    <tr>
      <td><strong>Hot Database</strong></td>
      <td>Replica</td>
      <td>Performance critical</td>
    </tr>
    <tr>
      <td><strong>Real-time Analytics</strong></td>
      <td>Replica</td>
      <td>Low latency required</td>
    </tr>
    <tr>
      <td><strong>Mixed Workloads</strong></td>
      <td>Hybrid Approach</td>
      <td>Balance efficiency and performance</td>
    </tr>
  </table>
  <div class="caption">Guidelines for selecting Replica vs Erasure Code</div>
</div>

<p><br /></p>

<h3 id="configuration-templates">Configuration Templates</h3>

<script src="https://gist.github.com/somaz94/1b060c16de13bede2a7dfc334fde83e9.js"></script>

<p><br /></p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<div class="quote-box">
  <p>Erasure Code represents a significant advancement in storage efficiency for Ceph clusters, offering substantial cost savings while maintaining high durability.</p>

  <p>The configuration complexity is offset by dramatic improvements in storage utilization, making it essential for large-scale deployments.</p>
</div>

<p><br /></p>

<h3 id="key-achievements">Key Achievements:</h3>

<ol>
  <li><strong>Storage Efficiency</strong>: Up to 60-80% storage utilization vs 33% with 3-way replication</li>
  <li><strong>Scalability</strong>: Better resource utilization for large datasets</li>
  <li><strong>Flexibility</strong>: Multiple profiles for different use cases</li>
  <li><strong>Integration</strong>: Seamless Kubernetes and application integration</li>
</ol>

<p><br /></p>

<h3 id="operational-benefits">Operational Benefits:</h3>

<ul>
  <li><strong>Cost Reduction</strong>: Significant hardware cost savings</li>
  <li><strong>Data Durability</strong>: Configurable failure tolerance</li>
  <li><strong>Performance Optimization</strong>: Tunable parameters for different workloads</li>
  <li><strong>Automated Management</strong>: Integration with existing Ceph tools</li>
</ul>

<p><br /></p>

<h4 id="future-considerations">Future Considerations:</h4>
<ul>
  <li>Evaluate emerging erasure code algorithms</li>
  <li>Implement automated tier migration between replica and erasure code pools</li>
  <li>Develop application-specific optimization profiles</li>
  <li>Consider hybrid storage architectures</li>
</ul>

<p>Mastering Erasure Code configuration enables organizations to build cost-effective, highly durable storage systems that scale efficiently with growing data requirements.</p>

<blockquote>
  <p><em>“Erasure Code transforms storage economics by delivering enterprise-grade durability at a fraction of traditional replication costs.”</em></p>
</blockquote>

<p><br /></p>

<hr />

<h2 id="references">References</h2>

<ul>
  <li><a href="https://docs.ceph.com/en/latest/rados/operations/erasure-code/">Ceph Erasure Code Documentation</a></li>
  <li><a href="https://docs.ceph.com/en/latest/rados/operations/crush-map/">CRUSH Map Documentation</a></li>
  <li><a href="https://docs.ceph.com/en/latest/rados/configuration/">Ceph Performance Tuning Guide</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Erasure_code">Erasure Code Theory and Implementation</a></li>
  <li><a href="https://docs.ceph.com/en/latest/rbd/rbd-config-ref/">Ceph RBD Erasure Code Pools</a></li>
  <li><a href="https://kubernetes-csi.github.io/docs/">Kubernetes CSI Integration</a></li>
</ul>


                <!-- Pagination links -->


            </article>

            
                <aside class="see-also">
                    <h2>See also</h2>
                    <ul>
                        
                        
                        
                            <li>
                                <a href="/category/virtualization/xen/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755589744/xen-1_ipuu6j.png">
                                    
                                    <h3>Xen Orchestra Complete Guide - XCP-ng Web Management Platform</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/data-engineering/oltp-olap/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755575326/oltp-olap_gqmvnx.png">
                                    
                                    <h3>Large-Scale Data Processing and Data Architecture Design</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/virtualization/kvm-nested/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755588581/kvm-nested_efue7m.png">
                                    
                                    <h3>KVM Nested Virtualization Complete Guide</h3>
                                </a>
                            </li>
                        
                    </ul>
                </aside>
            

        </section>

        <!-- Add time bar only for pages without pagination -->
        
            <div class="time-bar" data-minutes="13">
    <span class="time-completed"></span>
    <span class="time-remaining"></span>
    <div class="bar">
        <span class="completed" style="width:0%;"></span>
        <span class="remaining" style="width:100%;"></span>
    </div>
</div>

            <button class="toggle-preview" onclick="togglePreview()">
    <span>Hide Preview ▼</span>
</button>

<div id="recommendationSection" class="recommendation">
    <div class="message">
        <strong>Why don't you read something next?</strong>
        <div>
            <button>
                <svg><use xlink:href="#icon-arrow-right"></use></svg>
                <span>Go back to top</span>
            </button>
        </div>
    </div>
    <div id="previewSection" class="preview-section">
        
        <a href="/category/kubernetes/cilium-deep-dive/" class="post-preview">
            <div class="image">
                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755056273/cilium-1_h8xe9p.png">
                
            </div>
            <h3 class="title">Cilium Deep Dive - Advanced eBPF-based Kubernetes Networking and Security</h3>
        </a>
    </div>
</div>

<style>
.toggle-preview {
    position: fixed;
    bottom: 20px;
    right: 20px;
    background: #333;
    color: white;
    border: none;
    padding: 8px 15px;
    border-radius: 4px;
    cursor: pointer;
    z-index: 1000;
    opacity: 0;
    transition: opacity 0.3s ease;
}

.toggle-preview:hover {
    background: #444;
}

.toggle-preview.visible {
    opacity: 1;
}

.recommendation {
    margin-top: 1000px;
    display: block;
    transition: all 0.3s ease;
}

.recommendation.hidden {
    display: none;
}

.hide-preview {
    margin-left: 10px;
    background: none;
    border: 1px solid #666;
    color: #666;
    padding: 5px 10px;
    border-radius: 4px;
    cursor: pointer;
}

.hide-preview:hover {
    background: #f0f0f0;
}

.preview-section {
    max-height: 1000px;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
}

.preview-section.hidden {
    max-height: 0;
}
</style>

<script>
function togglePreview() {
    const recommendation = document.getElementById('recommendationSection');
    const button = document.querySelector('.toggle-preview span');
    
    if (recommendation.classList.contains('hidden')) {
        recommendation.classList.remove('hidden');
        button.textContent = 'Hide Preview ▼';
    } else {
        recommendation.classList.add('hidden');
        button.textContent = 'Show Preview ▲';
    }
}

window.addEventListener('scroll', function() {
    const toggleButton = document.querySelector('.toggle-preview');
    const recommendation = document.getElementById('recommendationSection');
    const rect = recommendation.getBoundingClientRect();
    
    if (rect.top <= window.innerHeight) {
        toggleButton.classList.add('visible');
    } else {
        toggleButton.classList.remove('visible');
    }
});
</script>

        

        <!-- Show modal if the post is the last one -->
        

        <!-- Show modal before user leaves the page -->
        

        <!-- Add your newsletter subscription form here -->

        <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;Comprehensive walkthrough of Ceph Erasure Code setup, configuration, and optimization including comparison with replica methods, practical implementation steps, and troubleshooting common issues.&quot;%20https://somaz.blog/category/storage/ceph-erasure-code/%20via%20&#64;twitter_username&hashtags=ceph,erasure-code,storage-efficiency,data-protection,performance-optimization,distributed-storage,infrastructure,automation"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://somaz.blog/category/storage/ceph-erasure-code/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

        

  <section class="author">
    <div class="details">
      
        <img class="img-rounded" src="/assets/img/uploads/profile.png" alt="Somaz">
      
      <p class="def">Author</p>
      <h3 class="name">
        <a href="/authors/somaz/">Somaz</a>
      </h3>
      <p class="desc">DevOps engineer focused on cloud infrastructure and automation</p>
      <p>
        
          <a href="https://github.com/somaz94" title="Github">
            <svg><use xlink:href="#icon-github"></use></svg>
          </a>
        
        
        
        
        
        
          <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
            <svg><use xlink:href="#icon-linkedin"></use></svg>
          </a>
        
        
          <a href="https://somaz.tistory.com" title="Tistory">
            <svg><use xlink:href="#icon-tistory"></use></svg>
          </a>
        
      </p>
    </div>
  </section>

  
  
  
  
  
  
  
  

  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Somaz",
      
      "image": "/assets/img/uploads/profile.png",
      
      "jobTitle": "DevOps Engineer",
      "url": "https://somaz.blog/authors/somaz/",
      "sameAs": [
        "https://github.com/somaz94","https://www.linkedin.com/in/somaz","https://{{ author.tistory_username }}.tistory.com"
      ]
  }
  </script>


        

<section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'https-somaz94-github-io';
        var disqus_title = '';
        var disqus_url = '/category/storage/ceph-erasure-code/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>



        <footer>
    <p>
      
        <a href="https://github.com/somaz94" title="Github">
          <svg><use xlink:href="#icon-github"></use></svg>
        </a>
      
      
        <a href="https://www.facebook.com/facebook_username" title="Facebook">
          <svg><use xlink:href="#icon-facebook"></use></svg>
        </a>
      
      
        <a href="https://twitter.com/twitter_username" title="Twitter">
          <svg><use xlink:href="#icon-twitter"></use></svg>
        </a>
      
      
        <a href="https://medium.com/@medium_username" title="Medium">
          <svg><use xlink:href="#icon-medium"></use></svg>
        </a>
      
      
        <a href="https://www.instagram.com/instagram_username" title="Instagram">
          <svg><use xlink:href="#icon-instagram"></use></svg>
        </a>
      
      
        <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
          <svg><use xlink:href="#icon-linkedin"></use></svg>
        </a>
      
      
        <a href="https://somaz.tistory.com" title="Tistory">
          <svg><use xlink:href="#icon-tistory"></use></svg>
        </a>
      
    </p>

    <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>


    <p>
      <a href="https://somaz.blog/sitemap.xml" title="sitemap">Sitemap</a> |
      <a href="https://somaz.blog/privacy-policy" title="Privacy Policy">Privacy Policy</a>
    </p>

    <p>
      <span>Somaz Tech Blog</span> <svg class="love"><use xlink:href="#icon-heart"></use></svg>
    </p>
</footer>










<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "somaz",
  "description": "DevOps engineer's tech blog.",
  "url": "https://somaz.blog/",
  "logo": {
      "@type": "ImageObject",
      "url": "https://somaz.blog/assets/img/icons/mediumtile.png",
      "width": "600",
      "height": "315"
  },
  "sameAs": [
    "https://github.com/somaz94","https://www.facebook.com/facebook_username","https://twitter.com/twitter_username","https://medium.com/@medium_username","https://www.instagram.com/instagram_username","https://www.linkedin.com/in/somaz","https://{{ site.tistory_username }}.tistory.com"
  ]
}
</script>

<!-- Include the script that allows Netlify CMS login -->
<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>

<!-- Include the website scripts -->
<script src="/assets/js/scripts.min.js"></script>

<!-- Include Google Analytics script -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  var host = window.location.hostname;
  if (host != 'localhost') {
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-XXXXXXXX-X');
  }
</script>
  


<!-- Include extra scripts -->



        

        
        
        
        
        
        
        
        
        <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "BlogPosting",
            "name": "Advanced Ceph Erasure Code Configuration and Performance Optimization",
            "headline": "Complete guide to implementing Erasure Code in Ceph clusters for storage efficiency and data protection",
            "description": "Comprehensive walkthrough of Ceph Erasure Code setup, configuration, and optimization including comparison with replica methods, practical implementation steps, and troubleshooting common issues.",
            "image": "https://res.cloudinary.com/dkcm26aem/image/upload/v1755133935/ceph-erasure-code_kheagu.png",
            "url": "https://somaz.blog/category/storage/ceph-erasure-code/",
            "articleBody": "



Overview

In large-scale storage systems, achieving both reliability and efficiency simultaneously presents significant challenges. Ceph addresses this through two distinct data protection methods: Replica and Erasure Code.

This article focuses primarily on Erasure Code configuration and practical implementation, highlighting its superior storage efficiency. We’ll explore the fundamental differences between Replica and Erasure Code approaches, provide step-by-step configuration guides, and address common troubleshooting scenarios.

Erasure Code offers substantial storage savings while maintaining high availability, making it ideal for environments where storage cost optimization is critical without compromising data durability.





What is Erasure Code?

Erasure Code is a data protection method that divides data into multiple chunks (k) and adds recovery chunks (m), storing the complete dataset across N = k + m blocks.

This approach enables data recovery even when some blocks are lost, providing resilience through mathematical redundancy rather than simple replication.



Key Characteristics:


  Data Segmentation: Original data split into k data chunks
  Parity Generation: m parity chunks calculated from data chunks
  Fault Tolerance: Can survive up to m chunk failures
  Storage Efficiency: Significantly better than traditional replication




Example Configuration:
# k=3, m=2 configuration
# - 3 data chunks + 2 parity chunks = 5 total chunks
# - Can survive up to 2 chunk failures
# - Storage efficiency: 3/5 = 60% (vs 33% for 3-way replication)






Replica vs Erasure Code Comparison


  
    
      Aspect
      Replica Method
      Erasure Code Method
    
    
      Data Protection
      Identical data copies
      Data segmentation + parity
    
    
      Storage Efficiency
      Very Low (33% for 3-replica)
      High (60% for k=3,m=2)
    
    
      Recovery Speed
      Fast (simple copy)
      Slower (parity calculation)
    
    
      CPU/Compute Load
      Low
      High (parity operations)
    
    
      Network Overhead
      High (full data copies)
      Lower (distributed chunks)
    
    
      Suitable Environment
      Fast recovery critical
      Storage space optimization
    
    
      Minimum OSDs
      Equal to replica count
      k + m
    
    
      Failure Tolerance
      n-1 failures
      m failures
    
  
  Comparison between Replica and Erasure Code strategies
  






Infrastructure Requirements



Minimum Cluster Configuration


  
    
      Component
      Requirement
      Recommendation
    
    
      OSDs
      k + m (minimum)
      k + m + 2 (for maintenance)
    
    
      Memory
      4GB per OSD
      8GB per OSD
    
    
      Network
      1Gbps
      10Gbps
    
    
      CPU
      1 core per OSD
      2 cores per OSD
    
    
      Failure Domains
      k + m distinct domains
      Well-distributed topology
    
  
  Minimum hardware and topology recommendations




Example Cluster Topology

# 3-node cluster example for k=3, m=2
Node 1: 2 OSDs (rack-1)
Node 2: 2 OSDs (rack-2)  
Node 3: 1 OSD (rack-3)
Total: 5 OSDs across 3 failure domains






Erasure Code Profile Configuration



Step 1: Verify Cluster Status

# Check Ceph version
ceph --version

# Verify cluster health
ceph -s

# Check OSD status
ceph osd tree

# Verify available failure domains
ceph osd crush tree




Step 2: Create Erasure Code Profile





Profile Parameters Explanation:

# Core Parameters
k=3                        # Number of data chunks
m=2                        # Number of parity chunks
plugin=jerasure           # Erasure coding algorithm

# Advanced Parameters
technique=reed_sol_van    # Specific technique within plugin
crush-failure-domain=host # Failure domain for chunk placement
crush-device-class=ssd    # Restrict to specific device class




Step 3: Verify Profile Creation

# List all erasure code profiles
ceph osd erasure-code-profile ls

# Display profile details
ceph osd erasure-code-profile get ec-profile-advanced

# Expected output:
# crush-device-class=ssd
# crush-failure-domain=host
# k=4
# m=2
# plugin=jerasure
# technique=reed_sol_van






Pool Creation and Management



Step 1: Calculate Placement Groups (PGs)

# Formula: (OSDs * 100) / (k + m) / pool_count
# Example: (12 OSDs * 100) / 6 / 2 pools = 100 PGs
# Round to nearest power of 2: 128 PGs

# For production environments, use Ceph PG calculator:
# https://ceph.io/pgcalc/




Step 2: Create Erasure Code Pool

# Create pool with calculated PGs
ceph osd pool create ec-pool-storage 128 erasure ec-profile-advanced

# Verify pool creation
ceph osd pool ls detail | grep ec-pool-storage

# Check pool status
ceph osd pool get ec-pool-storage all




Step 3: Configure Pool for Applications

# Enable for RGW (Object Storage)
ceph osd pool application enable ec-pool-storage rgw

# Enable for RBD (Block Storage) - requires metadata pool
ceph osd pool create ec-pool-metadata 32 replicated
ceph osd pool application enable ec-pool-metadata rbd
ceph osd pool application enable ec-pool-storage rbd

# Enable for CephFS (File Storage)
ceph osd pool application enable ec-pool-storage cephfs






Performance Optimization



Step 1: CRUSH Map Optimization

# Extract current CRUSH map
ceph osd getcrushmap -o crushmap.bin
crushtool -d crushmap.bin -o crushmap.txt

# Edit crushmap.txt to optimize placement rules
# Add custom rules for erasure code pools

# Compile and inject updated CRUSH map
crushtool -c crushmap.txt -o crushmap-new.bin
ceph osd setcrushmap -i crushmap-new.bin


Custom CRUSH Rule Example:

# Custom rule in crushmap.txt
rule ec_ssd_rule {
    id 1
    type erasure
    min_size 3
    max_size 6
    step take default class ssd
    step chooseleaf firstn 0 type host
    step emit
}




Step 2: Configure Pool Parameters

# Set optimal parameters for erasure code pool
ceph osd pool set ec-pool-storage pg_num 128
ceph osd pool set ec-pool-storage pgp_num 128

# Optimize for read performance
ceph osd pool set ec-pool-storage fast_read true

# Configure compression (optional)
ceph osd pool set ec-pool-storage compression_algorithm zstd
ceph osd pool set ec-pool-storage compression_mode force




Step 3: Monitor Performance Metrics

# Check pool statistics
ceph df detail

# Monitor I/O performance
ceph osd perf

# Check placement group distribution
ceph pg dump | grep ec-pool-storage

# Monitor erasure code operations
ceph daemon osd.0 perf dump | grep erasure






RBD Integration with Erasure Code



Step 1: Create Metadata and Data Pools

# Create replicated metadata pool
ceph osd pool create rbd-metadata 32 replicated

# Create erasure code data pool
ceph osd pool create rbd-ec-data 128 erasure ec-profile-advanced

# Enable RBD application
ceph osd pool application enable rbd-metadata rbd
ceph osd pool application enable rbd-ec-data rbd




Step 2: Configure RBD with Erasure Code Backend

# Create RBD image with erasure code data pool
rbd create --size 10G --data-pool rbd-ec-data rbd-metadata/test-image

# Verify image configuration
rbd info rbd-metadata/test-image

# Expected output:
# rbd image &apos;test-image&apos;:
#     size 10 GiB in 2560 objects
#     order 22 (4 MiB objects)
#     data_pool: rbd-ec-data




Step 3: Kubernetes StorageClass Configuration

# rbd-ec-storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rbd-erasure-code
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: &quot;false&quot;
provisioner: rbd.csi.ceph.com
parameters:
  clusterID: &quot;b9127830-b0cc-4e34-aa47-9d1a2e9949a8&quot;
  pool: &quot;rbd-metadata&quot;
  dataPool: &quot;rbd-ec-data&quot;
  imageFeatures: layering
  csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
  csi.storage.k8s.io/provisioner-secret-namespace: kube-system
  csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: kube-system
  csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
  csi.storage.k8s.io/node-stage-secret-namespace: kube-system
  csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
  - discard






Troubleshooting Common Issues



Issue 1: Degraded Data Redundancy

Symptoms:
HEALTH_WARN
Degraded data redundancy: 122 pgs undersized


Root Causes:

  Insufficient OSDs for k+m configuration
  OSDs not distributed across required failure domains
  OSD failures exceeding tolerance threshold


Resolution Steps:
# Check OSD distribution
ceph osd tree

# Verify failure domain configuration
ceph osd crush tree

# Add more OSDs if needed
ceph-volume lvm create --data /dev/sdX

# Check PG mapping
ceph pg ls-by-pool ec-pool-storage | head -10

# Force PG repair if necessary
ceph pg repair &amp;lt;pg_id&amp;gt;




Issue 2: Pool Application Not Enabled

Symptoms:
HEALTH_WARN
1 pool(s) do not have an application enabled


Resolution:
# Enable appropriate application
ceph osd pool application enable ec-pool-storage rgw

# Verify application status
ceph osd pool application get ec-pool-storage




Issue 3: Slow Recovery Performance

Symptoms:

  Extended recovery times
  High CPU utilization during recovery
  Network bandwidth saturation


Optimization Steps:
# Adjust recovery parameters
ceph config set osd osd_recovery_max_active 3
ceph config set osd osd_max_backfills 1
ceph config set osd osd_recovery_sleep 0.1

# Monitor recovery progress
ceph -s | grep recovery

# Check individual OSD performance
ceph daemon osd.0 perf dump | grep recovery




Issue 4: CRUSH Rule Conflicts

Symptoms:
HEALTH_ERR
Pool &apos;ec-pool-storage&apos; has no available OSDs for pg X.Y


Resolution:
# Check current CRUSH rules
ceph osd crush rule ls
ceph osd crush rule dump

# Verify pool&apos;s CRUSH rule
ceph osd pool get ec-pool-storage crush_rule

# Create appropriate CRUSH rule
ceph osd crush rule create-erasure ec-rule ec-profile-advanced

# Apply rule to pool
ceph osd pool set ec-pool-storage crush_rule ec-rule






Monitoring and Maintenance



Step 1: Setup Monitoring Scripts





Step 2: Automated Health Checks

# Create cron job for regular monitoring
cat &amp;gt; /etc/cron.d/ceph-monitoring &amp;lt;&amp;lt; &apos;EOF&apos;
# Monitor Ceph erasure code pools every 15 minutes
*/15 * * * * root /usr/local/bin/ceph-ec-monitor.sh &amp;gt;&amp;gt; /var/log/ceph-monitoring.log 2&amp;gt;&amp;amp;1
EOF




Step 3: Performance Baselines

# Establish performance baselines
echo &quot;=== Storage Efficiency Comparison ===&quot; &amp;gt; /var/log/ceph-baseline.log

# Calculate storage efficiency
TOTAL_RAW=$(ceph df | grep TOTAL | awk &apos;{print $2}&apos;)
TOTAL_USED=$(ceph df | grep TOTAL | awk &apos;{print $4}&apos;)
EFFICIENCY=$(echo &quot;scale=2; $TOTAL_USED / $TOTAL_RAW * 100&quot; | bc)

echo &quot;Raw Storage: $TOTAL_RAW&quot; &amp;gt;&amp;gt; /var/log/ceph-baseline.log
echo &quot;Used Storage: $TOTAL_USED&quot; &amp;gt;&amp;gt; /var/log/ceph-baseline.log
echo &quot;Efficiency: $EFFICIENCY%&quot; &amp;gt;&amp;gt; /var/log/ceph-baseline.log

# I/O performance baseline
rados bench -p ec-pool-storage 30 write &amp;gt; /var/log/ceph-write-baseline.log
rados bench -p ec-pool-storage 30 seq &amp;gt; /var/log/ceph-read-baseline.log






Best Practices and Recommendations



Production Deployment Guidelines


  Capacity Planning
    
      Plan for k+m+2 OSDs minimum for maintenance windows
      Consider network bandwidth for erasure code operations
      Account for increased CPU requirements
    
  
  Failure Domain Design
    
      Distribute OSDs across physical racks or availability zones
      Ensure each failure domain can handle chunk placement
      Plan for correlated failures (power, network)
    
  
  Performance Optimization
    
      Use fast SSDs for metadata pools
      Optimize CRUSH rules for data locality
      Monitor and tune recovery parameters
    
  




When to Choose Erasure Code


  
    
      Scenario
      Recommendation
      Rationale
    
    
      Cold Storage
      Erasure Code
      Storage efficiency priority
    
    
      Backup Systems
      Erasure Code
      Cost optimization critical
    
    
      Archive Data
      Erasure Code
      Infrequent access patterns
    
    
      Hot Database
      Replica
      Performance critical
    
    
      Real-time Analytics
      Replica
      Low latency required
    
    
      Mixed Workloads
      Hybrid Approach
      Balance efficiency and performance
    
  
  Guidelines for selecting Replica vs Erasure Code




Configuration Templates







Conclusion


  Erasure Code represents a significant advancement in storage efficiency for Ceph clusters, offering substantial cost savings while maintaining high durability.

  The configuration complexity is offset by dramatic improvements in storage utilization, making it essential for large-scale deployments.




Key Achievements:


  Storage Efficiency: Up to 60-80% storage utilization vs 33% with 3-way replication
  Scalability: Better resource utilization for large datasets
  Flexibility: Multiple profiles for different use cases
  Integration: Seamless Kubernetes and application integration




Operational Benefits:


  Cost Reduction: Significant hardware cost savings
  Data Durability: Configurable failure tolerance
  Performance Optimization: Tunable parameters for different workloads
  Automated Management: Integration with existing Ceph tools




Future Considerations:

  Evaluate emerging erasure code algorithms
  Implement automated tier migration between replica and erasure code pools
  Develop application-specific optimization profiles
  Consider hybrid storage architectures


Mastering Erasure Code configuration enables organizations to build cost-effective, highly durable storage systems that scale efficiently with growing data requirements.


  “Erasure Code transforms storage economics by delivering enterprise-grade durability at a fraction of traditional replication costs.”






References


  Ceph Erasure Code Documentation
  CRUSH Map Documentation
  Ceph Performance Tuning Guide
  Erasure Code Theory and Implementation
  Ceph RBD Erasure Code Pools
  Kubernetes CSI Integration

",
            "wordcount": "2380",
            "inLanguage": "en",
            "dateCreated": "2025-11-08/",
            "datePublished": "2025-11-08/",
            "dateModified": "2025-11-08/",
            "author": {
                "@type": "Person",
                "name": "Somaz",
                
                "image": "/assets/img/uploads/profile.png",
                
                "jobTitle": "DevOps Engineer",
                "url": "https://somaz.blog/authors/somaz/",
                "sameAs": [
                    "https://github.com/somaz94","https://www.linkedin.com/in/somaz"
                ]
            },
            "publisher": {
                "@type": "Organization",
                "name": "somaz",
                "url": "https://somaz.blog/",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://somaz.blog/assets/img/blog-image.png",
                    "width": "600",
                    "height": "315"
                }
            },
            "mainEntityOfPage": "True",
            "genre": "STORAGE",
            "articleSection": "STORAGE",
            "keywords": ["ceph","erasure-code","storage-efficiency","data-protection","performance-optimization","distributed-storage","infrastructure","automation"]
        }
        </script>
    </body>
</html>
