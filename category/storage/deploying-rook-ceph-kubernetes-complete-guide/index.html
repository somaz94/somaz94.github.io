<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    

    
    

    
    

    
    

    <!-- ✅ Google Tag Manager 추가 -->
    <script>
        (function(w,d,s,l,i){
            w[l]=w[l]||[];
            w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});
            var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';
            j.async=true;
            j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;
            f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer','GTM-MBP83N4Q');
    </script>
      <!-- ✅ End Google Tag Manager -->

    <!-- Mermaid.js 직접 로드 -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>

    <title>Deploying and Operating Rook-Ceph on Kubernetes: Complete Implementation Guide | somaz</title>
    <meta name="description" content="Step-by-step deployment of Rook-Ceph on Kubernetes including operator installation, cluster configuration, storage class setup, and dashboard management for ...">
    
        <meta name="keywords" content="rook-ceph, kubernetes, ceph, helm, storage-operator, distributed-storage, cloud-native, persistent-volumes, dashboard, kubespray">
    

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Deploying and Operating Rook-Ceph on Kubernetes: Complete Implementation Guide | somaz">
    <meta name="twitter:description" content="Step-by-step deployment of Rook-Ceph on Kubernetes including operator installation, cluster configuration, storage class setup, and dashboard management for ...">

    
        <meta property="twitter:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1755078066/rook-ceph-1_ompm5x.png">
    
    
    
        <meta name="twitter:site" content="@twitter_username">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="https://somaz.blog/category/storage/deploying-rook-ceph-kubernetes-complete-guide/">
    <meta property="og:title" content="Deploying and Operating Rook-Ceph on Kubernetes: Complete Implementation Guide | somaz">
    <meta property="og:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1755078066/rook-ceph-1_ompm5x.png">
    <meta property="og:description" content="Step-by-step deployment of Rook-Ceph on Kubernetes including operator installation, cluster configuration, storage class setup, and dashboard management for ...">
    <meta property="og:site_name" content="Somaz Tech Blog">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="somaz">
    <meta name="msapplication-TileColor" content="#141414">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#141414">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:300,400,700" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="canonical" href="https://somaz.blog/category/storage/deploying-rook-ceph-kubernetes-complete-guide/">
    <link rel="alternate" type="application/rss+xml" title="Somaz Tech Blog" href="https://somaz.blog/feed.xml" />

    <!-- Include extra styles -->
    

    <!-- JavaScript enabled/disabled -->
    <script>
        document.querySelector('html').classList.remove('no-js');
    </script>

    <!-- Google Adsense -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8725590811736154"
        crossorigin="anonymous"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet"> -->
    <!-- <link href="https://cdn.jsdelivr.net/gh/sunn-us/SUIT/fonts/variable/woff2/SUIT-Variable.css" rel="stylesheet"> -->
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css2?family=Albert+Sans:wght@400;500;700&display=swap" rel="stylesheet"> -->

    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml" />

</head>
<!-- ✅ Google Tag Manager (noscript) -->
<noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MBP83N4Q"
            height="0" width="0" style="display:none;visibility:hidden">
    </iframe>
</noscript>
<!-- ✅ End Google Tag Manager (noscript) -->
    <body class="has-push-menu">
        





        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 1000 1000"><path d="M969.8,870.3c27,27.7,27,71.8,0,99.1C955.7,983,937.9,990,920,990c-17.9,0-35.7-7-49.7-20.7L500,599L129.6,969.4C115.6,983,97.8,990,79.9,990s-35.7-7-49.7-20.7c-27-27.3-27-71.4,0-99.1L400.9,500L30.3,129.3c-27-27.3-27-71.4,0-99.1c27.3-27,71.8-27,99.4,0L500,400.9L870.4,30.2c27.7-27,71.8-27,99.4,0c27,27.7,27,71.8,0,99.1L599.1,500L969.8,870.3z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-clock" viewBox="0 0 1000 1000"><path d="M500,10C229.8,10,10,229.8,10,500c0,270.2,219.8,490,490,490c270.2,0,490-219.8,490-490C990,229.8,770.2,10,500,10z M500,910.2c-226.2,0-410.2-184-410.2-410.2c0-226.2,184-410.2,410.2-410.2c226.2,0,410.2,184,410.2,410.2C910.2,726.1,726.2,910.2,500,910.2z M753.1,374c8.2,11.9,5.2,28.1-6.6,36.3L509.9,573.7c-4.4,3.1-9.6,4.6-14.8,4.6c-4.1,0-8.3-1-12.1-3c-8.6-4.5-14-13.4-14-23.1V202.5c0-14.4,11.7-26.1,26.1-26.1c14.4,0,26.1,11.7,26.1,26.1v300l195.6-135.1C728.7,359.2,744.9,362.1,753.1,374z"/></symbol><symbol id="icon-calendar" viewBox="0 0 1000 1000"><path d="M920,500v420H80V500H920 M990,430H10v490c0,38.7,31.3,70,70,70h840c38.7,0,70-31.3,70-70V430L990,430z"/><path d="M850,80v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80H360v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80C72.8,80,10,142.7,10,220v140h980V220C990,142.7,927.2,80,850,80z"/><path d="M255,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C290,25.8,274.3,10,255,10z"/><path d="M745,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C780,25.8,764.3,10,745,10z"/></symbol><symbol id="icon-github" viewBox="0 0 12 14"><path d="M6 1q1.633 0 3.012 0.805t2.184 2.184 0.805 3.012q0 1.961-1.145 3.527t-2.957 2.168q-0.211 0.039-0.312-0.055t-0.102-0.234q0-0.023 0.004-0.598t0.004-1.051q0-0.758-0.406-1.109 0.445-0.047 0.801-0.141t0.734-0.305 0.633-0.52 0.414-0.82 0.16-1.176q0-0.93-0.617-1.609 0.289-0.711-0.062-1.594-0.219-0.070-0.633 0.086t-0.719 0.344l-0.297 0.187q-0.727-0.203-1.5-0.203t-1.5 0.203q-0.125-0.086-0.332-0.211t-0.652-0.301-0.664-0.105q-0.352 0.883-0.062 1.594-0.617 0.68-0.617 1.609 0 0.664 0.16 1.172t0.41 0.82 0.629 0.523 0.734 0.305 0.801 0.141q-0.305 0.281-0.383 0.805-0.164 0.078-0.352 0.117t-0.445 0.039-0.512-0.168-0.434-0.488q-0.148-0.25-0.379-0.406t-0.387-0.187l-0.156-0.023q-0.164 0-0.227 0.035t-0.039 0.090 0.070 0.109 0.102 0.094l0.055 0.039q0.172 0.078 0.34 0.297t0.246 0.398l0.078 0.18q0.102 0.297 0.344 0.48t0.523 0.234 0.543 0.055 0.434-0.027l0.18-0.031q0 0.297 0.004 0.691t0.004 0.426q0 0.141-0.102 0.234t-0.312 0.055q-1.812-0.602-2.957-2.168t-1.145-3.527q0-1.633 0.805-3.012t2.184-2.184 3.012-0.805zM2.273 9.617q0.023-0.055-0.055-0.094-0.078-0.023-0.102 0.016-0.023 0.055 0.055 0.094 0.070 0.047 0.102-0.016zM2.516 9.883q0.055-0.039-0.016-0.125-0.078-0.070-0.125-0.023-0.055 0.039 0.016 0.125 0.078 0.078 0.125 0.023zM2.75 10.234q0.070-0.055 0-0.148-0.062-0.102-0.133-0.047-0.070 0.039 0 0.141t0.133 0.055zM3.078 10.562q0.062-0.062-0.031-0.148-0.094-0.094-0.156-0.023-0.070 0.062 0.031 0.148 0.094 0.094 0.156 0.023zM3.523 10.758q0.023-0.086-0.102-0.125-0.117-0.031-0.148 0.055t0.102 0.117q0.117 0.047 0.148-0.047zM4.016 10.797q0-0.102-0.133-0.086-0.125 0-0.125 0.086 0 0.102 0.133 0.086 0.125 0 0.125-0.086zM4.469 10.719q-0.016-0.086-0.141-0.070-0.125 0.023-0.109 0.117t0.141 0.062 0.109-0.109z"></path></symbol><symbol id="icon-medium" viewBox="0 0 1000 1000"><path d="M336.5,240.2v641.5c0,9.1-2.3,16.9-6.8,23.2s-11.2,9.6-20,9.6c-6.2,0-12.2-1.5-18-4.4L37.3,782.7c-7.7-3.6-14.1-9.8-19.4-18.3S10,747.4,10,739V115.5c0-7.3,1.8-13.5,5.5-18.6c3.6-5.1,8.9-7.7,15.9-7.7c5.1,0,13.1,2.7,24.1,8.2l279.5,140C335.9,238.6,336.5,239.5,336.5,240.2L336.5,240.2z M371.5,295.5l292,473.6l-292-145.5V295.5z M990,305.3v576.4c0,9.1-2.6,16.5-7.7,22.1c-5.1,5.7-12,8.5-20.8,8.5s-17.3-2.4-25.7-7.1L694.7,784.9L990,305.3z M988.4,239.7c0,1.1-46.8,77.6-140.3,229.4C754.6,621,699.8,709.8,683.8,735.7L470.5,389l177.2-288.2c6.2-10.2,15.7-15.3,28.4-15.3c5.1,0,9.8,1.1,14.2,3.3l295.9,147.7C987.6,237.1,988.4,238.2,988.4,239.7L988.4,239.7z"/></symbol><symbol id="icon-instagram" viewBox="0 0 489.84 489.84"><path d="M249.62,50.46c65.4,0,73.14.25,99,1.43C372.47,53,385.44,57,394.07,60.32a75.88,75.88,0,0,1,28.16,18.32,75.88,75.88,0,0,1,18.32,28.16c3.35,8.63,7.34,21.6,8.43,45.48,1.18,25.83,1.43,33.57,1.43,99s-0.25,73.14-1.43,99c-1.09,23.88-5.08,36.85-8.43,45.48a81.11,81.11,0,0,1-46.48,46.48c-8.63,3.35-21.6,7.34-45.48,8.43-25.82,1.18-33.57,1.43-99,1.43s-73.15-.25-99-1.43c-23.88-1.09-36.85-5.08-45.48-8.43A75.88,75.88,0,0,1,77,423.86,75.88,75.88,0,0,1,58.69,395.7c-3.35-8.63-7.34-21.6-8.43-45.48-1.18-25.83-1.43-33.57-1.43-99s0.25-73.14,1.43-99c1.09-23.88,5.08-36.85,8.43-45.48A75.88,75.88,0,0,1,77,78.64a75.88,75.88,0,0,1,28.16-18.32c8.63-3.35,21.6-7.34,45.48-8.43,25.83-1.18,33.57-1.43,99-1.43m0-44.13c-66.52,0-74.86.28-101,1.47s-43.87,5.33-59.45,11.38A120.06,120.06,0,0,0,45.81,47.44,120.06,120.06,0,0,0,17.56,90.82C11.5,106.4,7.36,124.2,6.17,150.27s-1.47,34.46-1.47,101,0.28,74.86,1.47,101,5.33,43.87,11.38,59.45a120.06,120.06,0,0,0,28.25,43.38,120.06,120.06,0,0,0,43.38,28.25c15.58,6.05,33.38,10.19,59.45,11.38s34.46,1.47,101,1.47,74.86-.28,101-1.47,43.87-5.33,59.45-11.38a125.24,125.24,0,0,0,71.63-71.63c6.05-15.58,10.19-33.38,11.38-59.45s1.47-34.46,1.47-101-0.28-74.86-1.47-101-5.33-43.87-11.38-59.45a120.06,120.06,0,0,0-28.25-43.38,120.06,120.06,0,0,0-43.38-28.25C394.47,13.13,376.67,9,350.6,7.8s-34.46-1.47-101-1.47h0Z" transform="translate(-4.7 -6.33)" /><path d="M249.62,125.48A125.77,125.77,0,1,0,375.39,251.25,125.77,125.77,0,0,0,249.62,125.48Zm0,207.41a81.64,81.64,0,1,1,81.64-81.64A81.64,81.64,0,0,1,249.62,332.89Z" transform="translate(-4.7 -6.33)"/><circle cx="375.66" cy="114.18" r="29.39" /></symbol><symbol id="icon-linkedin" viewBox="0 0 12 14"><path d="M2.727 4.883v7.742h-2.578v-7.742h2.578zM2.891 2.492q0.008 0.57-0.395 0.953t-1.059 0.383h-0.016q-0.641 0-1.031-0.383t-0.391-0.953q0-0.578 0.402-0.957t1.051-0.379 1.039 0.379 0.398 0.957zM12 8.187v4.437h-2.57v-4.141q0-0.82-0.316-1.285t-0.988-0.465q-0.492 0-0.824 0.27t-0.496 0.668q-0.086 0.234-0.086 0.633v4.32h-2.57q0.016-3.117 0.016-5.055t-0.008-2.313l-0.008-0.375h2.57v1.125h-0.016q0.156-0.25 0.32-0.438t0.441-0.406 0.68-0.34 0.895-0.121q1.336 0 2.148 0.887t0.813 2.598z"></path></symbol><symbol id="icon-heart" viewBox="0 0 34 30"><path d="M17,29.7 L16.4,29.2 C3.5,18.7 0,15 0,9 C0,4 4,0 9,0 C13.1,0 15.4,2.3 17,4.1 C18.6,2.3 20.9,0 25,0 C30,0 34,4 34,9 C34,15 30.5,18.7 17.6,29.2 L17,29.7 Z M9,2 C5.1,2 2,5.1 2,9 C2,14.1 5.2,17.5 17,27.1 C28.8,17.5 32,14.1 32,9 C32,5.1 28.9,2 25,2 C21.5,2 19.6,4.1 18.1,5.8 L17,7.1 L15.9,5.8 C14.4,4.1 12.5,2 9,2 Z" id="Shape"></path></symbol><symbol id="icon-arrow-right" viewBox="0 0 25.452 25.452"><path d="M4.471,24.929v-2.004l12.409-9.788c0.122-0.101,0.195-0.251,0.195-0.411c0-0.156-0.073-0.31-0.195-0.409L4.471,2.526V0.522c0-0.2,0.115-0.384,0.293-0.469c0.18-0.087,0.396-0.066,0.552,0.061l15.47,12.202c0.123,0.1,0.195,0.253,0.195,0.409c0,0.16-0.072,0.311-0.195,0.411L5.316,25.34c-0.155,0.125-0.372,0.147-0.552,0.061C4.586,25.315,4.471,25.13,4.471,24.929z"/></symbol><symbol id="icon-star" viewBox="0 0 48 48"><path fill="currentColor" d="M44,24c0,11.045-8.955,20-20,20S4,35.045,4,24S12.955,4,24,4S44,12.955,44,24z"/><path fill="#ffffff" d="M24,11l3.898,7.898l8.703,1.301l-6.301,6.102l1.5,8.699L24,30.898L16.199,35l1.5-8.699l-6.301-6.102  l8.703-1.301L24,11z"/></symbol><symbol id="icon-read" viewBox="0 0 32 32"><path fill="currentColor" d="M29,4H3C1.343,4,0,5.343,0,7v18c0,1.657,1.343,3,3,3h10c0,0.552,0.448,1,1,1h4c0.552,0,1-0.448,1-1h10  c1.657,0,3-1.343,3-3V7C32,5.343,30.657,4,29,4z M29,5v20H18.708c-0.618,0-1.236,0.146-1.789,0.422l-0.419,0.21V5H29z M15.5,5  v20.632l-0.419-0.21C14.528,25.146,13.91,25,13.292,25H3V5H15.5z M31,25c0,1.103-0.897,2-2,2H18v1h-4v-1H3c-1.103,0-2-0.897-2-2V7  c0-0.737,0.405-1.375,1-1.722V25c0,0.552,0.448,1,1,1h10.292c0.466,0,0.925,0.108,1.342,0.317l0.919,0.46  c0.141,0.07,0.294,0.106,0.447,0.106c0.153,0,0.306-0.035,0.447-0.106l0.919-0.46C17.783,26.108,18.242,26,18.708,26H29  c0.552,0,1-0.448,1-1V5.278C30.595,5.625,31,6.263,31,7V25z M6,12.5C6,12.224,6.224,12,6.5,12h5c0.276,0,0.5,0.224,0.5,0.5  S11.776,13,11.5,13h-5C6.224,13,6,12.776,6,12.5z M6,14.5C6,14.224,6.224,14,6.5,14h5c0.276,0,0.5,0.224,0.5,0.5S11.776,15,11.5,15  h-5C6.224,15,6,14.776,6,14.5z M6,16.5C6,16.224,6.224,16,6.5,16h5c0.276,0,0.5,0.224,0.5,0.5S11.776,17,11.5,17h-5  C6.224,17,6,16.776,6,16.5z M20,12.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,13,25.5,13h-5  C20.224,13,20,12.776,20,12.5z M20,14.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,15,25.5,15h-5  C20.224,15,20,14.776,20,14.5z M20,16.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,17,25.5,17h-5  C20.224,17,20,16.776,20,16.5z"></path></symbol><symbol id="icon-tistory" viewBox="0 0 24 24"><path d="M4 4h16v3h-6v13h-4V7H4V4z"/></symbol></defs></svg>

        <header class="bar-header">
    <a id="menu" role="button">
        <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    </a>
    <h1 class="logo">
        <a href="/">
            
                somaz <span class="version">v3.1.2</span>
            
        </a>
    </h1>
    <a id="search" class="dosearch" role="button">
        <svg class="icon-search"><use xlink:href="#icon-search"></use></svg>
    </a>
    
        <a href="https://github.com/thiagorossener/jekflix-template" class="get-theme" role="button">
            Get this theme!
        </a>
    
</header>

<div id="mask" class="overlay"></div>

<aside class="sidebar" id="sidebar">
    <nav id="navigation">
      <h2>Menu</h2>
      <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>

    </nav>
</aside>

<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>



        <section class="post two-columns">
            <article role="article" class="post-content">
                <p class="post-info">
                    
                        <svg class="icon-calendar" id="date"><use xlink:href="#icon-calendar"></use></svg>
                        <time class="date" datetime="2025-11-26T00:00:00+00:00">
                            


November 26, 2025

                        </time>
                    
                    <svg id="clock" class="icon-clock"><use xlink:href="#icon-clock"></use></svg>
                    <span>15 min to read</span>
                </p>
                <h1 class="post-title">Deploying and Operating Rook-Ceph on Kubernetes: Complete Implementation Guide</h1>
                <p class="post-subtitle">A comprehensive guide to deploying Ceph storage on Kubernetes using Rook operator with Helm charts and operational best practices</p>

                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755078066/rook-ceph-1_ompm5x.png" alt="Featured image" class="post-cover">
                

                <!-- Pagination links -->



                <!-- Add your table of contents here -->


                <p><br /></p>

<hr />

<h2 id="overview">Overview</h2>

<p>In this article, we’ll cover the complete process of deploying and operating Ceph storage in a Kubernetes cluster using Rook-Ceph.</p>

<p>Rook is an open-source orchestration tool that manages Ceph in a Kubernetes-native way, abstracting complex distributed storage systems into Kubernetes resources for easy installation and operation. It helps simplify the deployment and management of sophisticated storage infrastructure.</p>

<p>In this implementation, we built a Kubernetes cluster using Kubespray on Google Cloud VM environment, then installed the Rook Operator and Rook-Ceph cluster using Helm Charts. We configured dedicated storage nodes for Ceph OSDs and properly customized Rook’s values.yaml to set up MON/MGR/OSD/MDS/Object Store components.</p>

<p>Subsequently, we configured RBD-based StorageClass to enable Pods to use Ceph block storage, and finally activated the Ceph Dashboard for web UI-based cluster monitoring capabilities.</p>

<p><br /></p>

<hr />

<h2 id="what-is-cephadm">What is Cephadm?</h2>

<p>Cephadm is Ceph’s latest deployment and management tool, introduced starting with the Ceph Octopus release.</p>

<p>It’s designed to simplify deploying, configuring, managing, and scaling Ceph clusters. It can bootstrap a cluster with a single command and deploys Ceph services using container technology.</p>

<p>Cephadm doesn’t rely on external configuration tools like Ansible, Rook, or Salt. However, these external configuration tools can be used to automate tasks not performed by cephadm itself.</p>

<h4 id="related-resources">Related Resources:</h4>
<ul>
  <li><a href="https://github.com/ceph/cephadm-ansible">cephadm-ansible</a></li>
  <li><a href="https://rook.io/docs/rook/v1.10/Getting-Started/intro/">Rook Introduction</a></li>
  <li><a href="https://github.com/ceph/ceph-salt">ceph-salt</a></li>
</ul>

<p><br /></p>

<hr />

<h2 id="what-is-rook-ceph">What is Rook-Ceph?</h2>

<p>Ceph is a scalable distributed storage solution for block storage, object storage, and shared file systems, proven through years of production deployments.</p>

<p>Rook is an open-source orchestration framework for deploying, operating, and managing distributed storage systems on Kubernetes. It focuses on automatically managing Ceph within Kubernetes clusters and integrating storage solutions into cloud-native environments.</p>

<p>Using Rook, you can easily deploy and manage complex distributed storage systems like Ceph while leveraging Kubernetes’ automation and management capabilities.</p>

<p><br /></p>

<hr />

<h2 id="kubernetes-installation">Kubernetes Installation</h2>

<p>Refer to the linked article for Kubernetes installation.</p>

<p><strong>Important note</strong>: Storage nodes must have at least 32GB of memory.</p>

<p><br /></p>

<h3 id="infrastructure-requirements">Infrastructure Requirements</h3>

<h4 id="master-node-control-plane">Master Node (Control Plane)</h4>

<div class="table-container">
  <table class="table-modern">
    <tr>
      <th>Component</th>
      <th>IP</th>
      <th>CPU</th>
      <th>Memory</th>
    </tr>
    <tr>
      <td>test-server</td>
      <td>10.77.101.18</td>
      <td>16</td>
      <td>32G</td>
    </tr>
  </table>
  <div class="caption">Master (control plane) node specification</div>
</div>

<h4 id="worker-nodes">Worker Nodes</h4>

<div class="table-container">
  <table class="table-modern">
    <tr>
      <th>Component</th>
      <th>IP</th>
      <th>CPU</th>
      <th>Memory</th>
    </tr>
    <tr>
      <td>test-server-agent</td>
      <td>10.77.101.12</td>
      <td>16</td>
      <td>32G</td>
    </tr>
    <tr>
      <td>test-server-storage</td>
      <td>10.77.101.16</td>
      <td>16</td>
      <td>32G</td>
    </tr>
  </table>
  <div class="caption">Worker node specifications</div>
</div>

<p><br /></p>

<h3 id="additional-storage-node-configuration">Additional Storage Node Configuration</h3>

<p>Add the following configuration for the storage node:</p>

<script src="https://gist.github.com/somaz94/9b6fa6550b304743aa48aa497accfe4c.js"></script>

<p><br /></p>

<h3 id="verify-installation">Verify Installation</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get nodes
NAME                  STATUS   ROLES           AGE   VERSION
test-server           Ready    control-plane   55s   v1.29.1
test-server-agent     Ready    &lt;none&gt;          55s   v1.29.1
test-server-storage   Ready    &lt;none&gt;          55s   v1.29.1
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="rook-ceph-operator-installation">Rook-Ceph Operator Installation</h2>

<p><br /></p>

<h3 id="clone-repository-and-prepare-values">Clone Repository and Prepare Values</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/rook/rook.git
<span class="nb">cd</span> ~/rook/deploy/charts/rook-ceph
<span class="nb">cp </span>values.yaml somaz-values.yaml
</code></pre></div></div>

<p><br /></p>

<h3 id="install-rook-operator-using-helm">Install Rook Operator using Helm</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add Rook Helm repository</span>
helm repo add rook-release https://charts.rook.io/release
helm repo update

<span class="c"># Install using local git repository</span>
helm <span class="nb">install</span> <span class="nt">--create-namespace</span> <span class="nt">--namespace</span> rook-ceph rook-ceph <span class="nb">.</span> <span class="nt">-f</span> somaz-values.yaml

<span class="c"># Upgrade (if needed)</span>
helm upgrade <span class="nt">--create-namespace</span> <span class="nt">--namespace</span> rook-ceph rook-ceph <span class="nb">.</span> <span class="nt">-f</span> somaz-values.yaml
</code></pre></div></div>

<p><br /></p>

<h3 id="verify-operator-installation">Verify Operator Installation</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check Custom Resource Definitions</span>
kubectl get crd | <span class="nb">grep </span>ceph

<span class="c"># Verify operator pod</span>
kubectl get pods <span class="nt">-n</span> rook-ceph
NAME                                 READY   STATUS    RESTARTS   AGE
rook-ceph-operator-799cf9f45-946r6   1/1     Running   0          28s
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="rook-ceph-cluster-installation">Rook-Ceph Cluster Installation</h2>

<p><br /></p>

<h3 id="prepare-storage-disks">Prepare Storage Disks</h3>

<p>First, verify and initialize disks on the test-server-storage node:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check available disks</span>
lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
loop0     7:0    0  63.9M  1 loop /snap/core20/2105
loop1     7:1    0 368.2M  1 loop /snap/google-cloud-cli/207
loop2     7:2    0  40.4M  1 loop /snap/snapd/20671
loop3     7:3    0  91.9M  1 loop /snap/lxd/24061
sda       8:0    0    50G  0 disk
├─sda1    8:1    0  49.9G  0 part /
├─sda14   8:14   0     4M  0 part
└─sda15   8:15   0   106M  0 part /boot/efi
sdb       8:16   0    50G  0 disk
sdc       8:32   0    50G  0 disk
sdd       8:48   0    50G  0 disk

<span class="c"># Initialize disks</span>
<span class="nb">sudo </span>wipefs <span class="nt">-a</span> /dev/sdb
<span class="nb">sudo </span>wipefs <span class="nt">-a</span> /dev/sdc
<span class="nb">sudo </span>wipefs <span class="nt">-a</span> /dev/sdd
</code></pre></div></div>

<p><br /></p>

<h3 id="configure-cluster-values">Configure Cluster Values</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> ~/rook/deploy/charts/rook-ceph-cluster
<span class="nb">cp </span>values.yaml somaz-values.yaml
</code></pre></div></div>

<p><br /></p>

<p>Edit the <code class="language-plaintext highlighter-rouge">somaz-values.yaml</code> file with the following key configurations:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">configOverride</span><span class="pi">:</span> <span class="pi">|</span>
  <span class="s">[global]</span>
  <span class="s">mon_allow_pool_delete = true</span>
  <span class="s">osd_pool_default_size = 1</span>
  <span class="s">osd_pool_default_min_size = 1</span>

<span class="c1"># Enable debugging toolbox</span>
<span class="na">toolbox</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>

<span class="na">cephClusterSpec</span><span class="pi">:</span>
  <span class="na">mon</span><span class="pi">:</span>
    <span class="na">count</span><span class="pi">:</span> <span class="m">1</span>
    <span class="na">allowMultiplePerNode</span><span class="pi">:</span> <span class="no">false</span>

  <span class="na">mgr</span><span class="pi">:</span>
    <span class="na">count</span><span class="pi">:</span> <span class="m">1</span>
    <span class="na">allowMultiplePerNode</span><span class="pi">:</span> <span class="no">false</span>
    <span class="na">modules</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">pg_autoscaler</span>
        <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>

  <span class="na">storage</span><span class="pi">:</span>
    <span class="na">useAllNodes</span><span class="pi">:</span> <span class="no">false</span>
    <span class="na">useAllDevices</span><span class="pi">:</span> <span class="no">false</span>
    <span class="na">nodes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">test-server-storage"</span>
        <span class="na">devices</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">sdb"</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">sdc"</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s2">"</span><span class="s">sdd"</span>

<span class="na">cephBlockPools</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">ceph-blockpool</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">failureDomain</span><span class="pi">:</span> <span class="s">host</span>
      <span class="na">replicated</span><span class="pi">:</span>
        <span class="na">size</span><span class="pi">:</span> <span class="m">1</span>

<span class="na">cephFileSystems</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">ceph-filesystem</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">metadataPool</span><span class="pi">:</span>
        <span class="na">replicated</span><span class="pi">:</span>
          <span class="na">size</span><span class="pi">:</span> <span class="m">1</span>
      <span class="na">dataPools</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">failureDomain</span><span class="pi">:</span> <span class="s">host</span>
          <span class="na">replicated</span><span class="pi">:</span>
            <span class="na">size</span><span class="pi">:</span> <span class="m">1</span>
    <span class="na">storageClass</span><span class="pi">:</span>
      <span class="na">enabled</span><span class="pi">:</span> <span class="no">false</span>
      <span class="na">isDefault</span><span class="pi">:</span> <span class="no">false</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">ceph-filesystem</span>

<span class="na">cephObjectStores</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">ceph-objectstore</span>
    <span class="na">spec</span><span class="pi">:</span>
      <span class="na">metadataPool</span><span class="pi">:</span>
        <span class="na">failureDomain</span><span class="pi">:</span> <span class="s">host</span>
        <span class="na">replicated</span><span class="pi">:</span>
          <span class="na">size</span><span class="pi">:</span> <span class="m">1</span>
      <span class="na">dataPool</span><span class="pi">:</span>
        <span class="na">failureDomain</span><span class="pi">:</span> <span class="s">host</span>
        <span class="na">erasureCoded</span><span class="pi">:</span>
          <span class="na">dataChunks</span><span class="pi">:</span> <span class="m">2</span>
          <span class="na">codingChunks</span><span class="pi">:</span> <span class="m">1</span>
      <span class="na">preservePoolsOnDelete</span><span class="pi">:</span> <span class="no">true</span>
      <span class="na">gateway</span><span class="pi">:</span>
        <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
        <span class="na">instances</span><span class="pi">:</span> <span class="m">1</span>
        <span class="na">priorityClassName</span><span class="pi">:</span> <span class="s">system-cluster-critical</span>
    <span class="na">storageClass</span><span class="pi">:</span>
      <span class="na">enabled</span><span class="pi">:</span> <span class="no">false</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">ceph-bucket</span>
      <span class="na">reclaimPolicy</span><span class="pi">:</span> <span class="s">Delete</span>
      <span class="na">volumeBindingMode</span><span class="pi">:</span> <span class="s2">"</span><span class="s">Immediate"</span>
      <span class="na">parameters</span><span class="pi">:</span>
        <span class="na">region</span><span class="pi">:</span> <span class="s">us-east-1</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="install-ceph-cluster">Install Ceph Cluster</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install Ceph cluster using Helm</span>
helm <span class="nb">install</span> <span class="nt">--create-namespace</span> <span class="nt">--namespace</span> rook-ceph rook-ceph-cluster <span class="nt">--set</span> <span class="nv">operatorNamespace</span><span class="o">=</span>rook-ceph <span class="nb">.</span> <span class="nt">-f</span> somaz-values.yaml

<span class="c"># Upgrade (if needed)</span>
helm upgrade <span class="nt">--create-namespace</span> <span class="nt">--namespace</span> rook-ceph rook-ceph-cluster <span class="nt">--set</span> <span class="nv">operatorNamespace</span><span class="o">=</span>rook-ceph <span class="nb">.</span> <span class="nt">-f</span> somaz-values.yaml
</code></pre></div></div>

<p><br /></p>

<h3 id="troubleshooting-installation-issues">Troubleshooting Installation Issues</h3>

<p>If installation fails, clean up and restart:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Delete Ceph cluster</span>
helm delete <span class="nt">-n</span> rook-ceph rook-ceph-cluster

<span class="c"># Delete resources</span>
kubectl delete cephblockpool <span class="nt">-n</span> rook-ceph ceph-blockpool
kubectl delete cephobjectstore <span class="nt">-n</span> rook-ceph ceph-objectstore
kubectl delete cephfilesystem <span class="nt">-n</span> rook-ceph ceph-filesystem
kubectl delete cephfilesystemsubvolumegroups.ceph.rook.io <span class="nt">-n</span> rook-ceph ceph-filesystem-csi

<span class="c"># Delete Ceph CRDs</span>
kubectl get crd <span class="nt">-o</span> name | <span class="nb">grep</span> <span class="s1">'ceph.'</span> | xargs kubectl delete
kubectl get crd <span class="nt">-o</span> name | <span class="nb">grep</span> <span class="s1">'objectbucket.'</span> | xargs kubectl delete

<span class="c"># Delete operator</span>
helm delete <span class="nt">-n</span> rook-ceph rook-ceph

<span class="c"># Verify cleanup</span>
kubectl get all <span class="nt">-n</span> rook-ceph

<span class="c"># Reinitialize disks</span>
<span class="nb">sudo </span>wipefs <span class="nt">-a</span> /dev/sdb
<span class="nb">sudo </span>wipefs <span class="nt">-a</span> /dev/sdc
<span class="nb">sudo </span>wipefs <span class="nt">-a</span> /dev/sdd

<span class="c"># Reinstall</span>
<span class="nb">cd</span> ~/rook/deploy/charts/rook-ceph
helm <span class="nb">install</span> <span class="nt">--create-namespace</span> <span class="nt">--namespace</span> rook-ceph rook-ceph <span class="se">\</span>
  <span class="nb">.</span> <span class="nt">-f</span> somaz-values.yaml

<span class="nb">cd</span> ~/rook/deploy/charts/rook-ceph-cluster
helm <span class="nb">install</span> <span class="nt">--create-namespace</span> <span class="nt">--namespace</span> rook-ceph rook-ceph-cluster <span class="se">\</span>
  <span class="nt">--set</span> <span class="nv">operatorNamespace</span><span class="o">=</span>rook-ceph <span class="nb">.</span> <span class="nt">-f</span> somaz-values.yaml
</code></pre></div></div>

<p><br /></p>

<h3 id="monitor-installation-progress">Monitor Installation Progress</h3>

<p>Check operator logs for troubleshooting:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl logs <span class="nt">-n</span> rook-ceph rook-ceph-operator-799cf9f45-wbxk5 | less
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="verification-and-testing">Verification and Testing</h2>

<p><br /></p>

<h3 id="verify-installation-1">Verify Installation</h3>

<p>Installation takes approximately 10 minutes. The warning “1 pool(s) have no replicas configured” is expected due to replica=1 configuration.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check all pods</span>
kubectl get pods <span class="nt">-n</span> rook-ceph
NAME                                                            READY   STATUS      RESTARTS        AGE
csi-cephfsplugin-7ldgx                                          2/2     Running     1 <span class="o">(</span>2m39s ago<span class="o">)</span>   3m26s
csi-cephfsplugin-ffmgn                                          2/2     Running     0               3m26s
csi-cephfsplugin-provisioner-67b5c7f475-5s659                   5/5     Running     1 <span class="o">(</span>2m32s ago<span class="o">)</span>   3m26s
csi-cephfsplugin-provisioner-67b5c7f475-lrkpm                   5/5     Running     0               3m26s
<span class="c"># ... additional pods</span>

<span class="c"># Check storage classes</span>
kubectl get storageclass
NAME                   PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
ceph-block <span class="o">(</span>default<span class="o">)</span>   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           <span class="nb">true                   </span>3m13s

<span class="c"># Check Ceph version</span>
kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> deploy/rook-ceph-tools <span class="nt">--</span> ceph version
ceph version 18.2.1 <span class="o">(</span>7fe91d5d5842e04be3b4f514d6dd990c54b29c76<span class="o">)</span> reef <span class="o">(</span>stable<span class="o">)</span>

<span class="c"># Check Ceph cluster status</span>
kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> deploy/rook-ceph-tools <span class="nt">--</span> ceph <span class="nt">-s</span>
  cluster:
    <span class="nb">id</span>:     e0949fd6-77ca-4572-bc62-bfe06d5127b8
    health: HEALTH_WARN
            11 pool<span class="o">(</span>s<span class="o">)</span> have no replicas configured

  services:
    mon: 1 daemons, quorum a <span class="o">(</span>age 4m<span class="o">)</span>
    mgr: a<span class="o">(</span>active, since 3m<span class="o">)</span>
    osd: 3 osds: 3 up <span class="o">(</span>since 3m<span class="o">)</span>, 3 <span class="k">in</span> <span class="o">(</span>since 3m<span class="o">)</span>

  data:
    volumes: 1/1 healthy
    pools:   11 pools, 137 pgs
    objects: 22 objects, 595 KiB
    usage:   94 MiB used, 150 GiB / 150 GiB avail
    pgs:     137 active+clean
</code></pre></div></div>

<p><img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755078120/rook-ceph-3_awlojb.png" alt="Ceph Status-1" />
<img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755078121/rook-ceph-2_b4m7mc.png" alt="Ceph Status-2" /></p>

<p><br /></p>

<h3 id="advanced-ceph-commands">Advanced Ceph Commands</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check health details (HEALTH_WARN can be ignored for replica=1)</span>
kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> deploy/rook-ceph-tools <span class="nt">--</span> ceph health detail

<span class="c"># Check placement group status</span>
kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> deploy/rook-ceph-tools <span class="nt">--</span> ceph pg dump_stuck

<span class="c"># Check OSD status</span>
kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> deploy/rook-ceph-tools <span class="nt">--</span> ceph osd status
ID  HOST                  USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE
 0  test-server-storage  34.6M  49.9G      0        0       0        0   exists,up
 1  test-server-storage  38.6M  49.9G      0        0       2      106   exists,up
 2  test-server-storage  34.6M  49.9G      0        0       0        0   exists,up

<span class="c"># Check OSD tree</span>
kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> deploy/rook-ceph-tools <span class="nt">--</span> ceph osd tree

<span class="c"># List OSD pools</span>
kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> deploy/rook-ceph-tools <span class="nt">--</span> ceph osd lspools

<span class="c"># Check Ceph resources</span>
kubectl get cephblockpool <span class="nt">-n</span> rook-ceph
kubectl get cephobjectstore <span class="nt">-n</span> rook-ceph
kubectl get cephfilesystem <span class="nt">-n</span> rook-ceph
kubectl get cephfilesystemsubvolumegroups.ceph.rook.io <span class="nt">-n</span> rook-ceph
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="testing-storage-functionality">Testing Storage Functionality</h2>

<p><br /></p>

<h3 id="create-test-pod-with-persistent-volume">Create Test Pod with Persistent Volume</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF &gt; test-pod.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">ceph-rbd-pvc</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">ceph-block</span>
  <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
      <span class="na">storage</span><span class="pi">:</span> <span class="s">1Gi</span>
<span class="nn">---</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">pod-using-ceph-rbd</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">my-container</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="na">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/var/lib/www/html"</span>
      <span class="na">name</span><span class="pi">:</span> <span class="s">mypd</span>
  <span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">mypd</span>
    <span class="na">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="na">claimName</span><span class="pi">:</span> <span class="s">ceph-rbd-pvc</span>
<span class="s">EOF</span>

<span class="s">kubectl apply -f test-pod.yaml</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="verify-test-resources">Verify Test Resources</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl get pv,pod,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   REASON   AGE
persistentvolume/pvc-27e58696-061e-40f3-afb4-f7c992179ffe   1Gi        RWO            Delete           Bound    default/ceph-rbd-pvc   ceph-block              25s

NAME                     READY   STATUS    RESTARTS   AGE
pod/pod-using-ceph-rbd   1/1     Running   0          25s

NAME                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/ceph-rbd-pvc   Bound    pvc-27e58696-061e-40f3-afb4-f7c992179ffe   1Gi        RWO            ceph-block     25s
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="ceph-dashboard-configuration">Ceph Dashboard Configuration</h2>

<p><br /></p>

<h3 id="create-external-dashboard-service">Create External Dashboard Service</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">cat &lt;&lt;EOF &gt; rook-ceph-mgr-dashboard-external.yaml</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">rook-ceph-mgr-dashboard-external-https</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">rook-ceph</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">rook-ceph-mgr</span>
    <span class="na">rook_cluster</span><span class="pi">:</span> <span class="s">rook-ceph</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">dashboard</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">8443</span>
    <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8443</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">rook-ceph-mgr</span>
    <span class="na">rook_cluster</span><span class="pi">:</span> <span class="s">rook-ceph</span>
    <span class="na">mgr_role</span><span class="pi">:</span> <span class="s">active</span>
  <span class="na">sessionAffinity</span><span class="pi">:</span> <span class="s">None</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
<span class="s">EOF</span>

<span class="s">kubectl apply -f rook-ceph-mgr-dashboard-external.yaml</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="configure-firewall-access">Configure Firewall Access</h3>

<div class="language-hcl highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Firewall ##</span>
<span class="nx">resource</span> <span class="s2">"google_compute_firewall"</span> <span class="s2">"nfs_server_ssh"</span> <span class="p">{</span>
  <span class="nx">name</span>    <span class="p">=</span> <span class="s2">"allow-ssh-nfs-server"</span>
  <span class="nx">network</span> <span class="p">=</span> <span class="nx">var</span><span class="err">.</span><span class="nx">shared_vpc</span>

  <span class="nx">allow</span> <span class="p">{</span>
    <span class="nx">protocol</span> <span class="p">=</span> <span class="s2">"tcp"</span>
    <span class="nx">ports</span>    <span class="p">=</span> <span class="p">[</span><span class="s2">"22"</span><span class="p">,</span> <span class="s2">"31571"</span><span class="p">]</span>
  <span class="p">}</span>

  <span class="nx">source_ranges</span> <span class="p">=</span> <span class="p">[</span><span class="s2">"${var.public_ip}/32"</span><span class="p">]</span>
  <span class="nx">target_tags</span>   <span class="p">=</span> <span class="p">[</span><span class="nx">var</span><span class="err">.</span><span class="nx">kubernetes_server</span><span class="p">,</span> <span class="nx">var</span><span class="err">.</span><span class="nx">kubernetes_client</span><span class="p">]</span>

  <span class="nx">depends_on</span> <span class="p">=</span> <span class="p">[</span><span class="nx">module</span><span class="err">.</span><span class="nx">vpc</span><span class="p">]</span>
<span class="p">}</span>
</code></pre></div></div>

<h3 id="dashboard-access-configuration">Dashboard Access Configuration</h3>

<p><br /></p>

<h4 id="get-default-password">Get Default Password</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Retrieve dashboard password</span>
kubectl <span class="nt">-n</span> rook-ceph get secret rook-ceph-dashboard-password <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">"{['data']['password']}"</span> | <span class="nb">base64</span> <span class="nt">--decode</span> <span class="o">&amp;&amp;</span> <span class="nb">echo</span>
.BzmuOklD&lt;D<span class="o">)</span>~T_@7,pr
</code></pre></div></div>

<p><br /></p>

<h4 id="reset-password-optional">Reset Password (Optional)</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create new password file</span>
<span class="nb">cat</span> <span class="o">&lt;&lt;</span><span class="no">EOF</span><span class="sh"> &gt; password.txt
somaz@2024
</span><span class="no">EOF

</span><span class="c"># Get tools pod name</span>
kubectl get pods <span class="nt">-n</span> rook-ceph | <span class="nb">grep </span>tools
rook-ceph-tools-d99b58dc-4xjpr                                  1/1     Running     0          55m

<span class="c"># Copy password file to tools pod</span>
kubectl <span class="nt">-n</span> rook-ceph <span class="nb">cp </span>password.txt rook-ceph-tools-d99b58dc-4xjpr:/tmp/password.txt

<span class="c"># Update dashboard credentials</span>
kubectl <span class="nt">-n</span> rook-ceph <span class="nb">exec</span> <span class="nt">-it</span> deploy/rook-ceph-tools <span class="nt">--</span> ceph dashboard set-login-credentials admin <span class="nt">-i</span> /tmp/password.txt
</code></pre></div></div>

<p><br /></p>

<h3 id="access-dashboard">Access Dashboard</h3>

<ol>
  <li><strong>URL</strong>: <code class="language-plaintext highlighter-rouge">https://[NODE-IP]:[NODE-PORT]</code></li>
  <li><strong>Username</strong>: <code class="language-plaintext highlighter-rouge">admin</code></li>
  <li><strong>Password</strong>: Retrieved from secret or reset password</li>
</ol>

<p><img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755078120/rook-ceph-4_h4uzxu.png" alt="Ceph Dashboard-1" /></p>

<p><br /></p>

<h4 id="the-dashboard-provides-comprehensive-monitoring-including">The dashboard provides comprehensive monitoring including:</h4>
<ul>
  <li>Cluster overview and health status</li>
  <li>OSD performance and utilization</li>
  <li>Pool statistics and configuration</li>
  <li>Physical disk visualization</li>
  <li>Performance metrics and graphs</li>
</ul>

<p><img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755078119/rook-ceph-5_nkqdxr.png" alt="Ceph Dashboard-2" /></p>

<p><br /></p>

<p><img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755078121/rook-ceph-6_mlbfdd.png" alt="Ceph Dashboard-3" /></p>

<p><br /></p>

<hr />

<h2 id="production-considerations">Production Considerations</h2>

<p><br /></p>

<h3 id="high-availability-configuration">High Availability Configuration</h3>

<p>For production deployments:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">cephClusterSpec</span><span class="pi">:</span>
  <span class="na">mon</span><span class="pi">:</span>
    <span class="na">count</span><span class="pi">:</span> <span class="m">3</span>  <span class="c1"># Odd number for quorum</span>
    <span class="na">allowMultiplePerNode</span><span class="pi">:</span> <span class="no">false</span>

  <span class="na">mgr</span><span class="pi">:</span>
    <span class="na">count</span><span class="pi">:</span> <span class="m">2</span>  <span class="c1"># Active-standby configuration</span>
    <span class="na">allowMultiplePerNode</span><span class="pi">:</span> <span class="no">false</span>

  <span class="na">storage</span><span class="pi">:</span>
    <span class="na">config</span><span class="pi">:</span>
      <span class="na">osdsPerDevice</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1"</span>
      <span class="na">metadataDevice</span><span class="pi">:</span> <span class="s2">"</span><span class="s">md0"</span>  <span class="c1"># Use SSD for metadata</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="resource-requirements">Resource Requirements</h3>

<ul>
  <li><strong>Memory</strong>: Minimum 32GB per storage node</li>
  <li><strong>CPU</strong>: 16+ cores recommended for storage nodes</li>
  <li><strong>Network</strong>: 10Gbps+ for optimal performance</li>
  <li><strong>Disks</strong>: Dedicated disks for OSDs, separate metadata devices</li>
</ul>

<p><br /></p>

<h3 id="security-best-practices">Security Best Practices</h3>

<ol>
  <li><strong>Network Security</strong>: Configure proper firewall rules</li>
  <li><strong>RBAC</strong>: Implement Kubernetes RBAC for Rook resources</li>
  <li><strong>Encryption</strong>: Enable encryption at rest and in transit</li>
  <li><strong>Dashboard Security</strong>: Use strong passwords and HTTPS</li>
</ol>

<p><br /></p>

<h3 id="monitoring-and-alerting">Monitoring and Alerting</h3>

<ol>
  <li><strong>Prometheus Integration</strong>: Enable Ceph metrics collection</li>
  <li><strong>Grafana Dashboards</strong>: Import Ceph-specific dashboards</li>
  <li><strong>Alert Rules</strong>: Configure alerts for cluster health</li>
  <li><strong>Log Aggregation</strong>: Centralize Ceph logs for analysis</li>
</ol>

<p><br /></p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>This comprehensive guide covered the complete deployment and operation of Rook-Ceph on Kubernetes, from infrastructure preparation through dashboard configuration.</p>

<p><br /></p>

<h3 id="key-achievements">Key Achievements:</h3>

<ol>
  <li><strong>Infrastructure Setup</strong>: Properly configured Kubernetes cluster with dedicated storage nodes</li>
  <li><strong>Operator Deployment</strong>: Successfully installed Rook operator using Helm</li>
  <li><strong>Cluster Configuration</strong>: Deployed Ceph cluster with proper resource allocation</li>
  <li><strong>Storage Integration</strong>: Configured storage classes and tested persistent volumes</li>
  <li><strong>Management Interface</strong>: Activated dashboard for monitoring and management</li>
</ol>

<p><br /></p>

<h3 id="best-practices-learned">Best Practices Learned:</h3>

<ul>
  <li><strong>Resource Planning</strong>: Adequate memory and CPU allocation is crucial</li>
  <li><strong>Disk Management</strong>: Proper disk initialization and configuration</li>
  <li><strong>Configuration Management</strong>: Using Helm charts for reproducible deployments</li>
  <li><strong>Monitoring</strong>: Dashboard provides essential cluster visibility</li>
  <li><strong>Troubleshooting</strong>: Systematic approach to problem resolution</li>
</ul>

<h4 id="future-enhancements">Future Enhancements:</h4>
<ul>
  <li>Implement multi-node Ceph cluster for production</li>
  <li>Configure backup and disaster recovery procedures</li>
  <li>Integrate with monitoring stack (Prometheus/Grafana)</li>
  <li>Implement automated capacity management</li>
  <li>Set up cross-cluster replication for geo-redundancy</li>
</ul>

<blockquote>
  <p>Rook-Ceph provides a powerful, Kubernetes-native solution for distributed storage, enabling organizations to run stateful applications with confidence in cloud-native environments.</p>
</blockquote>

<p><br /></p>

<hr />

<h2 id="references">References</h2>

<ul>
  <li><a href="https://rook.io/docs/rook/latest-release/Getting-Started/storage-architecture/#architecture">Rook Documentation</a></li>
  <li><a href="https://docs.ceph.com/en/latest/cephadm/">Ceph Official Documentation</a></li>
  <li><a href="https://github.com/rook/rook">Rook GitHub Repository</a></li>
  <li><a href="https://github.com/rook/rook/tree/master/deploy/charts/rook-ceph">Helm Charts for Rook</a></li>
  <li><a href="https://cloud.google.com/compute/docs/general-purpose-machines">Google Cloud Compute Engine</a></li>
  <li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Kubernetes Persistent Volumes</a></li>
</ul>


                <!-- Pagination links -->


            </article>

            
                <aside class="see-also">
                    <h2>See also</h2>
                    <ul>
                        
                        
                        
                            <li>
                                <a href="/category/data-engineering/oltp-olap/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755575326/oltp-olap_gqmvnx.png">
                                    
                                    <h3>Large-Scale Data Processing and Data Architecture Design</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/virtualization/kvm-nested/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755588581/kvm-nested_efue7m.png">
                                    
                                    <h3>KVM Nested Virtualization Complete Guide</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/data-engineering/data-flow-etl/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755575394/data-etl-1_sjgszi.png">
                                    
                                    <h3>Data ETL Pipeline Components and Architecture Guide</h3>
                                </a>
                            </li>
                        
                    </ul>
                </aside>
            

        </section>

        <!-- Add time bar only for pages without pagination -->
        
            <div class="time-bar" data-minutes="15">
    <span class="time-completed"></span>
    <span class="time-remaining"></span>
    <div class="bar">
        <span class="completed" style="width:0%;"></span>
        <span class="remaining" style="width:100%;"></span>
    </div>
</div>

            <button class="toggle-preview" onclick="togglePreview()">
    <span>Hide Preview ▼</span>
</button>

<div id="recommendationSection" class="recommendation">
    <div class="message">
        <strong>Why don't you read something next?</strong>
        <div>
            <button>
                <svg><use xlink:href="#icon-arrow-right"></use></svg>
                <span>Go back to top</span>
            </button>
        </div>
    </div>
    <div id="previewSection" class="preview-section">
        
        <a href="/category/iac/pulumi/" class="post-preview">
            <div class="image">
                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755152827/pulumi-1_bmpstq.png">
                
            </div>
            <h3 class="title">Pulumi: Modern Infrastructure as Code with Programming Languages</h3>
        </a>
    </div>
</div>

<style>
.toggle-preview {
    position: fixed;
    bottom: 20px;
    right: 20px;
    background: #333;
    color: white;
    border: none;
    padding: 8px 15px;
    border-radius: 4px;
    cursor: pointer;
    z-index: 1000;
    opacity: 0;
    transition: opacity 0.3s ease;
}

.toggle-preview:hover {
    background: #444;
}

.toggle-preview.visible {
    opacity: 1;
}

.recommendation {
    margin-top: 1000px;
    display: block;
    transition: all 0.3s ease;
}

.recommendation.hidden {
    display: none;
}

.hide-preview {
    margin-left: 10px;
    background: none;
    border: 1px solid #666;
    color: #666;
    padding: 5px 10px;
    border-radius: 4px;
    cursor: pointer;
}

.hide-preview:hover {
    background: #f0f0f0;
}

.preview-section {
    max-height: 1000px;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
}

.preview-section.hidden {
    max-height: 0;
}
</style>

<script>
function togglePreview() {
    const recommendation = document.getElementById('recommendationSection');
    const button = document.querySelector('.toggle-preview span');
    
    if (recommendation.classList.contains('hidden')) {
        recommendation.classList.remove('hidden');
        button.textContent = 'Hide Preview ▼';
    } else {
        recommendation.classList.add('hidden');
        button.textContent = 'Show Preview ▲';
    }
}

window.addEventListener('scroll', function() {
    const toggleButton = document.querySelector('.toggle-preview');
    const recommendation = document.getElementById('recommendationSection');
    const rect = recommendation.getBoundingClientRect();
    
    if (rect.top <= window.innerHeight) {
        toggleButton.classList.add('visible');
    } else {
        toggleButton.classList.remove('visible');
    }
});
</script>

        

        <!-- Show modal if the post is the last one -->
        

        <!-- Show modal before user leaves the page -->
        

        <!-- Add your newsletter subscription form here -->

        <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;Step-by-step deployment of Rook-Ceph on Kubernetes including operator installation, cluster configuration, storage class setup, and dashboard management for production-ready distributed storage.&quot;%20https://somaz.blog/category/storage/deploying-rook-ceph-kubernetes-complete-guide/%20via%20&#64;twitter_username&hashtags=rook-ceph,kubernetes,ceph,helm,storage-operator,distributed-storage,cloud-native,persistent-volumes,dashboard,kubespray"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://somaz.blog/category/storage/deploying-rook-ceph-kubernetes-complete-guide/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

        

  <section class="author">
    <div class="details">
      
        <img class="img-rounded" src="/assets/img/uploads/profile.png" alt="Somaz">
      
      <p class="def">Author</p>
      <h3 class="name">
        <a href="/authors/somaz/">Somaz</a>
      </h3>
      <p class="desc">DevOps engineer focused on cloud infrastructure and automation</p>
      <p>
        
          <a href="https://github.com/somaz94" title="Github">
            <svg><use xlink:href="#icon-github"></use></svg>
          </a>
        
        
        
        
        
        
          <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
            <svg><use xlink:href="#icon-linkedin"></use></svg>
          </a>
        
        
          <a href="https://somaz.tistory.com" title="Tistory">
            <svg><use xlink:href="#icon-tistory"></use></svg>
          </a>
        
      </p>
    </div>
  </section>

  
  
  
  
  
  
  
  

  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Somaz",
      
      "image": "/assets/img/uploads/profile.png",
      
      "jobTitle": "DevOps Engineer",
      "url": "https://somaz.blog/authors/somaz/",
      "sameAs": [
        "https://github.com/somaz94","https://www.linkedin.com/in/somaz","https://{{ author.tistory_username }}.tistory.com"
      ]
  }
  </script>


        

<section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'https-somaz94-github-io';
        var disqus_title = '';
        var disqus_url = '/category/storage/deploying-rook-ceph-kubernetes-complete-guide/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>



        <footer>
    <p>
      
        <a href="https://github.com/somaz94" title="Github">
          <svg><use xlink:href="#icon-github"></use></svg>
        </a>
      
      
        <a href="https://www.facebook.com/facebook_username" title="Facebook">
          <svg><use xlink:href="#icon-facebook"></use></svg>
        </a>
      
      
        <a href="https://twitter.com/twitter_username" title="Twitter">
          <svg><use xlink:href="#icon-twitter"></use></svg>
        </a>
      
      
        <a href="https://medium.com/@medium_username" title="Medium">
          <svg><use xlink:href="#icon-medium"></use></svg>
        </a>
      
      
        <a href="https://www.instagram.com/instagram_username" title="Instagram">
          <svg><use xlink:href="#icon-instagram"></use></svg>
        </a>
      
      
        <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
          <svg><use xlink:href="#icon-linkedin"></use></svg>
        </a>
      
      
        <a href="https://somaz.tistory.com" title="Tistory">
          <svg><use xlink:href="#icon-tistory"></use></svg>
        </a>
      
    </p>

    <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>


    <p>
      <a href="https://somaz.blog/sitemap.xml" title="sitemap">Sitemap</a> |
      <a href="https://somaz.blog/privacy-policy" title="Privacy Policy">Privacy Policy</a>
    </p>

    <p>
      <span>Somaz Tech Blog</span> <svg class="love"><use xlink:href="#icon-heart"></use></svg>
    </p>
</footer>










<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "somaz",
  "description": "DevOps engineer's tech blog.",
  "url": "https://somaz.blog/",
  "logo": {
      "@type": "ImageObject",
      "url": "https://somaz.blog/assets/img/icons/mediumtile.png",
      "width": "600",
      "height": "315"
  },
  "sameAs": [
    "https://github.com/somaz94","https://www.facebook.com/facebook_username","https://twitter.com/twitter_username","https://medium.com/@medium_username","https://www.instagram.com/instagram_username","https://www.linkedin.com/in/somaz","https://{{ site.tistory_username }}.tistory.com"
  ]
}
</script>

<!-- Include the script that allows Netlify CMS login -->
<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>

<!-- Include the website scripts -->
<script src="/assets/js/scripts.min.js"></script>

<!-- Include Google Analytics script -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  var host = window.location.hostname;
  if (host != 'localhost') {
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-XXXXXXXX-X');
  }
</script>
  


<!-- Include extra scripts -->



        

        
        
        
        
        
        
        
        
        <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "BlogPosting",
            "name": "Deploying and Operating Rook-Ceph on Kubernetes: Complete Implementation Guide",
            "headline": "A comprehensive guide to deploying Ceph storage on Kubernetes using Rook operator with Helm charts and operational best practices",
            "description": "Step-by-step deployment of Rook-Ceph on Kubernetes including operator installation, cluster configuration, storage class setup, and dashboard management for production-ready distributed storage.",
            "image": "https://res.cloudinary.com/dkcm26aem/image/upload/v1755078066/rook-ceph-1_ompm5x.png",
            "url": "https://somaz.blog/category/storage/deploying-rook-ceph-kubernetes-complete-guide/",
            "articleBody": "



Overview

In this article, we’ll cover the complete process of deploying and operating Ceph storage in a Kubernetes cluster using Rook-Ceph.

Rook is an open-source orchestration tool that manages Ceph in a Kubernetes-native way, abstracting complex distributed storage systems into Kubernetes resources for easy installation and operation. It helps simplify the deployment and management of sophisticated storage infrastructure.

In this implementation, we built a Kubernetes cluster using Kubespray on Google Cloud VM environment, then installed the Rook Operator and Rook-Ceph cluster using Helm Charts. We configured dedicated storage nodes for Ceph OSDs and properly customized Rook’s values.yaml to set up MON/MGR/OSD/MDS/Object Store components.

Subsequently, we configured RBD-based StorageClass to enable Pods to use Ceph block storage, and finally activated the Ceph Dashboard for web UI-based cluster monitoring capabilities.





What is Cephadm?

Cephadm is Ceph’s latest deployment and management tool, introduced starting with the Ceph Octopus release.

It’s designed to simplify deploying, configuring, managing, and scaling Ceph clusters. It can bootstrap a cluster with a single command and deploys Ceph services using container technology.

Cephadm doesn’t rely on external configuration tools like Ansible, Rook, or Salt. However, these external configuration tools can be used to automate tasks not performed by cephadm itself.

Related Resources:

  cephadm-ansible
  Rook Introduction
  ceph-salt






What is Rook-Ceph?

Ceph is a scalable distributed storage solution for block storage, object storage, and shared file systems, proven through years of production deployments.

Rook is an open-source orchestration framework for deploying, operating, and managing distributed storage systems on Kubernetes. It focuses on automatically managing Ceph within Kubernetes clusters and integrating storage solutions into cloud-native environments.

Using Rook, you can easily deploy and manage complex distributed storage systems like Ceph while leveraging Kubernetes’ automation and management capabilities.





Kubernetes Installation

Refer to the linked article for Kubernetes installation.

Important note: Storage nodes must have at least 32GB of memory.



Infrastructure Requirements

Master Node (Control Plane)


  
    
      Component
      IP
      CPU
      Memory
    
    
      test-server
      10.77.101.18
      16
      32G
    
  
  Master (control plane) node specification


Worker Nodes


  
    
      Component
      IP
      CPU
      Memory
    
    
      test-server-agent
      10.77.101.12
      16
      32G
    
    
      test-server-storage
      10.77.101.16
      16
      32G
    
  
  Worker node specifications




Additional Storage Node Configuration

Add the following configuration for the storage node:





Verify Installation

kubectl get nodes
NAME                  STATUS   ROLES           AGE   VERSION
test-server           Ready    control-plane   55s   v1.29.1
test-server-agent     Ready    &amp;lt;none&amp;gt;          55s   v1.29.1
test-server-storage   Ready    &amp;lt;none&amp;gt;          55s   v1.29.1






Rook-Ceph Operator Installation



Clone Repository and Prepare Values

git clone https://github.com/rook/rook.git
cd ~/rook/deploy/charts/rook-ceph
cp values.yaml somaz-values.yaml




Install Rook Operator using Helm

# Add Rook Helm repository
helm repo add rook-release https://charts.rook.io/release
helm repo update

# Install using local git repository
helm install --create-namespace --namespace rook-ceph rook-ceph . -f somaz-values.yaml

# Upgrade (if needed)
helm upgrade --create-namespace --namespace rook-ceph rook-ceph . -f somaz-values.yaml




Verify Operator Installation

# Check Custom Resource Definitions
kubectl get crd | grep ceph

# Verify operator pod
kubectl get pods -n rook-ceph
NAME                                 READY   STATUS    RESTARTS   AGE
rook-ceph-operator-799cf9f45-946r6   1/1     Running   0          28s






Rook-Ceph Cluster Installation



Prepare Storage Disks

First, verify and initialize disks on the test-server-storage node:

# Check available disks
lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
loop0     7:0    0  63.9M  1 loop /snap/core20/2105
loop1     7:1    0 368.2M  1 loop /snap/google-cloud-cli/207
loop2     7:2    0  40.4M  1 loop /snap/snapd/20671
loop3     7:3    0  91.9M  1 loop /snap/lxd/24061
sda       8:0    0    50G  0 disk
├─sda1    8:1    0  49.9G  0 part /
├─sda14   8:14   0     4M  0 part
└─sda15   8:15   0   106M  0 part /boot/efi
sdb       8:16   0    50G  0 disk
sdc       8:32   0    50G  0 disk
sdd       8:48   0    50G  0 disk

# Initialize disks
sudo wipefs -a /dev/sdb
sudo wipefs -a /dev/sdc
sudo wipefs -a /dev/sdd




Configure Cluster Values

cd ~/rook/deploy/charts/rook-ceph-cluster
cp values.yaml somaz-values.yaml




Edit the somaz-values.yaml file with the following key configurations:

configOverride: |
  [global]
  mon_allow_pool_delete = true
  osd_pool_default_size = 1
  osd_pool_default_min_size = 1

# Enable debugging toolbox
toolbox:
  enabled: true

cephClusterSpec:
  mon:
    count: 1
    allowMultiplePerNode: false

  mgr:
    count: 1
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true

  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
      - name: &quot;test-server-storage&quot;
        devices:
          - name: &quot;sdb&quot;
          - name: &quot;sdc&quot;
          - name: &quot;sdd&quot;

cephBlockPools:
  - name: ceph-blockpool
    spec:
      failureDomain: host
      replicated:
        size: 1

cephFileSystems:
  - name: ceph-filesystem
    spec:
      metadataPool:
        replicated:
          size: 1
      dataPools:
        - failureDomain: host
          replicated:
            size: 1
    storageClass:
      enabled: false
      isDefault: false
      name: ceph-filesystem

cephObjectStores:
  - name: ceph-objectstore
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 1
      dataPool:
        failureDomain: host
        erasureCoded:
          dataChunks: 2
          codingChunks: 1
      preservePoolsOnDelete: true
      gateway:
        port: 80
        instances: 1
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: false
      name: ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: &quot;Immediate&quot;
      parameters:
        region: us-east-1




Install Ceph Cluster

# Install Ceph cluster using Helm
helm install --create-namespace --namespace rook-ceph rook-ceph-cluster --set operatorNamespace=rook-ceph . -f somaz-values.yaml

# Upgrade (if needed)
helm upgrade --create-namespace --namespace rook-ceph rook-ceph-cluster --set operatorNamespace=rook-ceph . -f somaz-values.yaml




Troubleshooting Installation Issues

If installation fails, clean up and restart:

# Delete Ceph cluster
helm delete -n rook-ceph rook-ceph-cluster

# Delete resources
kubectl delete cephblockpool -n rook-ceph ceph-blockpool
kubectl delete cephobjectstore -n rook-ceph ceph-objectstore
kubectl delete cephfilesystem -n rook-ceph ceph-filesystem
kubectl delete cephfilesystemsubvolumegroups.ceph.rook.io -n rook-ceph ceph-filesystem-csi

# Delete Ceph CRDs
kubectl get crd -o name | grep &apos;ceph.&apos; | xargs kubectl delete
kubectl get crd -o name | grep &apos;objectbucket.&apos; | xargs kubectl delete

# Delete operator
helm delete -n rook-ceph rook-ceph

# Verify cleanup
kubectl get all -n rook-ceph

# Reinitialize disks
sudo wipefs -a /dev/sdb
sudo wipefs -a /dev/sdc
sudo wipefs -a /dev/sdd

# Reinstall
cd ~/rook/deploy/charts/rook-ceph
helm install --create-namespace --namespace rook-ceph rook-ceph \
  . -f somaz-values.yaml

cd ~/rook/deploy/charts/rook-ceph-cluster
helm install --create-namespace --namespace rook-ceph rook-ceph-cluster \
  --set operatorNamespace=rook-ceph . -f somaz-values.yaml




Monitor Installation Progress

Check operator logs for troubleshooting:

kubectl logs -n rook-ceph rook-ceph-operator-799cf9f45-wbxk5 | less






Verification and Testing



Verify Installation

Installation takes approximately 10 minutes. The warning “1 pool(s) have no replicas configured” is expected due to replica=1 configuration.

# Check all pods
kubectl get pods -n rook-ceph
NAME                                                            READY   STATUS      RESTARTS        AGE
csi-cephfsplugin-7ldgx                                          2/2     Running     1 (2m39s ago)   3m26s
csi-cephfsplugin-ffmgn                                          2/2     Running     0               3m26s
csi-cephfsplugin-provisioner-67b5c7f475-5s659                   5/5     Running     1 (2m32s ago)   3m26s
csi-cephfsplugin-provisioner-67b5c7f475-lrkpm                   5/5     Running     0               3m26s
# ... additional pods

# Check storage classes
kubectl get storageclass
NAME                   PROVISIONER                     RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
ceph-block (default)   rook-ceph.rbd.csi.ceph.com      Delete          Immediate           true                   3m13s

# Check Ceph version
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph version
ceph version 18.2.1 (7fe91d5d5842e04be3b4f514d6dd990c54b29c76) reef (stable)

# Check Ceph cluster status
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph -s
  cluster:
    id:     e0949fd6-77ca-4572-bc62-bfe06d5127b8
    health: HEALTH_WARN
            11 pool(s) have no replicas configured

  services:
    mon: 1 daemons, quorum a (age 4m)
    mgr: a(active, since 3m)
    osd: 3 osds: 3 up (since 3m), 3 in (since 3m)

  data:
    volumes: 1/1 healthy
    pools:   11 pools, 137 pgs
    objects: 22 objects, 595 KiB
    usage:   94 MiB used, 150 GiB / 150 GiB avail
    pgs:     137 active+clean







Advanced Ceph Commands

# Check health details (HEALTH_WARN can be ignored for replica=1)
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph health detail

# Check placement group status
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph pg dump_stuck

# Check OSD status
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph osd status
ID  HOST                  USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE
 0  test-server-storage  34.6M  49.9G      0        0       0        0   exists,up
 1  test-server-storage  38.6M  49.9G      0        0       2      106   exists,up
 2  test-server-storage  34.6M  49.9G      0        0       0        0   exists,up

# Check OSD tree
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph osd tree

# List OSD pools
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph osd lspools

# Check Ceph resources
kubectl get cephblockpool -n rook-ceph
kubectl get cephobjectstore -n rook-ceph
kubectl get cephfilesystem -n rook-ceph
kubectl get cephfilesystemsubvolumegroups.ceph.rook.io -n rook-ceph






Testing Storage Functionality



Create Test Pod with Persistent Volume

cat &amp;lt;&amp;lt;EOF &amp;gt; test-pod.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ceph-rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ceph-block
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-ceph-rbd
spec:
  containers:
  - name: my-container
    image: nginx
    volumeMounts:
    - mountPath: &quot;/var/lib/www/html&quot;
      name: mypd
  volumes:
  - name: mypd
    persistentVolumeClaim:
      claimName: ceph-rbd-pvc
EOF

kubectl apply -f test-pod.yaml




Verify Test Resources

kubectl get pv,pod,pvc
NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   REASON   AGE
persistentvolume/pvc-27e58696-061e-40f3-afb4-f7c992179ffe   1Gi        RWO            Delete           Bound    default/ceph-rbd-pvc   ceph-block              25s

NAME                     READY   STATUS    RESTARTS   AGE
pod/pod-using-ceph-rbd   1/1     Running   0          25s

NAME                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/ceph-rbd-pvc   Bound    pvc-27e58696-061e-40f3-afb4-f7c992179ffe   1Gi        RWO            ceph-block     25s






Ceph Dashboard Configuration



Create External Dashboard Service

cat &amp;lt;&amp;lt;EOF &amp;gt; rook-ceph-mgr-dashboard-external.yaml
apiVersion: v1
kind: Service
metadata:
  name: rook-ceph-mgr-dashboard-external-https
  namespace: rook-ceph
  labels:
    app: rook-ceph-mgr
    rook_cluster: rook-ceph
spec:
  ports:
  - name: dashboard
    port: 8443
    protocol: TCP
    targetPort: 8443
  selector:
    app: rook-ceph-mgr
    rook_cluster: rook-ceph
    mgr_role: active
  sessionAffinity: None
  type: NodePort
EOF

kubectl apply -f rook-ceph-mgr-dashboard-external.yaml




Configure Firewall Access

## Firewall ##
resource &quot;google_compute_firewall&quot; &quot;nfs_server_ssh&quot; {
  name    = &quot;allow-ssh-nfs-server&quot;
  network = var.shared_vpc

  allow {
    protocol = &quot;tcp&quot;
    ports    = [&quot;22&quot;, &quot;31571&quot;]
  }

  source_ranges = [&quot;${var.public_ip}/32&quot;]
  target_tags   = [var.kubernetes_server, var.kubernetes_client]

  depends_on = [module.vpc]
}


Dashboard Access Configuration



Get Default Password

# Retrieve dashboard password
kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath=&quot;{[&apos;data&apos;][&apos;password&apos;]}&quot; | base64 --decode &amp;amp;&amp;amp; echo
.BzmuOklD&amp;lt;D)~T_@7,pr




Reset Password (Optional)

# Create new password file
cat &amp;lt;&amp;lt;EOF &amp;gt; password.txt
somaz@2024
EOF

# Get tools pod name
kubectl get pods -n rook-ceph | grep tools
rook-ceph-tools-d99b58dc-4xjpr                                  1/1     Running     0          55m

# Copy password file to tools pod
kubectl -n rook-ceph cp password.txt rook-ceph-tools-d99b58dc-4xjpr:/tmp/password.txt

# Update dashboard credentials
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph dashboard set-login-credentials admin -i /tmp/password.txt




Access Dashboard


  URL: https://[NODE-IP]:[NODE-PORT]
  Username: admin
  Password: Retrieved from secret or reset password






The dashboard provides comprehensive monitoring including:

  Cluster overview and health status
  OSD performance and utilization
  Pool statistics and configuration
  Physical disk visualization
  Performance metrics and graphs












Production Considerations



High Availability Configuration

For production deployments:

cephClusterSpec:
  mon:
    count: 3  # Odd number for quorum
    allowMultiplePerNode: false

  mgr:
    count: 2  # Active-standby configuration
    allowMultiplePerNode: false

  storage:
    config:
      osdsPerDevice: &quot;1&quot;
      metadataDevice: &quot;md0&quot;  # Use SSD for metadata




Resource Requirements


  Memory: Minimum 32GB per storage node
  CPU: 16+ cores recommended for storage nodes
  Network: 10Gbps+ for optimal performance
  Disks: Dedicated disks for OSDs, separate metadata devices




Security Best Practices


  Network Security: Configure proper firewall rules
  RBAC: Implement Kubernetes RBAC for Rook resources
  Encryption: Enable encryption at rest and in transit
  Dashboard Security: Use strong passwords and HTTPS




Monitoring and Alerting


  Prometheus Integration: Enable Ceph metrics collection
  Grafana Dashboards: Import Ceph-specific dashboards
  Alert Rules: Configure alerts for cluster health
  Log Aggregation: Centralize Ceph logs for analysis






Conclusion

This comprehensive guide covered the complete deployment and operation of Rook-Ceph on Kubernetes, from infrastructure preparation through dashboard configuration.



Key Achievements:


  Infrastructure Setup: Properly configured Kubernetes cluster with dedicated storage nodes
  Operator Deployment: Successfully installed Rook operator using Helm
  Cluster Configuration: Deployed Ceph cluster with proper resource allocation
  Storage Integration: Configured storage classes and tested persistent volumes
  Management Interface: Activated dashboard for monitoring and management




Best Practices Learned:


  Resource Planning: Adequate memory and CPU allocation is crucial
  Disk Management: Proper disk initialization and configuration
  Configuration Management: Using Helm charts for reproducible deployments
  Monitoring: Dashboard provides essential cluster visibility
  Troubleshooting: Systematic approach to problem resolution


Future Enhancements:

  Implement multi-node Ceph cluster for production
  Configure backup and disaster recovery procedures
  Integrate with monitoring stack (Prometheus/Grafana)
  Implement automated capacity management
  Set up cross-cluster replication for geo-redundancy



  Rook-Ceph provides a powerful, Kubernetes-native solution for distributed storage, enabling organizations to run stateful applications with confidence in cloud-native environments.






References


  Rook Documentation
  Ceph Official Documentation
  Rook GitHub Repository
  Helm Charts for Rook
  Google Cloud Compute Engine
  Kubernetes Persistent Volumes

",
            "wordcount": "2791",
            "inLanguage": "en",
            "dateCreated": "2025-11-26/",
            "datePublished": "2025-11-26/",
            "dateModified": "2025-11-26/",
            "author": {
                "@type": "Person",
                "name": "Somaz",
                
                "image": "/assets/img/uploads/profile.png",
                
                "jobTitle": "DevOps Engineer",
                "url": "https://somaz.blog/authors/somaz/",
                "sameAs": [
                    "https://github.com/somaz94","https://www.linkedin.com/in/somaz"
                ]
            },
            "publisher": {
                "@type": "Organization",
                "name": "somaz",
                "url": "https://somaz.blog/",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://somaz.blog/assets/img/blog-image.png",
                    "width": "600",
                    "height": "315"
                }
            },
            "mainEntityOfPage": "True",
            "genre": "STORAGE",
            "articleSection": "STORAGE",
            "keywords": ["rook-ceph","kubernetes","ceph","helm","storage-operator","distributed-storage","cloud-native","persistent-volumes","dashboard","kubespray"]
        }
        </script>
    </body>
</html>
