<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    

    
    

    
    

    
    

    <!-- ✅ Google Tag Manager 추가 -->
    <script>
        (function(w,d,s,l,i){
            w[l]=w[l]||[];
            w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});
            var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';
            j.async=true;
            j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;
            f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer','GTM-MBP83N4Q');
    </script>
      <!-- ✅ End Google Tag Manager -->

    <!-- Mermaid.js 직접 로드 -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>

    <title>Ceph Node Management: Safe Removal and Addition of MON/MGR/OSD Components | somaz</title>
    <meta name="description" content="Step-by-step procedures for safely removing and adding Ceph nodes (MON/MGR/OSD) including pre-checks, scrubbing management, authentication cleanup, and autom...">
    
        <meta name="keywords" content="ceph, node-management, osd, monitor, manager, cluster-management, ansible, storage, distributed-storage, operational-procedures">
    

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Ceph Node Management: Safe Removal and Addition of MON/MGR/OSD Components | somaz">
    <meta name="twitter:description" content="Step-by-step procedures for safely removing and adding Ceph nodes (MON/MGR/OSD) including pre-checks, scrubbing management, authentication cleanup, and autom...">

    
        <meta property="twitter:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1755077743/ceph-node-1_iaahrd.png">
    
    
    
        <meta name="twitter:site" content="@twitter_username">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="https://somaz.blog/category/storage/ceph-node-management-safe-removal-addition/">
    <meta property="og:title" content="Ceph Node Management: Safe Removal and Addition of MON/MGR/OSD Components | somaz">
    <meta property="og:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1755077743/ceph-node-1_iaahrd.png">
    <meta property="og:description" content="Step-by-step procedures for safely removing and adding Ceph nodes (MON/MGR/OSD) including pre-checks, scrubbing management, authentication cleanup, and autom...">
    <meta property="og:site_name" content="Somaz Tech Blog">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="somaz">
    <meta name="msapplication-TileColor" content="#141414">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#141414">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:300,400,700" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="canonical" href="https://somaz.blog/category/storage/ceph-node-management-safe-removal-addition/">
    <link rel="alternate" type="application/rss+xml" title="Somaz Tech Blog" href="https://somaz.blog/feed.xml" />

    <!-- Include extra styles -->
    

    <!-- JavaScript enabled/disabled -->
    <script>
        document.querySelector('html').classList.remove('no-js');
    </script>

    <!-- Google Adsense -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8725590811736154"
        crossorigin="anonymous"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet"> -->
    <!-- <link href="https://cdn.jsdelivr.net/gh/sunn-us/SUIT/fonts/variable/woff2/SUIT-Variable.css" rel="stylesheet"> -->
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css2?family=Albert+Sans:wght@400;500;700&display=swap" rel="stylesheet"> -->

    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml" />

</head>
<!-- ✅ Google Tag Manager (noscript) -->
<noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MBP83N4Q"
            height="0" width="0" style="display:none;visibility:hidden">
    </iframe>
</noscript>
<!-- ✅ End Google Tag Manager (noscript) -->
    <body class="has-push-menu">
        





        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 1000 1000"><path d="M969.8,870.3c27,27.7,27,71.8,0,99.1C955.7,983,937.9,990,920,990c-17.9,0-35.7-7-49.7-20.7L500,599L129.6,969.4C115.6,983,97.8,990,79.9,990s-35.7-7-49.7-20.7c-27-27.3-27-71.4,0-99.1L400.9,500L30.3,129.3c-27-27.3-27-71.4,0-99.1c27.3-27,71.8-27,99.4,0L500,400.9L870.4,30.2c27.7-27,71.8-27,99.4,0c27,27.7,27,71.8,0,99.1L599.1,500L969.8,870.3z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-clock" viewBox="0 0 1000 1000"><path d="M500,10C229.8,10,10,229.8,10,500c0,270.2,219.8,490,490,490c270.2,0,490-219.8,490-490C990,229.8,770.2,10,500,10z M500,910.2c-226.2,0-410.2-184-410.2-410.2c0-226.2,184-410.2,410.2-410.2c226.2,0,410.2,184,410.2,410.2C910.2,726.1,726.2,910.2,500,910.2z M753.1,374c8.2,11.9,5.2,28.1-6.6,36.3L509.9,573.7c-4.4,3.1-9.6,4.6-14.8,4.6c-4.1,0-8.3-1-12.1-3c-8.6-4.5-14-13.4-14-23.1V202.5c0-14.4,11.7-26.1,26.1-26.1c14.4,0,26.1,11.7,26.1,26.1v300l195.6-135.1C728.7,359.2,744.9,362.1,753.1,374z"/></symbol><symbol id="icon-calendar" viewBox="0 0 1000 1000"><path d="M920,500v420H80V500H920 M990,430H10v490c0,38.7,31.3,70,70,70h840c38.7,0,70-31.3,70-70V430L990,430z"/><path d="M850,80v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80H360v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80C72.8,80,10,142.7,10,220v140h980V220C990,142.7,927.2,80,850,80z"/><path d="M255,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C290,25.8,274.3,10,255,10z"/><path d="M745,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C780,25.8,764.3,10,745,10z"/></symbol><symbol id="icon-github" viewBox="0 0 12 14"><path d="M6 1q1.633 0 3.012 0.805t2.184 2.184 0.805 3.012q0 1.961-1.145 3.527t-2.957 2.168q-0.211 0.039-0.312-0.055t-0.102-0.234q0-0.023 0.004-0.598t0.004-1.051q0-0.758-0.406-1.109 0.445-0.047 0.801-0.141t0.734-0.305 0.633-0.52 0.414-0.82 0.16-1.176q0-0.93-0.617-1.609 0.289-0.711-0.062-1.594-0.219-0.070-0.633 0.086t-0.719 0.344l-0.297 0.187q-0.727-0.203-1.5-0.203t-1.5 0.203q-0.125-0.086-0.332-0.211t-0.652-0.301-0.664-0.105q-0.352 0.883-0.062 1.594-0.617 0.68-0.617 1.609 0 0.664 0.16 1.172t0.41 0.82 0.629 0.523 0.734 0.305 0.801 0.141q-0.305 0.281-0.383 0.805-0.164 0.078-0.352 0.117t-0.445 0.039-0.512-0.168-0.434-0.488q-0.148-0.25-0.379-0.406t-0.387-0.187l-0.156-0.023q-0.164 0-0.227 0.035t-0.039 0.090 0.070 0.109 0.102 0.094l0.055 0.039q0.172 0.078 0.34 0.297t0.246 0.398l0.078 0.18q0.102 0.297 0.344 0.48t0.523 0.234 0.543 0.055 0.434-0.027l0.18-0.031q0 0.297 0.004 0.691t0.004 0.426q0 0.141-0.102 0.234t-0.312 0.055q-1.812-0.602-2.957-2.168t-1.145-3.527q0-1.633 0.805-3.012t2.184-2.184 3.012-0.805zM2.273 9.617q0.023-0.055-0.055-0.094-0.078-0.023-0.102 0.016-0.023 0.055 0.055 0.094 0.070 0.047 0.102-0.016zM2.516 9.883q0.055-0.039-0.016-0.125-0.078-0.070-0.125-0.023-0.055 0.039 0.016 0.125 0.078 0.078 0.125 0.023zM2.75 10.234q0.070-0.055 0-0.148-0.062-0.102-0.133-0.047-0.070 0.039 0 0.141t0.133 0.055zM3.078 10.562q0.062-0.062-0.031-0.148-0.094-0.094-0.156-0.023-0.070 0.062 0.031 0.148 0.094 0.094 0.156 0.023zM3.523 10.758q0.023-0.086-0.102-0.125-0.117-0.031-0.148 0.055t0.102 0.117q0.117 0.047 0.148-0.047zM4.016 10.797q0-0.102-0.133-0.086-0.125 0-0.125 0.086 0 0.102 0.133 0.086 0.125 0 0.125-0.086zM4.469 10.719q-0.016-0.086-0.141-0.070-0.125 0.023-0.109 0.117t0.141 0.062 0.109-0.109z"></path></symbol><symbol id="icon-medium" viewBox="0 0 1000 1000"><path d="M336.5,240.2v641.5c0,9.1-2.3,16.9-6.8,23.2s-11.2,9.6-20,9.6c-6.2,0-12.2-1.5-18-4.4L37.3,782.7c-7.7-3.6-14.1-9.8-19.4-18.3S10,747.4,10,739V115.5c0-7.3,1.8-13.5,5.5-18.6c3.6-5.1,8.9-7.7,15.9-7.7c5.1,0,13.1,2.7,24.1,8.2l279.5,140C335.9,238.6,336.5,239.5,336.5,240.2L336.5,240.2z M371.5,295.5l292,473.6l-292-145.5V295.5z M990,305.3v576.4c0,9.1-2.6,16.5-7.7,22.1c-5.1,5.7-12,8.5-20.8,8.5s-17.3-2.4-25.7-7.1L694.7,784.9L990,305.3z M988.4,239.7c0,1.1-46.8,77.6-140.3,229.4C754.6,621,699.8,709.8,683.8,735.7L470.5,389l177.2-288.2c6.2-10.2,15.7-15.3,28.4-15.3c5.1,0,9.8,1.1,14.2,3.3l295.9,147.7C987.6,237.1,988.4,238.2,988.4,239.7L988.4,239.7z"/></symbol><symbol id="icon-instagram" viewBox="0 0 489.84 489.84"><path d="M249.62,50.46c65.4,0,73.14.25,99,1.43C372.47,53,385.44,57,394.07,60.32a75.88,75.88,0,0,1,28.16,18.32,75.88,75.88,0,0,1,18.32,28.16c3.35,8.63,7.34,21.6,8.43,45.48,1.18,25.83,1.43,33.57,1.43,99s-0.25,73.14-1.43,99c-1.09,23.88-5.08,36.85-8.43,45.48a81.11,81.11,0,0,1-46.48,46.48c-8.63,3.35-21.6,7.34-45.48,8.43-25.82,1.18-33.57,1.43-99,1.43s-73.15-.25-99-1.43c-23.88-1.09-36.85-5.08-45.48-8.43A75.88,75.88,0,0,1,77,423.86,75.88,75.88,0,0,1,58.69,395.7c-3.35-8.63-7.34-21.6-8.43-45.48-1.18-25.83-1.43-33.57-1.43-99s0.25-73.14,1.43-99c1.09-23.88,5.08-36.85,8.43-45.48A75.88,75.88,0,0,1,77,78.64a75.88,75.88,0,0,1,28.16-18.32c8.63-3.35,21.6-7.34,45.48-8.43,25.83-1.18,33.57-1.43,99-1.43m0-44.13c-66.52,0-74.86.28-101,1.47s-43.87,5.33-59.45,11.38A120.06,120.06,0,0,0,45.81,47.44,120.06,120.06,0,0,0,17.56,90.82C11.5,106.4,7.36,124.2,6.17,150.27s-1.47,34.46-1.47,101,0.28,74.86,1.47,101,5.33,43.87,11.38,59.45a120.06,120.06,0,0,0,28.25,43.38,120.06,120.06,0,0,0,43.38,28.25c15.58,6.05,33.38,10.19,59.45,11.38s34.46,1.47,101,1.47,74.86-.28,101-1.47,43.87-5.33,59.45-11.38a125.24,125.24,0,0,0,71.63-71.63c6.05-15.58,10.19-33.38,11.38-59.45s1.47-34.46,1.47-101-0.28-74.86-1.47-101-5.33-43.87-11.38-59.45a120.06,120.06,0,0,0-28.25-43.38,120.06,120.06,0,0,0-43.38-28.25C394.47,13.13,376.67,9,350.6,7.8s-34.46-1.47-101-1.47h0Z" transform="translate(-4.7 -6.33)" /><path d="M249.62,125.48A125.77,125.77,0,1,0,375.39,251.25,125.77,125.77,0,0,0,249.62,125.48Zm0,207.41a81.64,81.64,0,1,1,81.64-81.64A81.64,81.64,0,0,1,249.62,332.89Z" transform="translate(-4.7 -6.33)"/><circle cx="375.66" cy="114.18" r="29.39" /></symbol><symbol id="icon-linkedin" viewBox="0 0 12 14"><path d="M2.727 4.883v7.742h-2.578v-7.742h2.578zM2.891 2.492q0.008 0.57-0.395 0.953t-1.059 0.383h-0.016q-0.641 0-1.031-0.383t-0.391-0.953q0-0.578 0.402-0.957t1.051-0.379 1.039 0.379 0.398 0.957zM12 8.187v4.437h-2.57v-4.141q0-0.82-0.316-1.285t-0.988-0.465q-0.492 0-0.824 0.27t-0.496 0.668q-0.086 0.234-0.086 0.633v4.32h-2.57q0.016-3.117 0.016-5.055t-0.008-2.313l-0.008-0.375h2.57v1.125h-0.016q0.156-0.25 0.32-0.438t0.441-0.406 0.68-0.34 0.895-0.121q1.336 0 2.148 0.887t0.813 2.598z"></path></symbol><symbol id="icon-heart" viewBox="0 0 34 30"><path d="M17,29.7 L16.4,29.2 C3.5,18.7 0,15 0,9 C0,4 4,0 9,0 C13.1,0 15.4,2.3 17,4.1 C18.6,2.3 20.9,0 25,0 C30,0 34,4 34,9 C34,15 30.5,18.7 17.6,29.2 L17,29.7 Z M9,2 C5.1,2 2,5.1 2,9 C2,14.1 5.2,17.5 17,27.1 C28.8,17.5 32,14.1 32,9 C32,5.1 28.9,2 25,2 C21.5,2 19.6,4.1 18.1,5.8 L17,7.1 L15.9,5.8 C14.4,4.1 12.5,2 9,2 Z" id="Shape"></path></symbol><symbol id="icon-arrow-right" viewBox="0 0 25.452 25.452"><path d="M4.471,24.929v-2.004l12.409-9.788c0.122-0.101,0.195-0.251,0.195-0.411c0-0.156-0.073-0.31-0.195-0.409L4.471,2.526V0.522c0-0.2,0.115-0.384,0.293-0.469c0.18-0.087,0.396-0.066,0.552,0.061l15.47,12.202c0.123,0.1,0.195,0.253,0.195,0.409c0,0.16-0.072,0.311-0.195,0.411L5.316,25.34c-0.155,0.125-0.372,0.147-0.552,0.061C4.586,25.315,4.471,25.13,4.471,24.929z"/></symbol><symbol id="icon-star" viewBox="0 0 48 48"><path fill="currentColor" d="M44,24c0,11.045-8.955,20-20,20S4,35.045,4,24S12.955,4,24,4S44,12.955,44,24z"/><path fill="#ffffff" d="M24,11l3.898,7.898l8.703,1.301l-6.301,6.102l1.5,8.699L24,30.898L16.199,35l1.5-8.699l-6.301-6.102  l8.703-1.301L24,11z"/></symbol><symbol id="icon-read" viewBox="0 0 32 32"><path fill="currentColor" d="M29,4H3C1.343,4,0,5.343,0,7v18c0,1.657,1.343,3,3,3h10c0,0.552,0.448,1,1,1h4c0.552,0,1-0.448,1-1h10  c1.657,0,3-1.343,3-3V7C32,5.343,30.657,4,29,4z M29,5v20H18.708c-0.618,0-1.236,0.146-1.789,0.422l-0.419,0.21V5H29z M15.5,5  v20.632l-0.419-0.21C14.528,25.146,13.91,25,13.292,25H3V5H15.5z M31,25c0,1.103-0.897,2-2,2H18v1h-4v-1H3c-1.103,0-2-0.897-2-2V7  c0-0.737,0.405-1.375,1-1.722V25c0,0.552,0.448,1,1,1h10.292c0.466,0,0.925,0.108,1.342,0.317l0.919,0.46  c0.141,0.07,0.294,0.106,0.447,0.106c0.153,0,0.306-0.035,0.447-0.106l0.919-0.46C17.783,26.108,18.242,26,18.708,26H29  c0.552,0,1-0.448,1-1V5.278C30.595,5.625,31,6.263,31,7V25z M6,12.5C6,12.224,6.224,12,6.5,12h5c0.276,0,0.5,0.224,0.5,0.5  S11.776,13,11.5,13h-5C6.224,13,6,12.776,6,12.5z M6,14.5C6,14.224,6.224,14,6.5,14h5c0.276,0,0.5,0.224,0.5,0.5S11.776,15,11.5,15  h-5C6.224,15,6,14.776,6,14.5z M6,16.5C6,16.224,6.224,16,6.5,16h5c0.276,0,0.5,0.224,0.5,0.5S11.776,17,11.5,17h-5  C6.224,17,6,16.776,6,16.5z M20,12.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,13,25.5,13h-5  C20.224,13,20,12.776,20,12.5z M20,14.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,15,25.5,15h-5  C20.224,15,20,14.776,20,14.5z M20,16.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,17,25.5,17h-5  C20.224,17,20,16.776,20,16.5z"></path></symbol><symbol id="icon-tistory" viewBox="0 0 24 24"><path d="M4 4h16v3h-6v13h-4V7H4V4z"/></symbol></defs></svg>

        <header class="bar-header">
    <a id="menu" role="button">
        <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    </a>
    <h1 class="logo">
        <a href="/">
            
                somaz <span class="version">v3.1.2</span>
            
        </a>
    </h1>
    <a id="search" class="dosearch" role="button">
        <svg class="icon-search"><use xlink:href="#icon-search"></use></svg>
    </a>
    
        <a href="https://github.com/thiagorossener/jekflix-template" class="get-theme" role="button">
            Get this theme!
        </a>
    
</header>

<div id="mask" class="overlay"></div>

<aside class="sidebar" id="sidebar">
    <nav id="navigation">
      <h2>Menu</h2>
      <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>

    </nav>
</aside>

<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>



        <section class="post two-columns">
            <article role="article" class="post-content">
                <p class="post-info">
                    
                        <svg class="icon-calendar" id="date"><use xlink:href="#icon-calendar"></use></svg>
                        <time class="date" datetime="2025-10-24T00:00:00+00:00">
                            


October 24, 2025

                        </time>
                    
                    <svg id="clock" class="icon-clock"><use xlink:href="#icon-clock"></use></svg>
                    <span>12 min to read</span>
                </p>
                <h1 class="post-title">Ceph Node Management: Safe Removal and Addition of MON/MGR/OSD Components</h1>
                <p class="post-subtitle">A comprehensive guide to safely removing and adding Ceph cluster nodes while maintaining data integrity and cluster stability</p>

                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755077743/ceph-node-1_iaahrd.png" alt="Featured image" class="post-cover">
                

                <!-- Pagination links -->



                <!-- Add your table of contents here -->


                <p><br /></p>

<hr />

<h2 id="overview">Overview</h2>

<p>Today, we’ll explore the procedures for safely removing and adding nodes in a Ceph cluster environment.</p>

<p>Ceph is a distributed storage system where various nodes with different roles are configured within the cluster. Operations involving node removal or addition during production require extremely careful consideration for data integrity and cluster stability. Particularly when removing OSDs, essential preparations include verifying available storage capacity, disabling scrubbing, and monitoring cluster rebalancing status.</p>

<p>In this practical exercise, we’ll perform the complete procedure of safely removing Ceph OSDs, MGRs, and MONs, followed by using Ansible to configure new nodes in the cluster.</p>

<p><br /></p>

<hr />

<h2 id="pre-operation-checklist">Pre-Operation Checklist</h2>

<h3 id="critical-verification-steps">Critical Verification Steps:</h3>
<ul>
  <li><strong>Always verify cluster available space</strong> before removal operations</li>
  <li><strong>Confirm sufficient capacity</strong> to accommodate the capacity of nodes being removed</li>
  <li><strong>Check cluster health status</strong> and resolve any existing issues</li>
  <li><strong>Plan maintenance window</strong> for potential performance impact</li>
</ul>

<p><br /></p>

<hr />

<h2 id="part-1-ceph-node-removal">Part 1: Ceph Node Removal</h2>

<p><br /></p>

<h3 id="step-1-cluster-status-and-capacity-verification">Step 1: Cluster Status and Capacity Verification</h3>

<p>Before any removal operation, thoroughly assess cluster health and capacity.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check overall cluster status</span>
<span class="nb">sudo </span>ceph <span class="nt">-s</span>

<span class="c"># Verify OSD capacity and utilization</span>
<span class="nb">sudo </span>ceph osd <span class="nb">df</span>

<span class="c"># Check cluster capacity distribution</span>
<span class="nb">sudo </span>ceph <span class="nb">df</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="step-2-disable-scrubbing-prevent-io-load">Step 2: Disable Scrubbing (Prevent I/O Load)</h3>

<p>Temporarily disable scrubbing operations to reduce I/O load during node removal.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Disable regular scrubbing</span>
<span class="nb">sudo </span>ceph osd <span class="nb">set </span>noscrub

<span class="c"># Disable deep scrubbing</span>
<span class="nb">sudo </span>ceph osd <span class="nb">set </span>nodeep-scrub
</code></pre></div></div>

<p><br /></p>

<h3 id="step-3-ceph-osd-removal">Step 3: Ceph OSD Removal</h3>

<p>Remove OSDs from the target Ceph node (example: ceph2 node OSD removal).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Display current OSD tree structure</span>
<span class="nb">sudo </span>ceph osd tree
ID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF
<span class="nt">-1</span>       0.78119 root default
<span class="nt">-3</span>       0.39059     host ceph1
 1   hdd 0.19530         osd.1            up  1.00000 1.00000
 2   hdd 0.19530         osd.2            up  1.00000 1.00000
<span class="nt">-5</span>       0.39059     host ceph2
 0   hdd 0.19530         osd.0            up  1.00000 1.00000
 3   hdd 0.19530         osd.3            up  1.00000 1.00000

<span class="c"># Remove osd.0 (4-step process)</span>
<span class="nb">sudo </span>ceph osd out osd.0
marked out osd.0.

<span class="nb">sudo </span>ceph osd down osd.0
marked down osd.0.

<span class="nb">sudo </span>ceph osd <span class="nb">rm </span>osd.0
removed osd.0

<span class="nb">sudo </span>ceph osd crush remove osd.0
removed item <span class="nb">id </span>0 name <span class="s1">'osd.0'</span> from crush map

<span class="c"># Remove osd.3 (4-step process)</span>
<span class="nb">sudo </span>ceph osd out osd.3
marked out osd.3.

<span class="nb">sudo </span>ceph osd down osd.3
marked down osd.3.

<span class="nb">sudo </span>ceph osd <span class="nb">rm </span>osd.3
removed osd.3

<span class="nb">sudo </span>ceph osd crush remove osd.3
removed item <span class="nb">id </span>3 name <span class="s1">'osd.3'</span> from crush map

<span class="c"># Remove host from CRUSH map</span>
<span class="nb">sudo </span>ceph osd crush remove ceph2
removed item <span class="nb">id</span> <span class="nt">-5</span> name <span class="s1">'ceph2'</span> from crush map

<span class="c"># Verify OSD tree after removal</span>
<span class="nb">sudo </span>ceph osd tree
ID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF
<span class="nt">-1</span>       0.39059 root default
<span class="nt">-3</span>       0.39059     host ceph1
 1   hdd 0.19530         osd.1            up  1.00000 1.00000
 2   hdd 0.19530         osd.2            up  1.00000 1.00000
</code></pre></div></div>

<p><strong>Important Note</strong>: When removing OSDs, execute <code class="language-plaintext highlighter-rouge">down</code> and <code class="language-plaintext highlighter-rouge">rm</code> commands immediately in sequence, as OSDs can automatically come back up.</p>

<p><br /></p>

<h3 id="step-4-clean-up-osd-authentication-entries">Step 4: Clean Up OSD Authentication Entries</h3>

<p>Remove authentication entries for the deleted OSDs.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List current authentication entries</span>
<span class="nb">sudo </span>ceph auth list
installed auth entries:

osd.0
        key: <span class="nv">AQDkfipjW6P1ERAAcCdTZJ6lATN7i8wxwh7j3Q</span><span class="o">==</span>
        caps: <span class="o">[</span>mgr] allow profile osd
        caps: <span class="o">[</span>mon] allow profile osd
        caps: <span class="o">[</span>osd] allow <span class="k">*</span>
osd.1
        key: <span class="nv">AQDkfipjud7XFhAAqEEuJJtSofEOnHH5isz63w</span><span class="o">==</span>
        caps: <span class="o">[</span>mgr] allow profile osd
        caps: <span class="o">[</span>mon] allow profile osd
        caps: <span class="o">[</span>osd] allow <span class="k">*</span>
<span class="c"># ... additional entries</span>

<span class="c"># Delete authentication for removed OSDs</span>
<span class="nb">sudo </span>ceph auth del osd.0
updated

<span class="nb">sudo </span>ceph auth del osd.3
updated

<span class="nb">sudo </span>ceph auth del mgr.ceph2
updated

<span class="c"># Verify authentication cleanup</span>
<span class="nb">sudo </span>ceph auth list
</code></pre></div></div>

<p><br /></p>

<h3 id="step-5-ceph-mon-removal">Step 5: Ceph MON Removal</h3>

<p>Remove the Monitor daemon from the target Ceph node.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check current monitor status</span>
<span class="nb">sudo </span>ceph mon <span class="nb">stat
</span>e1: 2 mons at <span class="o">{</span><span class="nv">ceph1</span><span class="o">=[</span>v2:10.3.2.206:3300/0,v1:10.3.2.206:6789/0],ceph2<span class="o">=[</span>v2:10.3.2.207:3300/0,v1:10.3.2.207:6789/0]<span class="o">}</span>, election epoch 4, leader 0 ceph1, quorum 0,1 ceph1,ceph2

<span class="c"># Remove monitor</span>
<span class="nb">sudo </span>ceph mon remove ceph2
removing mon.ceph2 at <span class="o">[</span>v2:10.3.2.207:3300/0,v1:10.3.2.207:6789/0], there will be 1 monitors

<span class="c"># Verify monitor removal</span>
<span class="nb">sudo </span>ceph <span class="nt">-s</span>
  cluster:
    <span class="nb">id</span>:     14675ee4-b9dd-440b-9e73-e4c00a62eab1
    health: HEALTH_WARN
            noscrub,nodeep-scrub flag<span class="o">(</span>s<span class="o">)</span> <span class="nb">set

  </span>services:
    mon: 1 daemons, quorum ceph1 <span class="o">(</span>age 4s<span class="o">)</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="step-6-ceph-mgr-removal">Step 6: Ceph MGR Removal</h3>

<p>Transition the Manager daemon to standby and then remove it.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check current manager status</span>
<span class="nb">sudo </span>ceph <span class="nt">-s</span>
  services:
    mon: 1 daemons, quorum ceph1 <span class="o">(</span>age 6m<span class="o">)</span>
    mgr: ceph2<span class="o">(</span>active, since 4w<span class="o">)</span>, standbys: ceph1

<span class="c"># Fail over the manager</span>
<span class="nb">sudo </span>ceph mgr fail ceph2

<span class="c"># Verify manager failover</span>
<span class="nb">sudo </span>ceph <span class="nt">-s</span>
  services:
    mon: 1 daemons, quorum ceph1 <span class="o">(</span>age 7m<span class="o">)</span>
    mgr: ceph1<span class="o">(</span>active, since 3s<span class="o">)</span>, standbys: ceph2

<span class="c"># SSH to target node and stop manager service</span>
ssh <span class="o">[</span>target-ceph-node]

<span class="c"># Check manager service status</span>
<span class="nb">sudo </span>systemctl status ceph-mgr@ceph2
● ceph-mgr@ceph2.service - Ceph cluster manager daemon
   Loaded: loaded <span class="o">(</span>/usr/lib/systemd/system/ceph-mgr@.service<span class="p">;</span> enabled<span class="p">;</span> vendor preset: disabled<span class="o">)</span>
   Active: active <span class="o">(</span>running<span class="o">)</span> since Tue 2022-08-16 17:34:45 KST<span class="p">;</span> 1 months 4 days ago

<span class="c"># Stop manager service</span>
<span class="nb">sudo </span>systemctl stop ceph-mgr@ceph2

<span class="c"># Verify service is stopped</span>
<span class="nb">sudo </span>systemctl status ceph-mgr@ceph2
● ceph-mgr@ceph2.service - Ceph cluster manager daemon
   Loaded: loaded <span class="o">(</span>/usr/lib/systemd/system/ceph-mgr@.service<span class="p">;</span> enabled<span class="p">;</span> vendor preset: disabled<span class="o">)</span>
   Active: inactive <span class="o">(</span>dead<span class="o">)</span> since Tue 2022-09-20 16:43:21 KST<span class="p">;</span> 11s ago
</code></pre></div></div>

<p><br /></p>

<h3 id="step-7-re-enable-scrubbing">Step 7: Re-enable Scrubbing</h3>

<p>Restore normal scrubbing operations after node removal.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Re-enable deep scrubbing</span>
<span class="nb">sudo </span>ceph osd <span class="nb">unset </span>nodeep-scrub
nodeep-scrub is <span class="nb">unset</span>

<span class="c"># Re-enable regular scrubbing</span>
<span class="nb">sudo </span>ceph osd <span class="nb">unset </span>noscrub
noscrub is <span class="nb">unset</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="step-8-final-cluster-status-verification">Step 8: Final Cluster Status Verification</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check final cluster status</span>
<span class="nb">sudo </span>ceph <span class="nt">-s</span>
  cluster:
    <span class="nb">id</span>:     14675ee4-b9dd-440b-9e73-e4c00a62eab1
    health: HEALTH_WARN

  services:
    mon: 1 daemons, quorum ceph1 <span class="o">(</span>age 99m<span class="o">)</span>
    mgr: ceph1<span class="o">(</span>active, since 91m<span class="o">)</span>
    osd: 2 osds: 2 up <span class="o">(</span>since 2h<span class="o">)</span>, 2 <span class="k">in</span> <span class="o">(</span>since 2h<span class="o">)</span>
    rgw: 3 daemons active <span class="o">(</span>master1.rgw0, master2.rgw0, master3.rgw0<span class="o">)</span>

  data:
    pools:   11 pools, 228 pgs
    objects: 4.41k objects, 15 GiB
    usage:   32 GiB used, 368 GiB / 400 GiB avail
    pgs:     228 active+clean
</code></pre></div></div>

<p><strong>Note</strong>: <code class="language-plaintext highlighter-rouge">HEALTH_WARN</code> status is expected after node removal. This will resolve once replacement nodes are added.</p>

<p><br /></p>

<hr />

<h2 id="part-2-ceph-node-addition">Part 2: Ceph Node Addition</h2>

<p><br /></p>

<h3 id="step-1-prepare-new-ceph-osd-node">Step 1: Prepare New Ceph OSD Node</h3>

<p>Install the same OS as existing Ceph nodes and assign an IP address.</p>

<p><br /></p>

<h3 id="step-2-ssh-key-exchange">Step 2: SSH Key Exchange</h3>

<p>Enable SSH access to the new node by exchanging public keys.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Copy SSH public key to new node</span>
ssh-copy-id <span class="o">[</span>target-ceph-node]
</code></pre></div></div>

<p><br /></p>

<h3 id="step-3-time-synchronization">Step 3: Time Synchronization</h3>

<p>Configure time synchronization to ensure cluster consistency.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># SSH to new node</span>
ssh <span class="o">[</span>target-ceph-node]

<span class="c"># Configure chrony</span>
<span class="nb">sudo </span>vi /etc/chrony.conf
server <span class="o">[</span>control-node-ip] iburst

<span class="c"># Restart chrony service</span>
<span class="nb">sudo </span>systemctl restart chronyd

<span class="c"># Verify time synchronization</span>
chronyc sources
210 Number of sources <span class="o">=</span> 1
MS Name/IP address                   Stratum Poll Reach LastRx Last sample
<span class="o">===========================================================================================</span>
^<span class="k">*</span> <span class="o">[</span>control-node-ip]                 3   6   377    36   +489us[+1186us] +/-   40ms
</code></pre></div></div>

<p><br /></p>

<h3 id="step-4-update-inventory-configuration">Step 4: Update Inventory Configuration</h3>

<h4 id="modify-hostsini">Modify hosts.ini</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Set zone name</span>
<span class="nv">ZONE_NAME</span><span class="o">=[</span>target-zone]

<span class="c"># Navigate to project directory</span>
<span class="nb">cd</span> ~/somaz

<span class="c"># Create backup of hosts.ini</span>
<span class="nb">cp </span>inventory/<span class="nv">$ZONE_NAME</span>/hosts.ini inventory/<span class="nv">$ZONE_NAME</span>/hosts.ini.ceph-add

<span class="c"># Edit hosts.ini for new node</span>
vi inventory/<span class="nv">$ZONE_NAME</span>/hosts.ini.ceph-add
</code></pre></div></div>

<p>Add new node configuration:</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add new node entry
</span><span class="nn">[new-node-name]</span> <span class="py">ip</span><span class="p">=</span><span class="s">[new-node-ip]</span>

<span class="c"># Ceph cluster configuration
</span><span class="nn">[mons]</span>
<span class="c"># [existing-ceph-node]    # Comment out existing
</span><span class="err">+</span> <span class="nn">[new-node-name]</span>

<span class="nn">[mgrs]</span>
<span class="c"># [existing-ceph-node]    # Comment out existing
</span><span class="err">+</span> <span class="nn">[new-node-name]</span>

<span class="nn">[osds]</span>
<span class="c"># [existing-ceph-node]    # Comment out existing
</span><span class="err">+</span> <span class="nn">[new-node-name]</span>
</code></pre></div></div>

<h4 id="modify-extra-varsyml">Modify extra-vars.yml</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create backup of extra-vars</span>
<span class="nb">cp </span>inventory/<span class="nv">$ZONE_NAME</span>/extra-vars.yml inventory/<span class="nv">$ZONE_NAME</span>/extra-vars.yml.ceph-add

<span class="c"># Edit extra-vars for OSD configuration</span>
vi inventory/<span class="nv">$ZONE_NAME</span>/extra-vars.yml.ceph-add
</code></pre></div></div>

<p>Update OSD configuration if disk layout differs:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ceph osd</span>
<span class="na">osd_objectstore</span><span class="pi">:</span> <span class="s">bluestore</span>
<span class="na">lvm_volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">data</span><span class="pi">:</span> <span class="s">/dev/sdb</span>
  <span class="pi">-</span> <span class="na">data</span><span class="pi">:</span> <span class="s">/dev/sdc</span>
</code></pre></div></div>

<p><strong>Note</strong>: Only modify extra-vars if OSD configuration changes. Use existing configuration for identical setups.</p>

<p><br /></p>

<h3 id="step-5-execute-ansible-playbook">Step 5: Execute Ansible Playbook</h3>

<p>Deploy the new node using Ansible automation.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Run Ansible playbook with specific tags</span>
ansible-playbook <span class="nt">-b</span> <span class="nt">-u</span> somaz <span class="nt">-i</span> inventory/<span class="nv">$ZONE_NAME</span>/hosts.ini.ceph-add <span class="nt">--extra-vars</span><span class="o">=</span>@inventory/<span class="nv">$ZONE_NAME</span>/extra-vars.yml.ceph-add site.yml <span class="nt">--tags</span><span class="o">=</span>setup-os,ceph
</code></pre></div></div>

<p><br /></p>

<h3 id="step-6-verify-osd-addition">Step 6: Verify OSD Addition</h3>

<p>Confirm successful node addition and cluster health restoration.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check cluster status</span>
<span class="nb">sudo </span>ceph <span class="nt">-s</span>
  cluster:
    <span class="nb">id</span>:     9893a83c-63e2-41b6-a538-f72008e15a01
    health: HEALTH_OK

  services:
    mon: 2 daemons, quorum ceph1,ceph2 <span class="o">(</span>age 32m<span class="o">)</span>
    mgr: ceph1<span class="o">(</span>active, since 3h<span class="o">)</span>, standbys: ceph2
    osd: 4 osds: 4 up <span class="o">(</span>since 6m<span class="o">)</span>, 4 <span class="k">in</span> <span class="o">(</span>since 6m<span class="o">)</span>
    rgw: 3 daemons active <span class="o">(</span>master1.rgw0, master2.rgw0, master3.rgw0<span class="o">)</span>

  data:
    pools:   11 pools, 228 pgs
    objects: 200 objects, 4.7 KiB
    usage:   4.1 GiB used, 796 GiB / 800 GiB avail
    pgs:     228 active+clean

<span class="c"># Verify OSD tree structure</span>
<span class="nb">sudo </span>ceph osd tree
ID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF
<span class="nt">-1</span>       0.78119 root default
<span class="nt">-3</span>       0.39059     host ceph1
 1   hdd 0.19530         osd.1            up  1.00000 1.00000
 3   hdd 0.19530         osd.3            up  1.00000 1.00000
<span class="nt">-5</span>       0.39059     host ceph2
 0   hdd 0.19530         osd.0            up  1.00000 1.00000
 2   hdd 0.19530         osd.2            up  1.00000 1.00000
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="best-practices-and-operational-guidelines">Best Practices and Operational Guidelines</h2>

<p><br /></p>

<h3 id="node-removal-sequence">Node Removal Sequence</h3>
<ol>
  <li><strong>OSD Removal</strong>: Always remove OSDs first to ensure data migration</li>
  <li><strong>MON Removal</strong>: Remove monitors while maintaining quorum</li>
  <li><strong>MGR Removal</strong>: Fail over managers before service termination</li>
</ol>

<p><br /></p>

<h3 id="critical-safety-measures">Critical Safety Measures</h3>
<ul>
  <li><strong>Capacity Planning</strong>: Ensure 2x capacity headroom before removal</li>
  <li><strong>Scrubbing Management</strong>: Disable during operations, re-enable afterward</li>
  <li><strong>Authentication Cleanup</strong>: Remove stale authentication entries</li>
  <li><strong>Status Monitoring</strong>: Continuously monitor cluster health during operations</li>
</ul>

<p><br /></p>

<h3 id="automation-benefits">Automation Benefits</h3>
<ul>
  <li><strong>Ansible Integration</strong>: Standardized deployment procedures</li>
  <li><strong>Configuration Management</strong>: Version-controlled inventory and variables</li>
  <li><strong>Repeatability</strong>: Consistent node provisioning across environments</li>
  <li><strong>Error Reduction</strong>: Automated procedures minimize human error</li>
</ul>

<p><br /></p>

<hr />

<h2 id="troubleshooting-common-issues">Troubleshooting Common Issues</h2>

<p><br /></p>

<h3 id="osd-removal-issues">OSD Removal Issues</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># If OSD comes back up unexpectedly</span>
<span class="nb">sudo </span>ceph osd down osd.X
<span class="nb">sudo </span>ceph osd out osd.X
<span class="c"># Immediately follow with rm command</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="monitor-quorum-problems">Monitor Quorum Problems</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ensure odd number of monitors (3, 5, 7)</span>
<span class="c"># Never reduce below minimum quorum requirements</span>
<span class="nb">sudo </span>ceph mon <span class="nb">stat</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="manager-failover-issues">Manager Failover Issues</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Force manager failover if automatic failover fails</span>
<span class="nb">sudo </span>ceph mgr fail <span class="o">[</span>manager-name]
<span class="c"># Wait for new active manager before proceeding</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="authentication-cleanup">Authentication Cleanup</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Verify no orphaned authentication entries remain</span>
<span class="nb">sudo </span>ceph auth list 
</code></pre></div></div>

<p><br /></p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<div class="quote-box">
  <p> Node management operations in Ceph clusters require more than simply adding or removing nodes. </p>
  <p> The interconnected nature of various system components demands precise sequencing and command execution. </p>
  <p> Through this practical exercise, we've learned the principles of removing Ceph nodes in the sequence: <strong>OSD → MON → MGR,</strong> </p>
  <p> along with the complete workflow of scrubbing deactivation → authentication deletion → service termination → verification. </p>
  <p> We've also acquired practical expertise in rapid automated deployment of new nodes using Ansible. </p>
</div>

<p><br /></p>

<h3 id="key-takeaways">Key Takeaways:</h3>

<ol>
  <li><strong>Sequential Approach</strong>: Follow proper removal order (OSD → MON → MGR)</li>
  <li><strong>Capacity Planning</strong>: Always verify sufficient capacity before operations</li>
  <li><strong>Automation Value</strong>: Ansible dramatically simplifies node provisioning</li>
  <li><strong>Safety First</strong>: Disable scrubbing and monitor cluster health throughout</li>
  <li><strong>Documentation</strong>: Maintain detailed procedures for emergency response</li>
</ol>

<h4 id="production-considerations">Production Considerations:</h4>
<ul>
  <li>Always implement backup and disaster recovery scenarios before Ceph configuration changes</li>
  <li>Verify available capacity and cluster status as prerequisites</li>
  <li>Master log analysis and status verification commands for rapid issue response</li>
  <li>Plan for potential performance impacts during maintenance windows</li>
</ul>

<p>This learning experience has significantly enhanced confidence in Ceph cluster operations. Future exploration will include advanced operational topics such as CephFS, RGW, and block device configurations.</p>

<blockquote>
  <p><em>“In rapidly changing infrastructure environments, safe and automated cluster management is paramount.”</em></p>
</blockquote>

<p><br /></p>

<hr />

<h2 id="references">References</h2>

<ul>
  <li><a href="https://docs.ceph.com/en/quincy/">Ceph Official Documentation</a></li>
  <li><a href="https://docs.ceph.com/en/latest/rados/operations/add-or-rm-osds/">Ceph OSD Management</a></li>
  <li><a href="https://docs.ceph.com/en/latest/rados/operations/add-or-rm-mons/">Ceph Monitor Operations</a></li>
  <li><a href="https://docs.ceph.com/ceph-ansible/">Ansible Ceph Deployment</a></li>
  <li><a href="https://docs.ceph.com/en/latest/rados/troubleshooting/">Ceph Troubleshooting Guide</a></li>
</ul>


                <!-- Pagination links -->


            </article>

            
                <aside class="see-also">
                    <h2>See also</h2>
                    <ul>
                        
                        
                        
                            <li>
                                <a href="/category/aws/aws-efs-kubernetes-guide/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1767685797/kubernetes-efs-1_ek4rbd.png">
                                    
                                    <h3>AWS Elastic File System (EFS) with Kubernetes - Complete Implementation Guide</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/cs/tls-https/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1767770122/tls-handshake-1_nwd3f0.png">
                                    
                                    <h3>TLS Handshake and Certificate Architecture - A Deep Dive into HTTPS Security</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/container/dockerfile/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755656803/dockerfile-1_xe6akt.png">
                                    
                                    <h3>Dockerfile - Complete Guide to Container Image Creation and Best Practices</h3>
                                </a>
                            </li>
                        
                    </ul>
                </aside>
            

        </section>

        <!-- Add time bar only for pages without pagination -->
        
            <div class="time-bar" data-minutes="12">
    <span class="time-completed"></span>
    <span class="time-remaining"></span>
    <div class="bar">
        <span class="completed" style="width:0%;"></span>
        <span class="remaining" style="width:100%;"></span>
    </div>
</div>

            <button class="toggle-preview" onclick="togglePreview()">
    <span>Hide Preview ▼</span>
</button>

<div id="recommendationSection" class="recommendation">
    <div class="message">
        <strong>Why don't you read something next?</strong>
        <div>
            <button>
                <svg><use xlink:href="#icon-arrow-right"></use></svg>
                <span>Go back to top</span>
            </button>
        </div>
    </div>
    <div id="previewSection" class="preview-section">
        
        <a href="/category/kubernetes/kubernetes-local-storage-openebs-longhorn-rook-ceph/" class="post-preview">
            <div class="image">
                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1754990152/kubernetes-local-storage_acrpto.png">
                
            </div>
            <h3 class="title">Kubernetes Local Storage Solutions — OpenEBS vs Longhorn vs Rook Ceph (Complete Comparison Guide)</h3>
        </a>
    </div>
</div>

<style>
.toggle-preview {
    position: fixed;
    bottom: 20px;
    right: 20px;
    background: #333;
    color: white;
    border: none;
    padding: 8px 15px;
    border-radius: 4px;
    cursor: pointer;
    z-index: 1000;
    opacity: 0;
    transition: opacity 0.3s ease;
}

.toggle-preview:hover {
    background: #444;
}

.toggle-preview.visible {
    opacity: 1;
}

.recommendation {
    margin-top: 1000px;
    display: block;
    transition: all 0.3s ease;
}

.recommendation.hidden {
    display: none;
}

.hide-preview {
    margin-left: 10px;
    background: none;
    border: 1px solid #666;
    color: #666;
    padding: 5px 10px;
    border-radius: 4px;
    cursor: pointer;
}

.hide-preview:hover {
    background: #f0f0f0;
}

.preview-section {
    max-height: 1000px;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
}

.preview-section.hidden {
    max-height: 0;
}
</style>

<script>
function togglePreview() {
    const recommendation = document.getElementById('recommendationSection');
    const button = document.querySelector('.toggle-preview span');
    
    if (recommendation.classList.contains('hidden')) {
        recommendation.classList.remove('hidden');
        button.textContent = 'Hide Preview ▼';
    } else {
        recommendation.classList.add('hidden');
        button.textContent = 'Show Preview ▲';
    }
}

window.addEventListener('scroll', function() {
    const toggleButton = document.querySelector('.toggle-preview');
    const recommendation = document.getElementById('recommendationSection');
    const rect = recommendation.getBoundingClientRect();
    
    if (rect.top <= window.innerHeight) {
        toggleButton.classList.add('visible');
    } else {
        toggleButton.classList.remove('visible');
    }
});
</script>

        

        <!-- Show modal if the post is the last one -->
        

        <!-- Show modal before user leaves the page -->
        

        <!-- Add your newsletter subscription form here -->

        <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;Step-by-step procedures for safely removing and adding Ceph nodes (MON/MGR/OSD) including pre-checks, scrubbing management, authentication cleanup, and automated deployment with Ansible.&quot;%20https://somaz.blog/category/storage/ceph-node-management-safe-removal-addition/%20via%20&#64;twitter_username&hashtags=ceph,node-management,osd,monitor,manager,cluster-management,ansible,storage,distributed-storage,operational-procedures"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://somaz.blog/category/storage/ceph-node-management-safe-removal-addition/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

        

  <section class="author">
    <div class="details">
      
        <img class="img-rounded" src="/assets/img/uploads/profile.png" alt="Somaz">
      
      <p class="def">Author</p>
      <h3 class="name">
        <a href="/authors/somaz/">Somaz</a>
      </h3>
      <p class="desc">DevOps engineer focused on cloud infrastructure and automation</p>
      <p>
        
          <a href="https://github.com/somaz94" title="Github">
            <svg><use xlink:href="#icon-github"></use></svg>
          </a>
        
        
        
        
        
        
          <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
            <svg><use xlink:href="#icon-linkedin"></use></svg>
          </a>
        
        
          <a href="https://somaz.tistory.com" title="Tistory">
            <svg><use xlink:href="#icon-tistory"></use></svg>
          </a>
        
      </p>
    </div>
  </section>

  
  
  
  
  
  
  
  

  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Somaz",
      
      "image": "/assets/img/uploads/profile.png",
      
      "jobTitle": "DevOps Engineer",
      "url": "https://somaz.blog/authors/somaz/",
      "sameAs": [
        "https://github.com/somaz94","https://www.linkedin.com/in/somaz","https://{{ author.tistory_username }}.tistory.com"
      ]
  }
  </script>


        

<section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'https-somaz94-github-io';
        var disqus_title = '';
        var disqus_url = '/category/storage/ceph-node-management-safe-removal-addition/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>



        <footer>
    <p>
      
        <a href="https://github.com/somaz94" title="Github">
          <svg><use xlink:href="#icon-github"></use></svg>
        </a>
      
      
        <a href="https://www.facebook.com/facebook_username" title="Facebook">
          <svg><use xlink:href="#icon-facebook"></use></svg>
        </a>
      
      
        <a href="https://twitter.com/twitter_username" title="Twitter">
          <svg><use xlink:href="#icon-twitter"></use></svg>
        </a>
      
      
        <a href="https://medium.com/@medium_username" title="Medium">
          <svg><use xlink:href="#icon-medium"></use></svg>
        </a>
      
      
        <a href="https://www.instagram.com/instagram_username" title="Instagram">
          <svg><use xlink:href="#icon-instagram"></use></svg>
        </a>
      
      
        <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
          <svg><use xlink:href="#icon-linkedin"></use></svg>
        </a>
      
      
        <a href="https://somaz.tistory.com" title="Tistory">
          <svg><use xlink:href="#icon-tistory"></use></svg>
        </a>
      
    </p>

    <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>


    <p>
      <a href="https://somaz.blog/sitemap.xml" title="sitemap">Sitemap</a> |
      <a href="https://somaz.blog/privacy-policy" title="Privacy Policy">Privacy Policy</a>
    </p>

    <p>
      <span>Somaz Tech Blog</span> <svg class="love"><use xlink:href="#icon-heart"></use></svg>
    </p>
</footer>










<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "somaz",
  "description": "DevOps engineer's tech blog.",
  "url": "https://somaz.blog/",
  "logo": {
      "@type": "ImageObject",
      "url": "https://somaz.blog/assets/img/icons/mediumtile.png",
      "width": "600",
      "height": "315"
  },
  "sameAs": [
    "https://github.com/somaz94","https://www.facebook.com/facebook_username","https://twitter.com/twitter_username","https://medium.com/@medium_username","https://www.instagram.com/instagram_username","https://www.linkedin.com/in/somaz","https://{{ site.tistory_username }}.tistory.com"
  ]
}
</script>

<!-- Include the script that allows Netlify CMS login -->
<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>

<!-- Include the website scripts -->
<script src="/assets/js/scripts.min.js"></script>

<!-- Include Google Analytics script -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  var host = window.location.hostname;
  if (host != 'localhost') {
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-XXXXXXXX-X');
  }
</script>
  


<!-- Include extra scripts -->



        

        
        
        
        
        
        
        
        
        <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "BlogPosting",
            "name": "Ceph Node Management: Safe Removal and Addition of MON/MGR/OSD Components",
            "headline": "A comprehensive guide to safely removing and adding Ceph cluster nodes while maintaining data integrity and cluster stability",
            "description": "Step-by-step procedures for safely removing and adding Ceph nodes (MON/MGR/OSD) including pre-checks, scrubbing management, authentication cleanup, and automated deployment with Ansible.",
            "image": "https://res.cloudinary.com/dkcm26aem/image/upload/v1755077743/ceph-node-1_iaahrd.png",
            "url": "https://somaz.blog/category/storage/ceph-node-management-safe-removal-addition/",
            "articleBody": "



Overview

Today, we’ll explore the procedures for safely removing and adding nodes in a Ceph cluster environment.

Ceph is a distributed storage system where various nodes with different roles are configured within the cluster. Operations involving node removal or addition during production require extremely careful consideration for data integrity and cluster stability. Particularly when removing OSDs, essential preparations include verifying available storage capacity, disabling scrubbing, and monitoring cluster rebalancing status.

In this practical exercise, we’ll perform the complete procedure of safely removing Ceph OSDs, MGRs, and MONs, followed by using Ansible to configure new nodes in the cluster.





Pre-Operation Checklist

Critical Verification Steps:

  Always verify cluster available space before removal operations
  Confirm sufficient capacity to accommodate the capacity of nodes being removed
  Check cluster health status and resolve any existing issues
  Plan maintenance window for potential performance impact






Part 1: Ceph Node Removal



Step 1: Cluster Status and Capacity Verification

Before any removal operation, thoroughly assess cluster health and capacity.

# Check overall cluster status
sudo ceph -s

# Verify OSD capacity and utilization
sudo ceph osd df

# Check cluster capacity distribution
sudo ceph df




Step 2: Disable Scrubbing (Prevent I/O Load)

Temporarily disable scrubbing operations to reduce I/O load during node removal.

# Disable regular scrubbing
sudo ceph osd set noscrub

# Disable deep scrubbing
sudo ceph osd set nodeep-scrub




Step 3: Ceph OSD Removal

Remove OSDs from the target Ceph node (example: ceph2 node OSD removal).

# Display current OSD tree structure
sudo ceph osd tree
ID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF
-1       0.78119 root default
-3       0.39059     host ceph1
 1   hdd 0.19530         osd.1            up  1.00000 1.00000
 2   hdd 0.19530         osd.2            up  1.00000 1.00000
-5       0.39059     host ceph2
 0   hdd 0.19530         osd.0            up  1.00000 1.00000
 3   hdd 0.19530         osd.3            up  1.00000 1.00000

# Remove osd.0 (4-step process)
sudo ceph osd out osd.0
marked out osd.0.

sudo ceph osd down osd.0
marked down osd.0.

sudo ceph osd rm osd.0
removed osd.0

sudo ceph osd crush remove osd.0
removed item id 0 name &apos;osd.0&apos; from crush map

# Remove osd.3 (4-step process)
sudo ceph osd out osd.3
marked out osd.3.

sudo ceph osd down osd.3
marked down osd.3.

sudo ceph osd rm osd.3
removed osd.3

sudo ceph osd crush remove osd.3
removed item id 3 name &apos;osd.3&apos; from crush map

# Remove host from CRUSH map
sudo ceph osd crush remove ceph2
removed item id -5 name &apos;ceph2&apos; from crush map

# Verify OSD tree after removal
sudo ceph osd tree
ID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF
-1       0.39059 root default
-3       0.39059     host ceph1
 1   hdd 0.19530         osd.1            up  1.00000 1.00000
 2   hdd 0.19530         osd.2            up  1.00000 1.00000


Important Note: When removing OSDs, execute down and rm commands immediately in sequence, as OSDs can automatically come back up.



Step 4: Clean Up OSD Authentication Entries

Remove authentication entries for the deleted OSDs.

# List current authentication entries
sudo ceph auth list
installed auth entries:

osd.0
        key: AQDkfipjW6P1ERAAcCdTZJ6lATN7i8wxwh7j3Q==
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.1
        key: AQDkfipjud7XFhAAqEEuJJtSofEOnHH5isz63w==
        caps: [mgr] allow profile osd
        caps: [mon] allow profile osd
        caps: [osd] allow *
# ... additional entries

# Delete authentication for removed OSDs
sudo ceph auth del osd.0
updated

sudo ceph auth del osd.3
updated

sudo ceph auth del mgr.ceph2
updated

# Verify authentication cleanup
sudo ceph auth list




Step 5: Ceph MON Removal

Remove the Monitor daemon from the target Ceph node.

# Check current monitor status
sudo ceph mon stat
e1: 2 mons at {ceph1=[v2:10.3.2.206:3300/0,v1:10.3.2.206:6789/0],ceph2=[v2:10.3.2.207:3300/0,v1:10.3.2.207:6789/0]}, election epoch 4, leader 0 ceph1, quorum 0,1 ceph1,ceph2

# Remove monitor
sudo ceph mon remove ceph2
removing mon.ceph2 at [v2:10.3.2.207:3300/0,v1:10.3.2.207:6789/0], there will be 1 monitors

# Verify monitor removal
sudo ceph -s
  cluster:
    id:     14675ee4-b9dd-440b-9e73-e4c00a62eab1
    health: HEALTH_WARN
            noscrub,nodeep-scrub flag(s) set

  services:
    mon: 1 daemons, quorum ceph1 (age 4s)




Step 6: Ceph MGR Removal

Transition the Manager daemon to standby and then remove it.

# Check current manager status
sudo ceph -s
  services:
    mon: 1 daemons, quorum ceph1 (age 6m)
    mgr: ceph2(active, since 4w), standbys: ceph1

# Fail over the manager
sudo ceph mgr fail ceph2

# Verify manager failover
sudo ceph -s
  services:
    mon: 1 daemons, quorum ceph1 (age 7m)
    mgr: ceph1(active, since 3s), standbys: ceph2

# SSH to target node and stop manager service
ssh [target-ceph-node]

# Check manager service status
sudo systemctl status ceph-mgr@ceph2
● ceph-mgr@ceph2.service - Ceph cluster manager daemon
   Loaded: loaded (/usr/lib/systemd/system/ceph-mgr@.service; enabled; vendor preset: disabled)
   Active: active (running) since Tue 2022-08-16 17:34:45 KST; 1 months 4 days ago

# Stop manager service
sudo systemctl stop ceph-mgr@ceph2

# Verify service is stopped
sudo systemctl status ceph-mgr@ceph2
● ceph-mgr@ceph2.service - Ceph cluster manager daemon
   Loaded: loaded (/usr/lib/systemd/system/ceph-mgr@.service; enabled; vendor preset: disabled)
   Active: inactive (dead) since Tue 2022-09-20 16:43:21 KST; 11s ago




Step 7: Re-enable Scrubbing

Restore normal scrubbing operations after node removal.

# Re-enable deep scrubbing
sudo ceph osd unset nodeep-scrub
nodeep-scrub is unset

# Re-enable regular scrubbing
sudo ceph osd unset noscrub
noscrub is unset




Step 8: Final Cluster Status Verification

# Check final cluster status
sudo ceph -s
  cluster:
    id:     14675ee4-b9dd-440b-9e73-e4c00a62eab1
    health: HEALTH_WARN

  services:
    mon: 1 daemons, quorum ceph1 (age 99m)
    mgr: ceph1(active, since 91m)
    osd: 2 osds: 2 up (since 2h), 2 in (since 2h)
    rgw: 3 daemons active (master1.rgw0, master2.rgw0, master3.rgw0)

  data:
    pools:   11 pools, 228 pgs
    objects: 4.41k objects, 15 GiB
    usage:   32 GiB used, 368 GiB / 400 GiB avail
    pgs:     228 active+clean


Note: HEALTH_WARN status is expected after node removal. This will resolve once replacement nodes are added.





Part 2: Ceph Node Addition



Step 1: Prepare New Ceph OSD Node

Install the same OS as existing Ceph nodes and assign an IP address.



Step 2: SSH Key Exchange

Enable SSH access to the new node by exchanging public keys.

# Copy SSH public key to new node
ssh-copy-id [target-ceph-node]




Step 3: Time Synchronization

Configure time synchronization to ensure cluster consistency.

# SSH to new node
ssh [target-ceph-node]

# Configure chrony
sudo vi /etc/chrony.conf
server [control-node-ip] iburst

# Restart chrony service
sudo systemctl restart chronyd

# Verify time synchronization
chronyc sources
210 Number of sources = 1
MS Name/IP address                   Stratum Poll Reach LastRx Last sample
===========================================================================================
^* [control-node-ip]                 3   6   377    36   +489us[+1186us] +/-   40ms




Step 4: Update Inventory Configuration

Modify hosts.ini

# Set zone name
ZONE_NAME=[target-zone]

# Navigate to project directory
cd ~/somaz

# Create backup of hosts.ini
cp inventory/$ZONE_NAME/hosts.ini inventory/$ZONE_NAME/hosts.ini.ceph-add

# Edit hosts.ini for new node
vi inventory/$ZONE_NAME/hosts.ini.ceph-add


Add new node configuration:

# Add new node entry
[new-node-name] ip=[new-node-ip]

# Ceph cluster configuration
[mons]
# [existing-ceph-node]    # Comment out existing
+ [new-node-name]

[mgrs]
# [existing-ceph-node]    # Comment out existing
+ [new-node-name]

[osds]
# [existing-ceph-node]    # Comment out existing
+ [new-node-name]


Modify extra-vars.yml

# Create backup of extra-vars
cp inventory/$ZONE_NAME/extra-vars.yml inventory/$ZONE_NAME/extra-vars.yml.ceph-add

# Edit extra-vars for OSD configuration
vi inventory/$ZONE_NAME/extra-vars.yml.ceph-add


Update OSD configuration if disk layout differs:

## ceph osd
osd_objectstore: bluestore
lvm_volumes:
  - data: /dev/sdb
  - data: /dev/sdc


Note: Only modify extra-vars if OSD configuration changes. Use existing configuration for identical setups.



Step 5: Execute Ansible Playbook

Deploy the new node using Ansible automation.

# Run Ansible playbook with specific tags
ansible-playbook -b -u somaz -i inventory/$ZONE_NAME/hosts.ini.ceph-add --extra-vars=@inventory/$ZONE_NAME/extra-vars.yml.ceph-add site.yml --tags=setup-os,ceph




Step 6: Verify OSD Addition

Confirm successful node addition and cluster health restoration.

# Check cluster status
sudo ceph -s
  cluster:
    id:     9893a83c-63e2-41b6-a538-f72008e15a01
    health: HEALTH_OK

  services:
    mon: 2 daemons, quorum ceph1,ceph2 (age 32m)
    mgr: ceph1(active, since 3h), standbys: ceph2
    osd: 4 osds: 4 up (since 6m), 4 in (since 6m)
    rgw: 3 daemons active (master1.rgw0, master2.rgw0, master3.rgw0)

  data:
    pools:   11 pools, 228 pgs
    objects: 200 objects, 4.7 KiB
    usage:   4.1 GiB used, 796 GiB / 800 GiB avail
    pgs:     228 active+clean

# Verify OSD tree structure
sudo ceph osd tree
ID CLASS WEIGHT  TYPE NAME            STATUS REWEIGHT PRI-AFF
-1       0.78119 root default
-3       0.39059     host ceph1
 1   hdd 0.19530         osd.1            up  1.00000 1.00000
 3   hdd 0.19530         osd.3            up  1.00000 1.00000
-5       0.39059     host ceph2
 0   hdd 0.19530         osd.0            up  1.00000 1.00000
 2   hdd 0.19530         osd.2            up  1.00000 1.00000






Best Practices and Operational Guidelines



Node Removal Sequence

  OSD Removal: Always remove OSDs first to ensure data migration
  MON Removal: Remove monitors while maintaining quorum
  MGR Removal: Fail over managers before service termination




Critical Safety Measures

  Capacity Planning: Ensure 2x capacity headroom before removal
  Scrubbing Management: Disable during operations, re-enable afterward
  Authentication Cleanup: Remove stale authentication entries
  Status Monitoring: Continuously monitor cluster health during operations




Automation Benefits

  Ansible Integration: Standardized deployment procedures
  Configuration Management: Version-controlled inventory and variables
  Repeatability: Consistent node provisioning across environments
  Error Reduction: Automated procedures minimize human error






Troubleshooting Common Issues



OSD Removal Issues
# If OSD comes back up unexpectedly
sudo ceph osd down osd.X
sudo ceph osd out osd.X
# Immediately follow with rm command




Monitor Quorum Problems
# Ensure odd number of monitors (3, 5, 7)
# Never reduce below minimum quorum requirements
sudo ceph mon stat




Manager Failover Issues
# Force manager failover if automatic failover fails
sudo ceph mgr fail [manager-name]
# Wait for new active manager before proceeding




Authentication Cleanup
# Verify no orphaned authentication entries remain
sudo ceph auth list 






Conclusion


   Node management operations in Ceph clusters require more than simply adding or removing nodes. 
   The interconnected nature of various system components demands precise sequencing and command execution. 
   Through this practical exercise, we&apos;ve learned the principles of removing Ceph nodes in the sequence: OSD → MON → MGR, 
   along with the complete workflow of scrubbing deactivation → authentication deletion → service termination → verification. 
   We&apos;ve also acquired practical expertise in rapid automated deployment of new nodes using Ansible. 




Key Takeaways:


  Sequential Approach: Follow proper removal order (OSD → MON → MGR)
  Capacity Planning: Always verify sufficient capacity before operations
  Automation Value: Ansible dramatically simplifies node provisioning
  Safety First: Disable scrubbing and monitor cluster health throughout
  Documentation: Maintain detailed procedures for emergency response


Production Considerations:

  Always implement backup and disaster recovery scenarios before Ceph configuration changes
  Verify available capacity and cluster status as prerequisites
  Master log analysis and status verification commands for rapid issue response
  Plan for potential performance impacts during maintenance windows


This learning experience has significantly enhanced confidence in Ceph cluster operations. Future exploration will include advanced operational topics such as CephFS, RGW, and block device configurations.


  “In rapidly changing infrastructure environments, safe and automated cluster management is paramount.”






References


  Ceph Official Documentation
  Ceph OSD Management
  Ceph Monitor Operations
  Ansible Ceph Deployment
  Ceph Troubleshooting Guide

",
            "wordcount": "2228",
            "inLanguage": "en",
            "dateCreated": "2025-10-24/",
            "datePublished": "2025-10-24/",
            "dateModified": "2025-10-24/",
            "author": {
                "@type": "Person",
                "name": "Somaz",
                
                "image": "/assets/img/uploads/profile.png",
                
                "jobTitle": "DevOps Engineer",
                "url": "https://somaz.blog/authors/somaz/",
                "sameAs": [
                    "https://github.com/somaz94","https://www.linkedin.com/in/somaz"
                ]
            },
            "publisher": {
                "@type": "Organization",
                "name": "somaz",
                "url": "https://somaz.blog/",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://somaz.blog/assets/img/blog-image.png",
                    "width": "600",
                    "height": "315"
                }
            },
            "mainEntityOfPage": "True",
            "genre": "STORAGE",
            "articleSection": "STORAGE",
            "keywords": ["ceph","node-management","osd","monitor","manager","cluster-management","ansible","storage","distributed-storage","operational-procedures"]
        }
        </script>
    </body>
</html>
