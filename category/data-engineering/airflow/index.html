<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    

    
    

    
    

    
    

    <!-- ✅ Google Tag Manager 추가 -->
    <script>
        (function(w,d,s,l,i){
            w[l]=w[l]||[];
            w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});
            var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';
            j.async=true;
            j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;
            f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer','GTM-MBP83N4Q');
    </script>
      <!-- ✅ End Google Tag Manager -->

    <!-- Mermaid.js 직접 로드 -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>

    <title>What is Apache Airflow and How to Install It? | somaz</title>
    <meta name="description" content="Learn about Apache Airflow workflow orchestration platform, its core concepts, architecture, and how to install it using Docker and Kubernetes">
    
        <meta name="keywords" content="airflow, data-engineering, workflow-orchestration, kubernetes, docker">
    

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="What is Apache Airflow and How to Install It? | somaz">
    <meta name="twitter:description" content="Learn about Apache Airflow workflow orchestration platform, its core concepts, architecture, and how to install it using Docker and Kubernetes">

    
        <meta property="twitter:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1767852742/airflow-1_k0rziq.png">
    
    
    
        <meta name="twitter:site" content="@twitter_username">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="https://somaz.blog/category/data-engineering/airflow/">
    <meta property="og:title" content="What is Apache Airflow and How to Install It? | somaz">
    <meta property="og:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1767852742/airflow-1_k0rziq.png">
    <meta property="og:description" content="Learn about Apache Airflow workflow orchestration platform, its core concepts, architecture, and how to install it using Docker and Kubernetes">
    <meta property="og:site_name" content="Somaz Tech Blog">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="somaz">
    <meta name="msapplication-TileColor" content="#141414">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#141414">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:300,400,700" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="canonical" href="https://somaz.blog/category/data-engineering/airflow/">
    <link rel="alternate" type="application/rss+xml" title="Somaz Tech Blog" href="https://somaz.blog/feed.xml" />

    <!-- Include extra styles -->
    

    <!-- JavaScript enabled/disabled -->
    <script>
        document.querySelector('html').classList.remove('no-js');
    </script>

    <!-- Google Adsense -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8725590811736154"
        crossorigin="anonymous"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet"> -->
    <!-- <link href="https://cdn.jsdelivr.net/gh/sunn-us/SUIT/fonts/variable/woff2/SUIT-Variable.css" rel="stylesheet"> -->
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css2?family=Albert+Sans:wght@400;500;700&display=swap" rel="stylesheet"> -->

    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml" />

</head>
<!-- ✅ Google Tag Manager (noscript) -->
<noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MBP83N4Q"
            height="0" width="0" style="display:none;visibility:hidden">
    </iframe>
</noscript>
<!-- ✅ End Google Tag Manager (noscript) -->
    <body class="has-push-menu">
        





        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 1000 1000"><path d="M969.8,870.3c27,27.7,27,71.8,0,99.1C955.7,983,937.9,990,920,990c-17.9,0-35.7-7-49.7-20.7L500,599L129.6,969.4C115.6,983,97.8,990,79.9,990s-35.7-7-49.7-20.7c-27-27.3-27-71.4,0-99.1L400.9,500L30.3,129.3c-27-27.3-27-71.4,0-99.1c27.3-27,71.8-27,99.4,0L500,400.9L870.4,30.2c27.7-27,71.8-27,99.4,0c27,27.7,27,71.8,0,99.1L599.1,500L969.8,870.3z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-clock" viewBox="0 0 1000 1000"><path d="M500,10C229.8,10,10,229.8,10,500c0,270.2,219.8,490,490,490c270.2,0,490-219.8,490-490C990,229.8,770.2,10,500,10z M500,910.2c-226.2,0-410.2-184-410.2-410.2c0-226.2,184-410.2,410.2-410.2c226.2,0,410.2,184,410.2,410.2C910.2,726.1,726.2,910.2,500,910.2z M753.1,374c8.2,11.9,5.2,28.1-6.6,36.3L509.9,573.7c-4.4,3.1-9.6,4.6-14.8,4.6c-4.1,0-8.3-1-12.1-3c-8.6-4.5-14-13.4-14-23.1V202.5c0-14.4,11.7-26.1,26.1-26.1c14.4,0,26.1,11.7,26.1,26.1v300l195.6-135.1C728.7,359.2,744.9,362.1,753.1,374z"/></symbol><symbol id="icon-calendar" viewBox="0 0 1000 1000"><path d="M920,500v420H80V500H920 M990,430H10v490c0,38.7,31.3,70,70,70h840c38.7,0,70-31.3,70-70V430L990,430z"/><path d="M850,80v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80H360v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80C72.8,80,10,142.7,10,220v140h980V220C990,142.7,927.2,80,850,80z"/><path d="M255,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C290,25.8,274.3,10,255,10z"/><path d="M745,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C780,25.8,764.3,10,745,10z"/></symbol><symbol id="icon-github" viewBox="0 0 12 14"><path d="M6 1q1.633 0 3.012 0.805t2.184 2.184 0.805 3.012q0 1.961-1.145 3.527t-2.957 2.168q-0.211 0.039-0.312-0.055t-0.102-0.234q0-0.023 0.004-0.598t0.004-1.051q0-0.758-0.406-1.109 0.445-0.047 0.801-0.141t0.734-0.305 0.633-0.52 0.414-0.82 0.16-1.176q0-0.93-0.617-1.609 0.289-0.711-0.062-1.594-0.219-0.070-0.633 0.086t-0.719 0.344l-0.297 0.187q-0.727-0.203-1.5-0.203t-1.5 0.203q-0.125-0.086-0.332-0.211t-0.652-0.301-0.664-0.105q-0.352 0.883-0.062 1.594-0.617 0.68-0.617 1.609 0 0.664 0.16 1.172t0.41 0.82 0.629 0.523 0.734 0.305 0.801 0.141q-0.305 0.281-0.383 0.805-0.164 0.078-0.352 0.117t-0.445 0.039-0.512-0.168-0.434-0.488q-0.148-0.25-0.379-0.406t-0.387-0.187l-0.156-0.023q-0.164 0-0.227 0.035t-0.039 0.090 0.070 0.109 0.102 0.094l0.055 0.039q0.172 0.078 0.34 0.297t0.246 0.398l0.078 0.18q0.102 0.297 0.344 0.48t0.523 0.234 0.543 0.055 0.434-0.027l0.18-0.031q0 0.297 0.004 0.691t0.004 0.426q0 0.141-0.102 0.234t-0.312 0.055q-1.812-0.602-2.957-2.168t-1.145-3.527q0-1.633 0.805-3.012t2.184-2.184 3.012-0.805zM2.273 9.617q0.023-0.055-0.055-0.094-0.078-0.023-0.102 0.016-0.023 0.055 0.055 0.094 0.070 0.047 0.102-0.016zM2.516 9.883q0.055-0.039-0.016-0.125-0.078-0.070-0.125-0.023-0.055 0.039 0.016 0.125 0.078 0.078 0.125 0.023zM2.75 10.234q0.070-0.055 0-0.148-0.062-0.102-0.133-0.047-0.070 0.039 0 0.141t0.133 0.055zM3.078 10.562q0.062-0.062-0.031-0.148-0.094-0.094-0.156-0.023-0.070 0.062 0.031 0.148 0.094 0.094 0.156 0.023zM3.523 10.758q0.023-0.086-0.102-0.125-0.117-0.031-0.148 0.055t0.102 0.117q0.117 0.047 0.148-0.047zM4.016 10.797q0-0.102-0.133-0.086-0.125 0-0.125 0.086 0 0.102 0.133 0.086 0.125 0 0.125-0.086zM4.469 10.719q-0.016-0.086-0.141-0.070-0.125 0.023-0.109 0.117t0.141 0.062 0.109-0.109z"></path></symbol><symbol id="icon-medium" viewBox="0 0 1000 1000"><path d="M336.5,240.2v641.5c0,9.1-2.3,16.9-6.8,23.2s-11.2,9.6-20,9.6c-6.2,0-12.2-1.5-18-4.4L37.3,782.7c-7.7-3.6-14.1-9.8-19.4-18.3S10,747.4,10,739V115.5c0-7.3,1.8-13.5,5.5-18.6c3.6-5.1,8.9-7.7,15.9-7.7c5.1,0,13.1,2.7,24.1,8.2l279.5,140C335.9,238.6,336.5,239.5,336.5,240.2L336.5,240.2z M371.5,295.5l292,473.6l-292-145.5V295.5z M990,305.3v576.4c0,9.1-2.6,16.5-7.7,22.1c-5.1,5.7-12,8.5-20.8,8.5s-17.3-2.4-25.7-7.1L694.7,784.9L990,305.3z M988.4,239.7c0,1.1-46.8,77.6-140.3,229.4C754.6,621,699.8,709.8,683.8,735.7L470.5,389l177.2-288.2c6.2-10.2,15.7-15.3,28.4-15.3c5.1,0,9.8,1.1,14.2,3.3l295.9,147.7C987.6,237.1,988.4,238.2,988.4,239.7L988.4,239.7z"/></symbol><symbol id="icon-instagram" viewBox="0 0 489.84 489.84"><path d="M249.62,50.46c65.4,0,73.14.25,99,1.43C372.47,53,385.44,57,394.07,60.32a75.88,75.88,0,0,1,28.16,18.32,75.88,75.88,0,0,1,18.32,28.16c3.35,8.63,7.34,21.6,8.43,45.48,1.18,25.83,1.43,33.57,1.43,99s-0.25,73.14-1.43,99c-1.09,23.88-5.08,36.85-8.43,45.48a81.11,81.11,0,0,1-46.48,46.48c-8.63,3.35-21.6,7.34-45.48,8.43-25.82,1.18-33.57,1.43-99,1.43s-73.15-.25-99-1.43c-23.88-1.09-36.85-5.08-45.48-8.43A75.88,75.88,0,0,1,77,423.86,75.88,75.88,0,0,1,58.69,395.7c-3.35-8.63-7.34-21.6-8.43-45.48-1.18-25.83-1.43-33.57-1.43-99s0.25-73.14,1.43-99c1.09-23.88,5.08-36.85,8.43-45.48A75.88,75.88,0,0,1,77,78.64a75.88,75.88,0,0,1,28.16-18.32c8.63-3.35,21.6-7.34,45.48-8.43,25.83-1.18,33.57-1.43,99-1.43m0-44.13c-66.52,0-74.86.28-101,1.47s-43.87,5.33-59.45,11.38A120.06,120.06,0,0,0,45.81,47.44,120.06,120.06,0,0,0,17.56,90.82C11.5,106.4,7.36,124.2,6.17,150.27s-1.47,34.46-1.47,101,0.28,74.86,1.47,101,5.33,43.87,11.38,59.45a120.06,120.06,0,0,0,28.25,43.38,120.06,120.06,0,0,0,43.38,28.25c15.58,6.05,33.38,10.19,59.45,11.38s34.46,1.47,101,1.47,74.86-.28,101-1.47,43.87-5.33,59.45-11.38a125.24,125.24,0,0,0,71.63-71.63c6.05-15.58,10.19-33.38,11.38-59.45s1.47-34.46,1.47-101-0.28-74.86-1.47-101-5.33-43.87-11.38-59.45a120.06,120.06,0,0,0-28.25-43.38,120.06,120.06,0,0,0-43.38-28.25C394.47,13.13,376.67,9,350.6,7.8s-34.46-1.47-101-1.47h0Z" transform="translate(-4.7 -6.33)" /><path d="M249.62,125.48A125.77,125.77,0,1,0,375.39,251.25,125.77,125.77,0,0,0,249.62,125.48Zm0,207.41a81.64,81.64,0,1,1,81.64-81.64A81.64,81.64,0,0,1,249.62,332.89Z" transform="translate(-4.7 -6.33)"/><circle cx="375.66" cy="114.18" r="29.39" /></symbol><symbol id="icon-linkedin" viewBox="0 0 12 14"><path d="M2.727 4.883v7.742h-2.578v-7.742h2.578zM2.891 2.492q0.008 0.57-0.395 0.953t-1.059 0.383h-0.016q-0.641 0-1.031-0.383t-0.391-0.953q0-0.578 0.402-0.957t1.051-0.379 1.039 0.379 0.398 0.957zM12 8.187v4.437h-2.57v-4.141q0-0.82-0.316-1.285t-0.988-0.465q-0.492 0-0.824 0.27t-0.496 0.668q-0.086 0.234-0.086 0.633v4.32h-2.57q0.016-3.117 0.016-5.055t-0.008-2.313l-0.008-0.375h2.57v1.125h-0.016q0.156-0.25 0.32-0.438t0.441-0.406 0.68-0.34 0.895-0.121q1.336 0 2.148 0.887t0.813 2.598z"></path></symbol><symbol id="icon-heart" viewBox="0 0 34 30"><path d="M17,29.7 L16.4,29.2 C3.5,18.7 0,15 0,9 C0,4 4,0 9,0 C13.1,0 15.4,2.3 17,4.1 C18.6,2.3 20.9,0 25,0 C30,0 34,4 34,9 C34,15 30.5,18.7 17.6,29.2 L17,29.7 Z M9,2 C5.1,2 2,5.1 2,9 C2,14.1 5.2,17.5 17,27.1 C28.8,17.5 32,14.1 32,9 C32,5.1 28.9,2 25,2 C21.5,2 19.6,4.1 18.1,5.8 L17,7.1 L15.9,5.8 C14.4,4.1 12.5,2 9,2 Z" id="Shape"></path></symbol><symbol id="icon-arrow-right" viewBox="0 0 25.452 25.452"><path d="M4.471,24.929v-2.004l12.409-9.788c0.122-0.101,0.195-0.251,0.195-0.411c0-0.156-0.073-0.31-0.195-0.409L4.471,2.526V0.522c0-0.2,0.115-0.384,0.293-0.469c0.18-0.087,0.396-0.066,0.552,0.061l15.47,12.202c0.123,0.1,0.195,0.253,0.195,0.409c0,0.16-0.072,0.311-0.195,0.411L5.316,25.34c-0.155,0.125-0.372,0.147-0.552,0.061C4.586,25.315,4.471,25.13,4.471,24.929z"/></symbol><symbol id="icon-star" viewBox="0 0 48 48"><path fill="currentColor" d="M44,24c0,11.045-8.955,20-20,20S4,35.045,4,24S12.955,4,24,4S44,12.955,44,24z"/><path fill="#ffffff" d="M24,11l3.898,7.898l8.703,1.301l-6.301,6.102l1.5,8.699L24,30.898L16.199,35l1.5-8.699l-6.301-6.102  l8.703-1.301L24,11z"/></symbol><symbol id="icon-read" viewBox="0 0 32 32"><path fill="currentColor" d="M29,4H3C1.343,4,0,5.343,0,7v18c0,1.657,1.343,3,3,3h10c0,0.552,0.448,1,1,1h4c0.552,0,1-0.448,1-1h10  c1.657,0,3-1.343,3-3V7C32,5.343,30.657,4,29,4z M29,5v20H18.708c-0.618,0-1.236,0.146-1.789,0.422l-0.419,0.21V5H29z M15.5,5  v20.632l-0.419-0.21C14.528,25.146,13.91,25,13.292,25H3V5H15.5z M31,25c0,1.103-0.897,2-2,2H18v1h-4v-1H3c-1.103,0-2-0.897-2-2V7  c0-0.737,0.405-1.375,1-1.722V25c0,0.552,0.448,1,1,1h10.292c0.466,0,0.925,0.108,1.342,0.317l0.919,0.46  c0.141,0.07,0.294,0.106,0.447,0.106c0.153,0,0.306-0.035,0.447-0.106l0.919-0.46C17.783,26.108,18.242,26,18.708,26H29  c0.552,0,1-0.448,1-1V5.278C30.595,5.625,31,6.263,31,7V25z M6,12.5C6,12.224,6.224,12,6.5,12h5c0.276,0,0.5,0.224,0.5,0.5  S11.776,13,11.5,13h-5C6.224,13,6,12.776,6,12.5z M6,14.5C6,14.224,6.224,14,6.5,14h5c0.276,0,0.5,0.224,0.5,0.5S11.776,15,11.5,15  h-5C6.224,15,6,14.776,6,14.5z M6,16.5C6,16.224,6.224,16,6.5,16h5c0.276,0,0.5,0.224,0.5,0.5S11.776,17,11.5,17h-5  C6.224,17,6,16.776,6,16.5z M20,12.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,13,25.5,13h-5  C20.224,13,20,12.776,20,12.5z M20,14.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,15,25.5,15h-5  C20.224,15,20,14.776,20,14.5z M20,16.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,17,25.5,17h-5  C20.224,17,20,16.776,20,16.5z"></path></symbol><symbol id="icon-tistory" viewBox="0 0 24 24"><path d="M4 4h16v3h-6v13h-4V7H4V4z"/></symbol></defs></svg>

        <header class="bar-header">
    <a id="menu" role="button">
        <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    </a>
    <h1 class="logo">
        <a href="/">
            
                somaz <span class="version">v3.1.2</span>
            
        </a>
    </h1>
    <a id="search" class="dosearch" role="button">
        <svg class="icon-search"><use xlink:href="#icon-search"></use></svg>
    </a>
    
        <a href="https://github.com/thiagorossener/jekflix-template" class="get-theme" role="button">
            Get this theme!
        </a>
    
</header>

<div id="mask" class="overlay"></div>

<aside class="sidebar" id="sidebar">
    <nav id="navigation">
      <h2>Menu</h2>
      <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>

    </nav>
</aside>

<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>



        <section class="post two-columns">
            <article role="article" class="post-content">
                <p class="post-info">
                    
                        <svg class="icon-calendar" id="date"><use xlink:href="#icon-calendar"></use></svg>
                        <time class="date" datetime="2025-12-02T00:00:00+00:00">
                            


December 2, 2025

                        </time>
                    
                    <svg id="clock" class="icon-clock"><use xlink:href="#icon-clock"></use></svg>
                    <span>14 min to read</span>
                </p>
                <h1 class="post-title">What is Apache Airflow and How to Install It?</h1>
                <p class="post-subtitle">A comprehensive guide to Apache Airflow concepts and installation</p>

                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1767852742/airflow-1_k0rziq.png" alt="Featured image" class="post-cover">
                

                <!-- Pagination links -->



                <!-- Add your table of contents here -->


                <p><br /></p>

<hr />

<h2 id="overview">Overview</h2>

<p>Today we’ll explore Apache Airflow, a powerful data pipeline and workflow orchestration tool.</p>

<p>Airflow is an essential tool in data engineering, DevOps, and MLOps that helps manage complex task dependencies and automated execution.</p>

<p>Originally developed at Airbnb and now maintained by the Apache Software Foundation,</p>

<p>Airflow offers various operators and extensibility, making it flexible for use across diverse cloud and on-premises environments.</p>

<p>In this post, we’ll understand Airflow’s core concepts and components, compare it with Kubernetes-based orchestration tool Argo Workflow, and learn how to install Airflow in Docker and Kubernetes environments.</p>

<p><br /></p>

<hr />

<h2 id="what-is-apache-airflow">What is Apache Airflow?</h2>

<p>Apache Airflow is an open-source tool for orchestrating complex computational workflows and data processing pipelines.</p>

<p>Started by Airbnb in 2014 and became part of the Apache Software Foundation in 2016, Airflow is used to manage task execution and ensure they run in the correct order within specified workflows.</p>

<p><br /></p>

<h3 id="introduction-to-apache-airflow">Introduction to Apache Airflow</h3>

<p>Airflow provides a platform to programmatically author, schedule, and monitor workflows, making it particularly valuable for data engineering teams managing complex ETL processes and data pipelines.</p>

<h4 id="key-design-principles">Key design principles:</h4>
<ul>
  <li><strong>Programmatic Workflow Definition:</strong> Define workflows as code using Python</li>
  <li><strong>Dynamic Pipeline Generation:</strong> Create workflows dynamically based on configuration</li>
  <li><strong>Extensible Architecture:</strong> Rich ecosystem of operators and hooks</li>
  <li><strong>Rich User Interface:</strong> Web-based UI for monitoring and managing workflows</li>
  <li><strong>Scalable Execution:</strong> Support for various execution environments</li>
</ul>

<blockquote>
  <p>Airflow excels in environments where complex data dependencies need to be managed reliably and where workflow logic changes frequently.</p>
</blockquote>

<p><br /></p>

<h3 id="core-concepts-and-components">Core Concepts and Components</h3>

<div style="width: 100%; margin: 20px auto;">
  <div class="mermaid">
    graph TD
      A[DAG Definition] --&gt; |Parsed by| B[Scheduler]
      B --&gt; |Creates| C[Task Instances]
      C --&gt; |Queued for| D[Executor]
      D --&gt; |Executes on| E[Workers]
      E --&gt; |Updates Status| F[Metadata Database]
      B --&gt; |Reads from| F
      G[Web Server] --&gt; |Queries| F
      G --&gt; |Displays| H[Web UI]
      I[Operators] --&gt; |Define| C
      J[Hooks] --&gt; |Used by| I
      
      style A fill:#f5f5f5,stroke:#333,stroke-width:1px
      style B fill:#a5d6a7,stroke:#333,stroke-width:1px
      style C fill:#64b5f6,stroke:#333,stroke-width:1px
      style D fill:#ffcc80,stroke:#333,stroke-width:1px
      style E fill:#ce93d8,stroke:#333,stroke-width:1px
      style F fill:#ef9a9a,stroke:#333,stroke-width:1px
      style G fill:#9fa8da,stroke:#333,stroke-width:1px
      style H fill:#f5f5f5,stroke:#333,stroke-width:1px
      style I fill:#81c784,stroke:#333,stroke-width:1px
      style J fill:#ffb74d,stroke:#333,stroke-width:1px
  </div>
</div>

<p><br /></p>

<div class="table-container">
  <table class="table-beauty">
    <tr>
      <th style="width: 20%;">Component</th>
      <th style="width: 80%;">Description</th>
    </tr>
    <tr>
      <td><strong>DAGs (Directed Acyclic Graphs)</strong></td>
      <td>
        <ul>
          <li>Core concept representing a collection of tasks to execute</li>
          <li>Organized to reflect relationships and dependencies</li>
          <li>Acyclic graph with directed edges, preventing infinite loops</li>
          <li>Defined as Python code for maximum flexibility</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Tasks</strong></td>
      <td>
        <ul>
          <li>Individual units of work within a DAG</li>
          <li>Instances of operators that define specific actions</li>
          <li>Can execute Python functions, SQL commands, or system operations</li>
          <li>Have defined dependencies and execution order</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Operators</strong></td>
      <td>
        <ul>
          <li>Define the actual work performed by tasks</li>
          <li>BashOperator: Execute bash commands</li>
          <li>PythonOperator: Run Python functions</li>
          <li>KubernetesPodOperator: Run containers in Kubernetes</li>
          <li>Database operators: PostgresOperator, MySQLOperator, etc.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Scheduler</strong></td>
      <td>
        <ul>
          <li>Monitors all tasks and DAGs</li>
          <li>Triggers task instances when dependencies are met</li>
          <li>Determines what to execute and when</li>
          <li>Handles retry logic and failure scenarios</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Executors</strong></td>
      <td>
        <ul>
          <li>Mechanism that actually runs tasks</li>
          <li>LocalExecutor: Single-machine parallel execution</li>
          <li>CeleryExecutor: Distributed execution using Celery</li>
          <li>KubernetesExecutor: Container-based execution in Kubernetes</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Hooks</strong></td>
      <td>
        <ul>
          <li>Interfaces to external platforms and databases</li>
          <li>Provide connection management and authentication</li>
          <li>Support for MySQL, PostgreSQL, AWS, GCP, Azure, etc.</li>
          <li>Used by operators to interact with external systems</li>
        </ul>
      </td>
    </tr>
  </table>
</div>

<p><br /></p>

<h3 id="airflow-architecture-diagram">Airflow Architecture Diagram</h3>

<div style="width: 100%; margin: 20px auto;">
  <div class="mermaid">
    graph TD
      A[DAG Files] --&gt; |Parsed by| B[Scheduler]
      B --&gt; |Schedules Tasks| C[Executor]
      C --&gt; |Executes| D[Workers]
      D --&gt; |Updates Status| E[Metadata Database]
      F[Web Server] --&gt; |Queries| E
      F --&gt; |Serves| G[Web UI]
      B --&gt; |Reads/Writes| E
      H[Users] --&gt; |Interact with| G
      I[External Systems] --&gt; |Connected via| J[Hooks]
      D --&gt; |Uses| J
      
      style A fill:#f5f5f5,stroke:#333,stroke-width:1px
      style B fill:#a5d6a7,stroke:#333,stroke-width:1px
      style C fill:#64b5f6,stroke:#333,stroke-width:1px
      style D fill:#ffcc80,stroke:#333,stroke-width:1px
      style E fill:#ef9a9a,stroke:#333,stroke-width:1px
      style F fill:#ce93d8,stroke:#333,stroke-width:1px
      style G fill:#9fa8da,stroke:#333,stroke-width:1px
      style H fill:#f5f5f5,stroke:#333,stroke-width:1px
      style I fill:#81c784,stroke:#333,stroke-width:1px
      style J fill:#ffb74d,stroke:#333,stroke-width:1px
  </div>
</div>

<div class="info-box info-box-default-not-check">
  <strong>Architecture Components</strong>
  <ul>
    <li><strong>Scheduler</strong>: Reads DAG files and schedules task execution</li>
    <li><strong>Web Server</strong>: Provides UI for monitoring, triggering, and viewing history</li>
    <li><strong>Executor</strong>: Handles actual task execution (Local, Celery, Kubernetes, etc.)</li>
    <li><strong>Metadata Database</strong>: Stores state information (task success/failure, DAG history)</li>
    <li><strong>Workers</strong>: Execution units used by the executor (especially important in CeleryExecutor)</li>
  </ul>
</div>

<p><br /></p>

<hr />

<h2 id="apache-airflow-vs-argo-workflow">Apache Airflow vs Argo Workflow</h2>

<p>Understanding the differences between Airflow and Argo Workflow helps in choosing the right tool for your environment and use case.</p>

<p><br /></p>

<div class="table-container">
  <table class="table-beauty">
    <tr>
      <th style="width: 25%;">Feature</th>
      <th style="width: 37.5%;">Apache Airflow</th>
      <th style="width: 37.5%;">Argo Workflow</th>
    </tr>
    <tr>
      <td><strong>Workflow Definition</strong></td>
      <td>Python scripts enabling complex logic and integrations</td>
      <td>YAML definitions with direct Kubernetes resource integration</td>
    </tr>
    <tr>
      <td><strong>DAG Support</strong></td>
      <td>Native support for DAGs to manage task dependencies and orchestration</td>
      <td>Supports DAGs for managing dependencies and execution order within Kubernetes</td>
    </tr>
    <tr>
      <td><strong>Execution Environment</strong></td>
      <td>Runs on standalone servers or clusters, typically managed with Celery, Kubernetes, etc.</td>
      <td>Runs natively in Kubernetes, executing workflow steps using Pods</td>
    </tr>
    <tr>
      <td><strong>Scalability</strong></td>
      <td>Scalable through executors like Celery, Kubernetes. Tasks scale based on worker availability</td>
      <td>Highly scalable due to Kubernetes integration with dynamic Pod allocation</td>
    </tr>
    <tr>
      <td><strong>User Interface</strong></td>
      <td>Rich UI for workflow monitoring, retries, and visualization</td>
      <td>Simplified UI primarily for visualizing workflows and managing Kubernetes objects directly</td>
    </tr>
    <tr>
      <td><strong>Community &amp; Support</strong></td>
      <td>Extensive community support with various plugins and third-party tools</td>
      <td>Growing community with Kubernetes-based support and integrations</td>
    </tr>
  </table>
</div>

<div class="info-box info-box-success-not-check">
  <strong>Choosing Between Airflow and Argo Workflow</strong>
  <ul>
    <li><strong>Apache Airflow</strong> is better suited for complex data pipeline orchestration where you need programmatic task definition and management using Python</li>
    <li><strong>Argo Workflows</strong> excels in containerized Kubernetes environments and is ideal for DevOps and MLOps pipelines where Kubernetes is already in use</li>
  </ul>
</div>

<p><br /></p>

<hr />

<h2 id="executor-selection-guide">Executor Selection Guide</h2>

<p>Choosing the right executor is crucial for your Airflow deployment’s performance and scalability.</p>

<p><br /></p>

<div class="table-container">
  <table class="table-beauty">
    <tr>
      <th style="width: 25%;">Executor Type</th>
      <th style="width: 45%;">Characteristics</th>
      <th style="width: 30%;">Recommended For</th>
    </tr>
    <tr>
      <td><strong>SequentialExecutor</strong></td>
      <td>Default setting. Executes only one task at a time</td>
      <td>Testing purposes, local environments</td>
    </tr>
    <tr>
      <td><strong>LocalExecutor</strong></td>
      <td>Supports parallel execution. Based on multiprocessing</td>
      <td>Small-scale workflows, single-machine deployments</td>
    </tr>
    <tr>
      <td><strong>CeleryExecutor</strong></td>
      <td>Supports distributed environments. Workers consume tasks from queue</td>
      <td>Complex workflows where parallelism is important</td>
    </tr>
    <tr>
      <td><strong>KubernetesExecutor</strong></td>
      <td>Executes tasks as Pods. Fully distributed environment</td>
      <td>Cloud environments, Kubernetes-native deployments</td>
    </tr>
  </table>
</div>

<p><br /></p>

<hr />

<h2 id="installation-guide">Installation Guide</h2>

<p>This section provides practical examples for installing Airflow in different environments.</p>

<p><br /></p>

<h3 id="installation-1-localexecutor-with-docker">Installation 1: LocalExecutor with Docker</h3>

<p>This example demonstrates running Airflow with LocalExecutor using Docker Compose.</p>

<h4 id="prerequisites">Prerequisites</h4>
<ul>
  <li>Docker</li>
  <li>Docker Compose</li>
</ul>

<h4 id="directory-structure">Directory Structure</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>airflow-local/
├── docker-compose.yaml
└── dags/
    └── sample_dag.py
</code></pre></div></div>

<h4 id="docker-compose-configuration">Docker Compose Configuration</h4>

<div class="info-box info-box-default-not-check">
  <strong>docker-compose.yaml</strong>
</div>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s1">'</span><span class="s">3'</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">postgres</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">postgres:13</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="na">POSTGRES_USER</span><span class="pi">:</span> <span class="s">airflow</span>
      <span class="na">POSTGRES_PASSWORD</span><span class="pi">:</span> <span class="s">airflow</span>
      <span class="na">POSTGRES_DB</span><span class="pi">:</span> <span class="s">airflow</span>

  <span class="na">airflow-init</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">apache/airflow:2.8.1</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">postgres</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="na">AIRFLOW__CORE__EXECUTOR</span><span class="pi">:</span> <span class="s">LocalExecutor</span>
      <span class="na">AIRFLOW__CORE__SQL_ALCHEMY_CONN</span><span class="pi">:</span> <span class="s">postgresql+psycopg2://airflow:airflow@postgres/airflow</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">./dags:/opt/airflow/dags</span>
      <span class="pi">-</span> <span class="s">./logs:/opt/airflow/logs</span>
      <span class="pi">-</span> <span class="s">./plugins:/opt/airflow/plugins</span>
    <span class="na">entrypoint</span><span class="pi">:</span> <span class="pi">&gt;</span>
      <span class="s">bash -c "airflow db init &amp;&amp; airflow users create</span>
      <span class="s">--username admin --firstname admin --lastname user --role Admin</span>
      <span class="s">--email admin@email.com --password admin123"</span>

  <span class="na">airflow-webserver</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">apache/airflow:2.8.1</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">airflow-init</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">8080:8080"</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="na">AIRFLOW__CORE__EXECUTOR</span><span class="pi">:</span> <span class="s">LocalExecutor</span>
      <span class="na">AIRFLOW__CORE__SQL_ALCHEMY_CONN</span><span class="pi">:</span> <span class="s">postgresql+psycopg2://airflow:airflow@postgres/airflow</span>
      <span class="na">AIRFLOW__CORE__LOAD_EXAMPLES</span><span class="pi">:</span> <span class="s2">"</span><span class="s">false"</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">./dags:/opt/airflow/dags</span>
      <span class="pi">-</span> <span class="s">./logs:/opt/airflow/logs</span>
      <span class="pi">-</span> <span class="s">./plugins:/opt/airflow/plugins</span>
    <span class="na">command</span><span class="pi">:</span> <span class="s">webserver</span>

  <span class="na">airflow-scheduler</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">apache/airflow:2.8.1</span>
    <span class="na">depends_on</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">airflow-webserver</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="na">AIRFLOW__CORE__EXECUTOR</span><span class="pi">:</span> <span class="s">LocalExecutor</span>
      <span class="na">AIRFLOW__CORE__SQL_ALCHEMY_CONN</span><span class="pi">:</span> <span class="s">postgresql+psycopg2://airflow:airflow@postgres/airflow</span>
    <span class="na">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">./dags:/opt/airflow/dags</span>
      <span class="pi">-</span> <span class="s">./logs:/opt/airflow/logs</span>
      <span class="pi">-</span> <span class="s">./plugins:/opt/airflow/plugins</span>
    <span class="na">command</span><span class="pi">:</span> <span class="s">scheduler</span>
</code></pre></div></div>

<h4 id="sample-dag">Sample DAG</h4>

<h4 id="dagssample_dagpy"><code class="language-plaintext highlighter-rouge">dags/sample_dag.py</code></h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span> <span class="nn">airflow.operators.bash</span> <span class="kn">import</span> <span class="n">BashOperator</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span><span class="s">"sample_dag"</span><span class="p">,</span> <span class="n">start_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2024</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">schedule_interval</span><span class="o">=</span><span class="s">"@daily"</span><span class="p">,</span> <span class="n">catchup</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="s">"hello"</span><span class="p">,</span> <span class="n">bash_command</span><span class="o">=</span><span class="s">"echo 'Hello Airflow'"</span><span class="p">)</span>
    <span class="n">t2</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span><span class="n">task_id</span><span class="o">=</span><span class="s">"bye"</span><span class="p">,</span> <span class="n">bash_command</span><span class="o">=</span><span class="s">"echo 'Bye Airflow'"</span><span class="p">)</span>
    <span class="n">t1</span> <span class="o">&gt;&gt;</span> <span class="n">t2</span>
</code></pre></div></div>

<h4 id="execution-steps">Execution Steps</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 1. Start services with docker-compose</span>
docker-compose up <span class="nt">-d</span>

<span class="c"># 2. Access Web UI</span>
open http://localhost:8080

<span class="c"># 3. Login credentials</span>
<span class="c"># username: admin</span>
<span class="c"># password: admin123</span>

<span class="c"># 4. CLI commands for verification</span>
<span class="c"># Access webserver container</span>
docker <span class="nb">exec</span> <span class="nt">-it</span> airflow-local-airflow-webserver-1 bash

<span class="c"># List DAGs</span>
airflow dags list

<span class="c"># Trigger DAG</span>
airflow dags trigger sample_dag

<span class="c"># Check task status</span>
airflow tasks list sample_dag

<span class="c"># Clean up</span>
docker-compose down
</code></pre></div></div>

<p><br /></p>

<h3 id="installation-2-kubernetesexecutor-with-helm">Installation 2: KubernetesExecutor with Helm</h3>

<p>This example shows how to deploy Airflow in Kubernetes using Helm charts.</p>

<h4 id="prerequisites-1">Prerequisites</h4>
<ul>
  <li>Kubernetes cluster (Minikube, Kind, GKE, etc.)</li>
  <li>Helm installed</li>
  <li>kubectl CLI installed</li>
</ul>

<h4 id="helm-chart-setup">Helm Chart Setup</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add Helm repository</span>
helm repo add apache-airflow https://airflow.apache.org

<span class="c"># Update Helm chart repository</span>
helm repo update

<span class="c"># Install with custom values</span>
helm <span class="nb">install </span>airflow apache-airflow/airflow <span class="nt">-n</span> airflow <span class="nt">--create-namespace</span> <span class="nt">-f</span> airflow-values.yaml
</code></pre></div></div>

<h4 id="sample-values-configuration">Sample Values Configuration</h4>

<h4 id="airflow-valuesyaml"><code class="language-plaintext highlighter-rouge">airflow-values.yaml</code></h4>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">########################################</span>
<span class="c1">## Airflow Basic Configuration</span>
<span class="c1">########################################</span>
<span class="na">airflow</span><span class="pi">:</span>
  <span class="na">image</span><span class="pi">:</span>
    <span class="na">repository</span><span class="pi">:</span> <span class="s">apache/airflow</span>
    <span class="na">tag</span><span class="pi">:</span> <span class="s">2.8.4-python3.9</span>
  <span class="na">executor</span><span class="pi">:</span> <span class="s">KubernetesExecutor</span>
  <span class="na">fernetKey</span><span class="pi">:</span> <span class="s2">"</span><span class="s">$(openssl</span><span class="nv"> </span><span class="s">rand</span><span class="nv"> </span><span class="s">-base64</span><span class="nv"> </span><span class="s">32)"</span>
  <span class="na">webserverSecretKey</span><span class="pi">:</span> <span class="s2">"</span><span class="s">$(openssl</span><span class="nv"> </span><span class="s">rand</span><span class="nv"> </span><span class="s">-hex</span><span class="nv"> </span><span class="s">16)"</span>
  <span class="na">config</span><span class="pi">:</span>
    <span class="na">AIRFLOW__WEBSERVER__EXPOSE_CONFIG</span><span class="pi">:</span> <span class="s2">"</span><span class="s">False"</span>
    <span class="na">AIRFLOW__CORE__LOAD_EXAMPLES</span><span class="pi">:</span> <span class="s2">"</span><span class="s">False"</span>
  <span class="na">users</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">username</span><span class="pi">:</span> <span class="s">admin</span>
      <span class="na">password</span><span class="pi">:</span> <span class="s">admin</span>
      <span class="na">role</span><span class="pi">:</span> <span class="s">Admin</span>
      <span class="na">email</span><span class="pi">:</span> <span class="s">admin@example.com</span>
      <span class="na">firstName</span><span class="pi">:</span> <span class="s">Admin</span>
      <span class="na">lastName</span><span class="pi">:</span> <span class="s">User</span>

<span class="c1">########################################</span>
<span class="c1">## DAG Configuration</span>
<span class="c1">########################################</span>
<span class="na">dags</span><span class="pi">:</span>
  <span class="na">persistence</span><span class="pi">:</span>
    <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">storageClass</span><span class="pi">:</span> <span class="s2">"</span><span class="s">standard"</span>
  <span class="na">gitSync</span><span class="pi">:</span>
    <span class="na">enabled</span><span class="pi">:</span> <span class="no">false</span>

<span class="c1">########################################</span>
<span class="c1">## Webserver Configuration</span>
<span class="c1">########################################</span>
<span class="na">webserver</span><span class="pi">:</span>
  <span class="na">service</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">airflow-ui</span>
        <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
        <span class="na">nodePort</span><span class="pi">:</span> <span class="m">30080</span>

<span class="c1">########################################</span>
<span class="c1">## Scheduler Configuration</span>
<span class="c1">########################################</span>
<span class="na">scheduler</span><span class="pi">:</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>

<span class="c1">########################################</span>
<span class="c1">## Triggerer Configuration</span>
<span class="c1">########################################</span>
<span class="na">triggerer</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>

<span class="c1">########################################</span>
<span class="c1">## PostgreSQL Configuration</span>
<span class="c1">########################################</span>
<span class="na">postgresql</span><span class="pi">:</span>
  <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
  <span class="na">persistence</span><span class="pi">:</span>
    <span class="na">enabled</span><span class="pi">:</span> <span class="no">true</span>
    <span class="na">size</span><span class="pi">:</span> <span class="s">8Gi</span>
    <span class="na">storageClass</span><span class="pi">:</span> <span class="s2">"</span><span class="s">standard"</span>
</code></pre></div></div>

<h4 id="verification-commands">Verification Commands</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check deployment status</span>
kubectl get pods <span class="nt">-n</span> airflow

<span class="c"># Check services</span>
kubectl get svc <span class="nt">-n</span> airflow

<span class="c"># Access Web UI (NodePort)</span>
<span class="c"># http://&lt;node-ip&gt;:30080</span>

<span class="c"># Check logs</span>
kubectl logs <span class="nt">-f</span> deployment/airflow-scheduler <span class="nt">-n</span> airflow

<span class="c"># Clean up</span>
helm uninstall airflow <span class="nt">-n</span> airflow
kubectl delete namespace airflow
</code></pre></div></div>

<p><br /></p>

<h4 id="check-and-login">Check and login.</h4>

<p><img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755572471/airflow-2_y2hhbw.png" alt="airflow-2" /></p>

<h4 id="run-the-dag-i-created">Run the DAG I created.</h4>

<p><img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755572471/airflow-3_yshqli.png" alt="airflow-3" /></p>

<h4 id="graph-can-also-be-checked-as-shown-below">Graph can also be checked as shown below.</h4>

<p><img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1755572470/airflow-4_jxfsn7.png" alt="airflow-4" /></p>

<p><br /></p>

<hr />

<h2 id="troubleshooting-guide">Troubleshooting Guide</h2>

<p>Common issues and their solutions when working with Airflow deployments.</p>

<p><br /></p>

<div class="table-container">
  <table class="table-beauty">
    <tr>
      <th style="width: 30%;">Issue</th>
      <th style="width: 70%;">Solution</th>
    </tr>
    <tr>
      <td><strong>DAGs not appearing</strong></td>
      <td>
        <ul>
          <li>Verify dags folder is properly mounted in volumes</li>
          <li>Check file permissions and ownership</li>
          <li>Restart scheduler: <code>kubectl rollout restart deployment airflow-scheduler -n airflow</code></li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>DAGs not recognized</strong></td>
      <td>
        <ul>
          <li>Check Python syntax errors in DAG files</li>
          <li>Verify imports and dependencies</li>
          <li>Check scheduler logs for parsing errors</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Tasks failing to execute</strong></td>
      <td>
        <ul>
          <li>Check task logs in Web UI → DAG → Task → Log tab</li>
          <li>Verify executor configuration and resources</li>
          <li>Ensure worker pods have necessary permissions</li>
        </ul>
      </td>
    </tr>
    <tr>
      <td><strong>Scheduler not running</strong></td>
      <td>
        <ul>
          <li>Check container/pod status: <code>kubectl get pods -n airflow</code></li>
          <li>Verify database connectivity</li>
          <li>Check scheduler logs for error messages</li>
        </ul>
      </td>
    </tr>
  </table>
</div>

<p><br /></p>

<h3 id="additional-troubleshooting-checklist">Additional Troubleshooting Checklist</h3>

<div class="info-box info-box-default-not-check">
  <strong>Key Verification Points</strong>
  <ul>
    <li><strong>DAG Path &amp; Persistence</strong>: Verify DAG path is correct in Helm values.yaml and PVC is properly mounted</li>
    <li><strong>File Ownership &amp; Permissions</strong>: Ensure airflow user can read DAG files (check chmod, chown)</li>
    <li><strong>Python Syntax Errors</strong>: Check airflow-scheduler pod logs for DAG parsing errors</li>
    <li><strong>GitSync Configuration</strong>: If using GitSync, verify Git repository synchronization</li>
    <li><strong>DAG File Location</strong>: Confirm files are in /opt/airflow/dags path, or modify AIRFLOW__CORE__DAGS_FOLDER if different</li>
  </ul>
</div>

<p><br /></p>

<hr />

<h2 id="implementation-considerations">Implementation Considerations</h2>

<div class="info-box info-box-success-not-check">
  <strong>⚠️ Key Deployment Considerations</strong>
  <p>When implementing an Airflow solution, consider these critical factors:</p>
</div>

<p><br /></p>

<h3 id="resource-planning">Resource Planning</h3>

<p><strong>Airflow Component Resource Requirements:</strong></p>

<h4 id="scheduler">Scheduler</h4>
<ul>
  <li><strong>CPU</strong>: 1-2 cores for small deployments, 4+ cores for large environments</li>
  <li><strong>Memory</strong>: 2-4GB base, scales with DAG complexity and count</li>
  <li><strong>Storage</strong>: Fast storage for metadata database access</li>
</ul>

<h4 id="webserver">Webserver</h4>
<ul>
  <li><strong>CPU</strong>: 1-2 cores, scales with concurrent user sessions</li>
  <li><strong>Memory</strong>: 1-2GB base plus memory for UI operations</li>
  <li><strong>Network</strong>: Consider load balancing for high availability</li>
</ul>

<h4 id="workers-for-celeryexecutor">Workers (for CeleryExecutor)</h4>
<ul>
  <li><strong>CPU</strong>: Varies based on task requirements</li>
  <li><strong>Memory</strong>: Depends on task memory usage patterns</li>
  <li><strong>Autoscaling</strong>: Configure based on queue depth and response time requirements</li>
</ul>

<h3 id="database-considerations">Database Considerations</h3>

<p><strong>Metadata Database Planning:</strong></p>

<h4 id="database-selection">Database Selection</h4>
<ul>
  <li><strong>PostgreSQL</strong>: Recommended for production deployments</li>
  <li><strong>MySQL</strong>: Alternative option with good performance</li>
  <li><strong>SQLite</strong>: Development and testing only</li>
</ul>

<h4 id="performance-optimization">Performance Optimization</h4>
<ul>
  <li><strong>Connection Pooling</strong>: Configure appropriate pool sizes</li>
  <li><strong>Indexing</strong>: Ensure proper indexing on frequently queried tables</li>
  <li><strong>Backup Strategy</strong>: Regular backups with point-in-time recovery capability</li>
</ul>

<p><br /></p>

<hr />

<h2 id="key-points">Key Points</h2>

<div class="info-box info-box-success-not-check">
  <strong>Apache Airflow Summary</strong>
  <ul>
    <li>
      <strong>Core Strengths</strong><br />
      - Programmatic workflow definition using Python<br />
      - Rich ecosystem of operators and hooks<br />
      - Powerful web UI for monitoring and management<br />
      - Scalable execution with multiple executor options
    </li>
    <li>
      <strong>Architecture Benefits</strong><br />
      - Modular design with clear separation of concerns<br />
      - Flexible executor selection for different environments<br />
      - Extensive integration capabilities<br />
      - Strong community support and ecosystem
    </li>
    <li>
      <strong>Implementation Best Practices</strong><br />
      - Choose appropriate executor for your environment<br />
      - Plan for proper resource allocation and scaling<br />
      - Implement robust monitoring and alerting<br />
      - Consider security and access control requirements
    </li>
  </ul>
</div>

<p><br /></p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>Apache Airflow provides a powerful platform for orchestrating complex data workflows with its explicit DAG structure, intuitive UI, and diverse operator ecosystem.</p>

<p>It’s widely used across data engineering, batch processing, ETL, and machine learning pipeline domains.</p>

<p>The flexibility of Python-based DAG definitions and extensive external system integration hooks are particular strengths.</p>

<p>For Kubernetes environments, cloud-native alternatives like Argo Workflows are also worth considering.</p>

<p>Each tool has different advantages depending on use cases and environments, so it’s important to adopt the right tool for your specific needs.</p>

<p>With a proper understanding of Airflow’s concepts and components, you’ll be able to build more systematic data-driven automation and orchestration environments.</p>

<p><br /></p>

<hr />

<h2 id="references">References</h2>
<ul>
  <li><a href="https://airflow.apache.org/docs/apache-airflow/stable/">Apache Airflow Official Documentation</a></li>
  <li><a href="https://github.com/airflow-helm/charts">Airflow Helm Charts</a></li>
  <li><a href="https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/">Kubernetes Operator Documentation</a></li>
  <li><a href="https://coffeewhale.com/kubernetes/workflow/argo/2020/02/14/argo-wf/">Argo Workflows Comparison</a></li>
  <li><a href="https://developnote-blog.tistory.com/176">Airflow Best Practices</a></li>
</ul>


                <!-- Pagination links -->


            </article>

            
                <aside class="see-also">
                    <h2>See also</h2>
                    <ul>
                        
                        
                        
                            <li>
                                <a href="/category/container/docker-compose/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755654072/docker-compose_l2hvau.png">
                                    
                                    <h3>Docker Compose - Complete Guide to Multi-Container Application Orchestration</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/ai/claude-gemini/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1767603840/claude-gemini-1_q0lkop.png">
                                    
                                    <h3>Claude 4.5 Sonnet vs Gemini 3 Pro: The Ultimate 2026 AI Model Showdown</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/iac/ansible/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755657938/ansible-1_qtomxt.png">
                                    
                                    <h3>Ansible - Complete Guide to Infrastructure Automation and Galaxy Platform</h3>
                                </a>
                            </li>
                        
                    </ul>
                </aside>
            

        </section>

        <!-- Add time bar only for pages without pagination -->
        
            <div class="time-bar" data-minutes="14">
    <span class="time-completed"></span>
    <span class="time-remaining"></span>
    <div class="bar">
        <span class="completed" style="width:0%;"></span>
        <span class="remaining" style="width:100%;"></span>
    </div>
</div>

            <button class="toggle-preview" onclick="togglePreview()">
    <span>Hide Preview ▼</span>
</button>

<div id="recommendationSection" class="recommendation">
    <div class="message">
        <strong>Why don't you read something next?</strong>
        <div>
            <button>
                <svg><use xlink:href="#icon-arrow-right"></use></svg>
                <span>Go back to top</span>
            </button>
        </div>
    </div>
    <div id="previewSection" class="preview-section">
        
        <a href="/category/virtualization/libvirt-virsh/" class="post-preview">
            <div class="image">
                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1755587965/libvirt-virsh_hb18ke.png">
                
            </div>
            <h3 class="title">Libvirt Complete Guide - Linux Virtualization Management Tool</h3>
        </a>
    </div>
</div>

<style>
.toggle-preview {
    position: fixed;
    bottom: 20px;
    right: 20px;
    background: #333;
    color: white;
    border: none;
    padding: 8px 15px;
    border-radius: 4px;
    cursor: pointer;
    z-index: 1000;
    opacity: 0;
    transition: opacity 0.3s ease;
}

.toggle-preview:hover {
    background: #444;
}

.toggle-preview.visible {
    opacity: 1;
}

.recommendation {
    margin-top: 1000px;
    display: block;
    transition: all 0.3s ease;
}

.recommendation.hidden {
    display: none;
}

.hide-preview {
    margin-left: 10px;
    background: none;
    border: 1px solid #666;
    color: #666;
    padding: 5px 10px;
    border-radius: 4px;
    cursor: pointer;
}

.hide-preview:hover {
    background: #f0f0f0;
}

.preview-section {
    max-height: 1000px;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
}

.preview-section.hidden {
    max-height: 0;
}
</style>

<script>
function togglePreview() {
    const recommendation = document.getElementById('recommendationSection');
    const button = document.querySelector('.toggle-preview span');
    
    if (recommendation.classList.contains('hidden')) {
        recommendation.classList.remove('hidden');
        button.textContent = 'Hide Preview ▼';
    } else {
        recommendation.classList.add('hidden');
        button.textContent = 'Show Preview ▲';
    }
}

window.addEventListener('scroll', function() {
    const toggleButton = document.querySelector('.toggle-preview');
    const recommendation = document.getElementById('recommendationSection');
    const rect = recommendation.getBoundingClientRect();
    
    if (rect.top <= window.innerHeight) {
        toggleButton.classList.add('visible');
    } else {
        toggleButton.classList.remove('visible');
    }
});
</script>

        

        <!-- Show modal if the post is the last one -->
        

        <!-- Show modal before user leaves the page -->
        

        <!-- Add your newsletter subscription form here -->

        <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;Learn about Apache Airflow workflow orchestration platform, its core concepts, architecture, and how to install it using Docker and Kubernetes&quot;%20https://somaz.blog/category/data-engineering/airflow/%20via%20&#64;twitter_username&hashtags=airflow,data-engineering,workflow-orchestration,kubernetes,docker"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://somaz.blog/category/data-engineering/airflow/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

        

  <section class="author">
    <div class="details">
      
        <img class="img-rounded" src="/assets/img/uploads/profile.png" alt="Somaz">
      
      <p class="def">Author</p>
      <h3 class="name">
        <a href="/authors/somaz/">Somaz</a>
      </h3>
      <p class="desc">DevOps engineer focused on cloud infrastructure and automation</p>
      <p>
        
          <a href="https://github.com/somaz94" title="Github">
            <svg><use xlink:href="#icon-github"></use></svg>
          </a>
        
        
        
        
        
        
          <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
            <svg><use xlink:href="#icon-linkedin"></use></svg>
          </a>
        
        
          <a href="https://somaz.tistory.com" title="Tistory">
            <svg><use xlink:href="#icon-tistory"></use></svg>
          </a>
        
      </p>
    </div>
  </section>

  
  
  
  
  
  
  
  

  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Somaz",
      
      "image": "/assets/img/uploads/profile.png",
      
      "jobTitle": "DevOps Engineer",
      "url": "https://somaz.blog/authors/somaz/",
      "sameAs": [
        "https://github.com/somaz94","https://www.linkedin.com/in/somaz","https://{{ author.tistory_username }}.tistory.com"
      ]
  }
  </script>


        

<section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'https-somaz94-github-io';
        var disqus_title = '';
        var disqus_url = '/category/data-engineering/airflow/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>



        <footer>
    <p>
      
        <a href="https://github.com/somaz94" title="Github">
          <svg><use xlink:href="#icon-github"></use></svg>
        </a>
      
      
        <a href="https://www.facebook.com/facebook_username" title="Facebook">
          <svg><use xlink:href="#icon-facebook"></use></svg>
        </a>
      
      
        <a href="https://twitter.com/twitter_username" title="Twitter">
          <svg><use xlink:href="#icon-twitter"></use></svg>
        </a>
      
      
        <a href="https://medium.com/@medium_username" title="Medium">
          <svg><use xlink:href="#icon-medium"></use></svg>
        </a>
      
      
        <a href="https://www.instagram.com/instagram_username" title="Instagram">
          <svg><use xlink:href="#icon-instagram"></use></svg>
        </a>
      
      
        <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
          <svg><use xlink:href="#icon-linkedin"></use></svg>
        </a>
      
      
        <a href="https://somaz.tistory.com" title="Tistory">
          <svg><use xlink:href="#icon-tistory"></use></svg>
        </a>
      
    </p>

    <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>


    <p>
      <a href="https://somaz.blog/sitemap.xml" title="sitemap">Sitemap</a> |
      <a href="https://somaz.blog/privacy-policy" title="Privacy Policy">Privacy Policy</a>
    </p>

    <p>
      <span>Somaz Tech Blog</span> <svg class="love"><use xlink:href="#icon-heart"></use></svg>
    </p>
</footer>










<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "somaz",
  "description": "DevOps engineer's tech blog.",
  "url": "https://somaz.blog/",
  "logo": {
      "@type": "ImageObject",
      "url": "https://somaz.blog/assets/img/icons/mediumtile.png",
      "width": "600",
      "height": "315"
  },
  "sameAs": [
    "https://github.com/somaz94","https://www.facebook.com/facebook_username","https://twitter.com/twitter_username","https://medium.com/@medium_username","https://www.instagram.com/instagram_username","https://www.linkedin.com/in/somaz","https://{{ site.tistory_username }}.tistory.com"
  ]
}
</script>

<!-- Include the script that allows Netlify CMS login -->
<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>

<!-- Include the website scripts -->
<script src="/assets/js/scripts.min.js"></script>

<!-- Include Google Analytics script -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  var host = window.location.hostname;
  if (host != 'localhost') {
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-XXXXXXXX-X');
  }
</script>
  


<!-- Include extra scripts -->



        

        
        
        
        
        
        
        
        
        <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "BlogPosting",
            "name": "What is Apache Airflow and How to Install It?",
            "headline": "A comprehensive guide to Apache Airflow concepts and installation",
            "description": "Learn about Apache Airflow workflow orchestration platform, its core concepts, architecture, and how to install it using Docker and Kubernetes",
            "image": "https://res.cloudinary.com/dkcm26aem/image/upload/v1767852742/airflow-1_k0rziq.png",
            "url": "https://somaz.blog/category/data-engineering/airflow/",
            "articleBody": "



Overview

Today we’ll explore Apache Airflow, a powerful data pipeline and workflow orchestration tool.

Airflow is an essential tool in data engineering, DevOps, and MLOps that helps manage complex task dependencies and automated execution.

Originally developed at Airbnb and now maintained by the Apache Software Foundation,

Airflow offers various operators and extensibility, making it flexible for use across diverse cloud and on-premises environments.

In this post, we’ll understand Airflow’s core concepts and components, compare it with Kubernetes-based orchestration tool Argo Workflow, and learn how to install Airflow in Docker and Kubernetes environments.





What is Apache Airflow?

Apache Airflow is an open-source tool for orchestrating complex computational workflows and data processing pipelines.

Started by Airbnb in 2014 and became part of the Apache Software Foundation in 2016, Airflow is used to manage task execution and ensure they run in the correct order within specified workflows.



Introduction to Apache Airflow

Airflow provides a platform to programmatically author, schedule, and monitor workflows, making it particularly valuable for data engineering teams managing complex ETL processes and data pipelines.

Key design principles:

  Programmatic Workflow Definition: Define workflows as code using Python
  Dynamic Pipeline Generation: Create workflows dynamically based on configuration
  Extensible Architecture: Rich ecosystem of operators and hooks
  Rich User Interface: Web-based UI for monitoring and managing workflows
  Scalable Execution: Support for various execution environments



  Airflow excels in environments where complex data dependencies need to be managed reliably and where workflow logic changes frequently.




Core Concepts and Components


  
    graph TD
      A[DAG Definition] --&amp;gt; |Parsed by| B[Scheduler]
      B --&amp;gt; |Creates| C[Task Instances]
      C --&amp;gt; |Queued for| D[Executor]
      D --&amp;gt; |Executes on| E[Workers]
      E --&amp;gt; |Updates Status| F[Metadata Database]
      B --&amp;gt; |Reads from| F
      G[Web Server] --&amp;gt; |Queries| F
      G --&amp;gt; |Displays| H[Web UI]
      I[Operators] --&amp;gt; |Define| C
      J[Hooks] --&amp;gt; |Used by| I
      
      style A fill:#f5f5f5,stroke:#333,stroke-width:1px
      style B fill:#a5d6a7,stroke:#333,stroke-width:1px
      style C fill:#64b5f6,stroke:#333,stroke-width:1px
      style D fill:#ffcc80,stroke:#333,stroke-width:1px
      style E fill:#ce93d8,stroke:#333,stroke-width:1px
      style F fill:#ef9a9a,stroke:#333,stroke-width:1px
      style G fill:#9fa8da,stroke:#333,stroke-width:1px
      style H fill:#f5f5f5,stroke:#333,stroke-width:1px
      style I fill:#81c784,stroke:#333,stroke-width:1px
      style J fill:#ffb74d,stroke:#333,stroke-width:1px
  





  
    
      Component
      Description
    
    
      DAGs (Directed Acyclic Graphs)
      
        
          Core concept representing a collection of tasks to execute
          Organized to reflect relationships and dependencies
          Acyclic graph with directed edges, preventing infinite loops
          Defined as Python code for maximum flexibility
        
      
    
    
      Tasks
      
        
          Individual units of work within a DAG
          Instances of operators that define specific actions
          Can execute Python functions, SQL commands, or system operations
          Have defined dependencies and execution order
        
      
    
    
      Operators
      
        
          Define the actual work performed by tasks
          BashOperator: Execute bash commands
          PythonOperator: Run Python functions
          KubernetesPodOperator: Run containers in Kubernetes
          Database operators: PostgresOperator, MySQLOperator, etc.
        
      
    
    
      Scheduler
      
        
          Monitors all tasks and DAGs
          Triggers task instances when dependencies are met
          Determines what to execute and when
          Handles retry logic and failure scenarios
        
      
    
    
      Executors
      
        
          Mechanism that actually runs tasks
          LocalExecutor: Single-machine parallel execution
          CeleryExecutor: Distributed execution using Celery
          KubernetesExecutor: Container-based execution in Kubernetes
        
      
    
    
      Hooks
      
        
          Interfaces to external platforms and databases
          Provide connection management and authentication
          Support for MySQL, PostgreSQL, AWS, GCP, Azure, etc.
          Used by operators to interact with external systems
        
      
    
  




Airflow Architecture Diagram


  
    graph TD
      A[DAG Files] --&amp;gt; |Parsed by| B[Scheduler]
      B --&amp;gt; |Schedules Tasks| C[Executor]
      C --&amp;gt; |Executes| D[Workers]
      D --&amp;gt; |Updates Status| E[Metadata Database]
      F[Web Server] --&amp;gt; |Queries| E
      F --&amp;gt; |Serves| G[Web UI]
      B --&amp;gt; |Reads/Writes| E
      H[Users] --&amp;gt; |Interact with| G
      I[External Systems] --&amp;gt; |Connected via| J[Hooks]
      D --&amp;gt; |Uses| J
      
      style A fill:#f5f5f5,stroke:#333,stroke-width:1px
      style B fill:#a5d6a7,stroke:#333,stroke-width:1px
      style C fill:#64b5f6,stroke:#333,stroke-width:1px
      style D fill:#ffcc80,stroke:#333,stroke-width:1px
      style E fill:#ef9a9a,stroke:#333,stroke-width:1px
      style F fill:#ce93d8,stroke:#333,stroke-width:1px
      style G fill:#9fa8da,stroke:#333,stroke-width:1px
      style H fill:#f5f5f5,stroke:#333,stroke-width:1px
      style I fill:#81c784,stroke:#333,stroke-width:1px
      style J fill:#ffb74d,stroke:#333,stroke-width:1px
  



  Architecture Components
  
    Scheduler: Reads DAG files and schedules task execution
    Web Server: Provides UI for monitoring, triggering, and viewing history
    Executor: Handles actual task execution (Local, Celery, Kubernetes, etc.)
    Metadata Database: Stores state information (task success/failure, DAG history)
    Workers: Execution units used by the executor (especially important in CeleryExecutor)
  






Apache Airflow vs Argo Workflow

Understanding the differences between Airflow and Argo Workflow helps in choosing the right tool for your environment and use case.




  
    
      Feature
      Apache Airflow
      Argo Workflow
    
    
      Workflow Definition
      Python scripts enabling complex logic and integrations
      YAML definitions with direct Kubernetes resource integration
    
    
      DAG Support
      Native support for DAGs to manage task dependencies and orchestration
      Supports DAGs for managing dependencies and execution order within Kubernetes
    
    
      Execution Environment
      Runs on standalone servers or clusters, typically managed with Celery, Kubernetes, etc.
      Runs natively in Kubernetes, executing workflow steps using Pods
    
    
      Scalability
      Scalable through executors like Celery, Kubernetes. Tasks scale based on worker availability
      Highly scalable due to Kubernetes integration with dynamic Pod allocation
    
    
      User Interface
      Rich UI for workflow monitoring, retries, and visualization
      Simplified UI primarily for visualizing workflows and managing Kubernetes objects directly
    
    
      Community &amp;amp; Support
      Extensive community support with various plugins and third-party tools
      Growing community with Kubernetes-based support and integrations
    
  



  Choosing Between Airflow and Argo Workflow
  
    Apache Airflow is better suited for complex data pipeline orchestration where you need programmatic task definition and management using Python
    Argo Workflows excels in containerized Kubernetes environments and is ideal for DevOps and MLOps pipelines where Kubernetes is already in use
  






Executor Selection Guide

Choosing the right executor is crucial for your Airflow deployment’s performance and scalability.




  
    
      Executor Type
      Characteristics
      Recommended For
    
    
      SequentialExecutor
      Default setting. Executes only one task at a time
      Testing purposes, local environments
    
    
      LocalExecutor
      Supports parallel execution. Based on multiprocessing
      Small-scale workflows, single-machine deployments
    
    
      CeleryExecutor
      Supports distributed environments. Workers consume tasks from queue
      Complex workflows where parallelism is important
    
    
      KubernetesExecutor
      Executes tasks as Pods. Fully distributed environment
      Cloud environments, Kubernetes-native deployments
    
  






Installation Guide

This section provides practical examples for installing Airflow in different environments.



Installation 1: LocalExecutor with Docker

This example demonstrates running Airflow with LocalExecutor using Docker Compose.

Prerequisites

  Docker
  Docker Compose


Directory Structure
airflow-local/
├── docker-compose.yaml
└── dags/
    └── sample_dag.py


Docker Compose Configuration


  docker-compose.yaml


version: &apos;3&apos;
services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow

  airflow-init:
    image: apache/airflow:2.8.1
    depends_on:
      - postgres
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    entrypoint: &amp;gt;
      bash -c &quot;airflow db init &amp;amp;&amp;amp; airflow users create
      --username admin --firstname admin --lastname user --role Admin
      --email admin@email.com --password admin123&quot;

  airflow-webserver:
    image: apache/airflow:2.8.1
    depends_on:
      - airflow-init
    ports:
      - &quot;8080:8080&quot;
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: &quot;false&quot;
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.8.1
    depends_on:
      - airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    command: scheduler


Sample DAG

dags/sample_dag.py

from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG(&quot;sample_dag&quot;, start_date=datetime(2024, 1, 1), schedule_interval=&quot;@daily&quot;, catchup=False) as dag:
    t1 = BashOperator(task_id=&quot;hello&quot;, bash_command=&quot;echo &apos;Hello Airflow&apos;&quot;)
    t2 = BashOperator(task_id=&quot;bye&quot;, bash_command=&quot;echo &apos;Bye Airflow&apos;&quot;)
    t1 &amp;gt;&amp;gt; t2


Execution Steps

# 1. Start services with docker-compose
docker-compose up -d

# 2. Access Web UI
open http://localhost:8080

# 3. Login credentials
# username: admin
# password: admin123

# 4. CLI commands for verification
# Access webserver container
docker exec -it airflow-local-airflow-webserver-1 bash

# List DAGs
airflow dags list

# Trigger DAG
airflow dags trigger sample_dag

# Check task status
airflow tasks list sample_dag

# Clean up
docker-compose down




Installation 2: KubernetesExecutor with Helm

This example shows how to deploy Airflow in Kubernetes using Helm charts.

Prerequisites

  Kubernetes cluster (Minikube, Kind, GKE, etc.)
  Helm installed
  kubectl CLI installed


Helm Chart Setup

# Add Helm repository
helm repo add apache-airflow https://airflow.apache.org

# Update Helm chart repository
helm repo update

# Install with custom values
helm install airflow apache-airflow/airflow -n airflow --create-namespace -f airflow-values.yaml


Sample Values Configuration

airflow-values.yaml

########################################
## Airflow Basic Configuration
########################################
airflow:
  image:
    repository: apache/airflow
    tag: 2.8.4-python3.9
  executor: KubernetesExecutor
  fernetKey: &quot;$(openssl rand -base64 32)&quot;
  webserverSecretKey: &quot;$(openssl rand -hex 16)&quot;
  config:
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: &quot;False&quot;
    AIRFLOW__CORE__LOAD_EXAMPLES: &quot;False&quot;
  users:
    - username: admin
      password: admin
      role: Admin
      email: admin@example.com
      firstName: Admin
      lastName: User

########################################
## DAG Configuration
########################################
dags:
  persistence:
    enabled: true
    storageClass: &quot;standard&quot;
  gitSync:
    enabled: false

########################################
## Webserver Configuration
########################################
webserver:
  service:
    type: NodePort
    ports:
      - name: airflow-ui
        port: 8080
        targetPort: 8080
        nodePort: 30080

########################################
## Scheduler Configuration
########################################
scheduler:
  replicas: 1

########################################
## Triggerer Configuration
########################################
triggerer:
  enabled: true
  replicas: 1

########################################
## PostgreSQL Configuration
########################################
postgresql:
  enabled: true
  persistence:
    enabled: true
    size: 8Gi
    storageClass: &quot;standard&quot;


Verification Commands

# Check deployment status
kubectl get pods -n airflow

# Check services
kubectl get svc -n airflow

# Access Web UI (NodePort)
# http://&amp;lt;node-ip&amp;gt;:30080

# Check logs
kubectl logs -f deployment/airflow-scheduler -n airflow

# Clean up
helm uninstall airflow -n airflow
kubectl delete namespace airflow




Check and login.



Run the DAG I created.



Graph can also be checked as shown below.







Troubleshooting Guide

Common issues and their solutions when working with Airflow deployments.




  
    
      Issue
      Solution
    
    
      DAGs not appearing
      
        
          Verify dags folder is properly mounted in volumes
          Check file permissions and ownership
          Restart scheduler: kubectl rollout restart deployment airflow-scheduler -n airflow
        
      
    
    
      DAGs not recognized
      
        
          Check Python syntax errors in DAG files
          Verify imports and dependencies
          Check scheduler logs for parsing errors
        
      
    
    
      Tasks failing to execute
      
        
          Check task logs in Web UI → DAG → Task → Log tab
          Verify executor configuration and resources
          Ensure worker pods have necessary permissions
        
      
    
    
      Scheduler not running
      
        
          Check container/pod status: kubectl get pods -n airflow
          Verify database connectivity
          Check scheduler logs for error messages
        
      
    
  




Additional Troubleshooting Checklist


  Key Verification Points
  
    DAG Path &amp;amp; Persistence: Verify DAG path is correct in Helm values.yaml and PVC is properly mounted
    File Ownership &amp;amp; Permissions: Ensure airflow user can read DAG files (check chmod, chown)
    Python Syntax Errors: Check airflow-scheduler pod logs for DAG parsing errors
    GitSync Configuration: If using GitSync, verify Git repository synchronization
    DAG File Location: Confirm files are in /opt/airflow/dags path, or modify AIRFLOW__CORE__DAGS_FOLDER if different
  






Implementation Considerations


  ⚠️ Key Deployment Considerations
  When implementing an Airflow solution, consider these critical factors:




Resource Planning

Airflow Component Resource Requirements:

Scheduler

  CPU: 1-2 cores for small deployments, 4+ cores for large environments
  Memory: 2-4GB base, scales with DAG complexity and count
  Storage: Fast storage for metadata database access


Webserver

  CPU: 1-2 cores, scales with concurrent user sessions
  Memory: 1-2GB base plus memory for UI operations
  Network: Consider load balancing for high availability


Workers (for CeleryExecutor)

  CPU: Varies based on task requirements
  Memory: Depends on task memory usage patterns
  Autoscaling: Configure based on queue depth and response time requirements


Database Considerations

Metadata Database Planning:

Database Selection

  PostgreSQL: Recommended for production deployments
  MySQL: Alternative option with good performance
  SQLite: Development and testing only


Performance Optimization

  Connection Pooling: Configure appropriate pool sizes
  Indexing: Ensure proper indexing on frequently queried tables
  Backup Strategy: Regular backups with point-in-time recovery capability






Key Points


  Apache Airflow Summary
  
    
      Core Strengths
      - Programmatic workflow definition using Python
      - Rich ecosystem of operators and hooks
      - Powerful web UI for monitoring and management
      - Scalable execution with multiple executor options
    
    
      Architecture Benefits
      - Modular design with clear separation of concerns
      - Flexible executor selection for different environments
      - Extensive integration capabilities
      - Strong community support and ecosystem
    
    
      Implementation Best Practices
      - Choose appropriate executor for your environment
      - Plan for proper resource allocation and scaling
      - Implement robust monitoring and alerting
      - Consider security and access control requirements
    
  






Conclusion

Apache Airflow provides a powerful platform for orchestrating complex data workflows with its explicit DAG structure, intuitive UI, and diverse operator ecosystem.

It’s widely used across data engineering, batch processing, ETL, and machine learning pipeline domains.

The flexibility of Python-based DAG definitions and extensive external system integration hooks are particular strengths.

For Kubernetes environments, cloud-native alternatives like Argo Workflows are also worth considering.

Each tool has different advantages depending on use cases and environments, so it’s important to adopt the right tool for your specific needs.

With a proper understanding of Airflow’s concepts and components, you’ll be able to build more systematic data-driven automation and orchestration environments.





References

  Apache Airflow Official Documentation
  Airflow Helm Charts
  Kubernetes Operator Documentation
  Argo Workflows Comparison
  Airflow Best Practices

",
            "wordcount": "2677",
            "inLanguage": "en",
            "dateCreated": "2025-12-02/",
            "datePublished": "2025-12-02/",
            "dateModified": "2025-12-02/",
            "author": {
                "@type": "Person",
                "name": "Somaz",
                
                "image": "/assets/img/uploads/profile.png",
                
                "jobTitle": "DevOps Engineer",
                "url": "https://somaz.blog/authors/somaz/",
                "sameAs": [
                    "https://github.com/somaz94","https://www.linkedin.com/in/somaz"
                ]
            },
            "publisher": {
                "@type": "Organization",
                "name": "somaz",
                "url": "https://somaz.blog/",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://somaz.blog/assets/img/blog-image.png",
                    "width": "600",
                    "height": "315"
                }
            },
            "mainEntityOfPage": "True",
            "genre": "DATA-ENGINEERING",
            "articleSection": "DATA-ENGINEERING",
            "keywords": ["airflow","data-engineering","workflow-orchestration","kubernetes","docker"]
        }
        </script>
    </body>
</html>
