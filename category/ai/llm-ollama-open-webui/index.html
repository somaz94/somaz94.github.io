<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    

    
    

    
    

    
    

    <!-- ✅ Google Tag Manager 추가 -->
    <script>
        (function(w,d,s,l,i){
            w[l]=w[l]||[];
            w[l].push({'gtm.start': new Date().getTime(),event:'gtm.js'});
            var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';
            j.async=true;
            j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;
            f.parentNode.insertBefore(j,f);
        })(window,document,'script','dataLayer','GTM-MBP83N4Q');
    </script>
      <!-- ✅ End Google Tag Manager -->

    <!-- Mermaid.js 직접 로드 -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>

    <title>Building an On-Premises LLM System with Ollama + Open WebUI | somaz</title>
    <meta name="description" content="Comprehensive tutorial for DevOps engineers on building a secure, cost-effective on-premises LLM system using Ollama and Open WebUI with Docker Compose, NFS ...">
    
        <meta name="keywords" content="ollama, open-webui, llm, on-premises-ai, docker-compose, nvidia-gpu, nfs-storage, devops, infrastructure">
    

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Building an On-Premises LLM System with Ollama + Open WebUI | somaz">
    <meta name="twitter:description" content="Comprehensive tutorial for DevOps engineers on building a secure, cost-effective on-premises LLM system using Ollama and Open WebUI with Docker Compose, NFS ...">

    
        <meta property="twitter:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1770607787/ollama-openwebui-llm-1_dl6rlu.png">
    
    
    
        <meta name="twitter:site" content="@twitter_username">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="https://somaz.blog/category/ai/llm-ollama-open-webui/">
    <meta property="og:title" content="Building an On-Premises LLM System with Ollama + Open WebUI | somaz">
    <meta property="og:image" content="https://res.cloudinary.com/dkcm26aem/image/upload/v1770607787/ollama-openwebui-llm-1_dl6rlu.png">
    <meta property="og:description" content="Comprehensive tutorial for DevOps engineers on building a secure, cost-effective on-premises LLM system using Ollama and Open WebUI with Docker Compose, NFS ...">
    <meta property="og:site_name" content="Somaz Tech Blog">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="somaz">
    <meta name="msapplication-TileColor" content="#141414">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#141414">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:300,400,700" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="canonical" href="https://somaz.blog/category/ai/llm-ollama-open-webui/">
    <link rel="alternate" type="application/rss+xml" title="Somaz Tech Blog" href="https://somaz.blog/feed.xml" />

    <!-- Include extra styles -->
    

    <!-- JavaScript enabled/disabled -->
    <script>
        document.querySelector('html').classList.remove('no-js');
    </script>

    <!-- Google Adsense -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8725590811736154"
        crossorigin="anonymous"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet"> -->
    <!-- <link href="https://cdn.jsdelivr.net/gh/sunn-us/SUIT/fonts/variable/woff2/SUIT-Variable.css" rel="stylesheet"> -->
    <link href="https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@400;500;700&display=swap" rel="stylesheet">
    <!-- <link href="https://fonts.googleapis.com/css2?family=Albert+Sans:wght@400;500;700&display=swap" rel="stylesheet"> -->

    <link rel="sitemap" type="application/xml" title="Sitemap" href="/sitemap.xml" />

</head>
<!-- ✅ Google Tag Manager (noscript) -->
<noscript>
    <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MBP83N4Q"
            height="0" width="0" style="display:none;visibility:hidden">
    </iframe>
</noscript>
<!-- ✅ End Google Tag Manager (noscript) -->
    <body class="has-push-menu">
        





        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 1000 1000"><path d="M969.8,870.3c27,27.7,27,71.8,0,99.1C955.7,983,937.9,990,920,990c-17.9,0-35.7-7-49.7-20.7L500,599L129.6,969.4C115.6,983,97.8,990,79.9,990s-35.7-7-49.7-20.7c-27-27.3-27-71.4,0-99.1L400.9,500L30.3,129.3c-27-27.3-27-71.4,0-99.1c27.3-27,71.8-27,99.4,0L500,400.9L870.4,30.2c27.7-27,71.8-27,99.4,0c27,27.7,27,71.8,0,99.1L599.1,500L969.8,870.3z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-clock" viewBox="0 0 1000 1000"><path d="M500,10C229.8,10,10,229.8,10,500c0,270.2,219.8,490,490,490c270.2,0,490-219.8,490-490C990,229.8,770.2,10,500,10z M500,910.2c-226.2,0-410.2-184-410.2-410.2c0-226.2,184-410.2,410.2-410.2c226.2,0,410.2,184,410.2,410.2C910.2,726.1,726.2,910.2,500,910.2z M753.1,374c8.2,11.9,5.2,28.1-6.6,36.3L509.9,573.7c-4.4,3.1-9.6,4.6-14.8,4.6c-4.1,0-8.3-1-12.1-3c-8.6-4.5-14-13.4-14-23.1V202.5c0-14.4,11.7-26.1,26.1-26.1c14.4,0,26.1,11.7,26.1,26.1v300l195.6-135.1C728.7,359.2,744.9,362.1,753.1,374z"/></symbol><symbol id="icon-calendar" viewBox="0 0 1000 1000"><path d="M920,500v420H80V500H920 M990,430H10v490c0,38.7,31.3,70,70,70h840c38.7,0,70-31.3,70-70V430L990,430z"/><path d="M850,80v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80H360v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80C72.8,80,10,142.7,10,220v140h980V220C990,142.7,927.2,80,850,80z"/><path d="M255,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C290,25.8,274.3,10,255,10z"/><path d="M745,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C780,25.8,764.3,10,745,10z"/></symbol><symbol id="icon-github" viewBox="0 0 12 14"><path d="M6 1q1.633 0 3.012 0.805t2.184 2.184 0.805 3.012q0 1.961-1.145 3.527t-2.957 2.168q-0.211 0.039-0.312-0.055t-0.102-0.234q0-0.023 0.004-0.598t0.004-1.051q0-0.758-0.406-1.109 0.445-0.047 0.801-0.141t0.734-0.305 0.633-0.52 0.414-0.82 0.16-1.176q0-0.93-0.617-1.609 0.289-0.711-0.062-1.594-0.219-0.070-0.633 0.086t-0.719 0.344l-0.297 0.187q-0.727-0.203-1.5-0.203t-1.5 0.203q-0.125-0.086-0.332-0.211t-0.652-0.301-0.664-0.105q-0.352 0.883-0.062 1.594-0.617 0.68-0.617 1.609 0 0.664 0.16 1.172t0.41 0.82 0.629 0.523 0.734 0.305 0.801 0.141q-0.305 0.281-0.383 0.805-0.164 0.078-0.352 0.117t-0.445 0.039-0.512-0.168-0.434-0.488q-0.148-0.25-0.379-0.406t-0.387-0.187l-0.156-0.023q-0.164 0-0.227 0.035t-0.039 0.090 0.070 0.109 0.102 0.094l0.055 0.039q0.172 0.078 0.34 0.297t0.246 0.398l0.078 0.18q0.102 0.297 0.344 0.48t0.523 0.234 0.543 0.055 0.434-0.027l0.18-0.031q0 0.297 0.004 0.691t0.004 0.426q0 0.141-0.102 0.234t-0.312 0.055q-1.812-0.602-2.957-2.168t-1.145-3.527q0-1.633 0.805-3.012t2.184-2.184 3.012-0.805zM2.273 9.617q0.023-0.055-0.055-0.094-0.078-0.023-0.102 0.016-0.023 0.055 0.055 0.094 0.070 0.047 0.102-0.016zM2.516 9.883q0.055-0.039-0.016-0.125-0.078-0.070-0.125-0.023-0.055 0.039 0.016 0.125 0.078 0.078 0.125 0.023zM2.75 10.234q0.070-0.055 0-0.148-0.062-0.102-0.133-0.047-0.070 0.039 0 0.141t0.133 0.055zM3.078 10.562q0.062-0.062-0.031-0.148-0.094-0.094-0.156-0.023-0.070 0.062 0.031 0.148 0.094 0.094 0.156 0.023zM3.523 10.758q0.023-0.086-0.102-0.125-0.117-0.031-0.148 0.055t0.102 0.117q0.117 0.047 0.148-0.047zM4.016 10.797q0-0.102-0.133-0.086-0.125 0-0.125 0.086 0 0.102 0.133 0.086 0.125 0 0.125-0.086zM4.469 10.719q-0.016-0.086-0.141-0.070-0.125 0.023-0.109 0.117t0.141 0.062 0.109-0.109z"></path></symbol><symbol id="icon-medium" viewBox="0 0 1000 1000"><path d="M336.5,240.2v641.5c0,9.1-2.3,16.9-6.8,23.2s-11.2,9.6-20,9.6c-6.2,0-12.2-1.5-18-4.4L37.3,782.7c-7.7-3.6-14.1-9.8-19.4-18.3S10,747.4,10,739V115.5c0-7.3,1.8-13.5,5.5-18.6c3.6-5.1,8.9-7.7,15.9-7.7c5.1,0,13.1,2.7,24.1,8.2l279.5,140C335.9,238.6,336.5,239.5,336.5,240.2L336.5,240.2z M371.5,295.5l292,473.6l-292-145.5V295.5z M990,305.3v576.4c0,9.1-2.6,16.5-7.7,22.1c-5.1,5.7-12,8.5-20.8,8.5s-17.3-2.4-25.7-7.1L694.7,784.9L990,305.3z M988.4,239.7c0,1.1-46.8,77.6-140.3,229.4C754.6,621,699.8,709.8,683.8,735.7L470.5,389l177.2-288.2c6.2-10.2,15.7-15.3,28.4-15.3c5.1,0,9.8,1.1,14.2,3.3l295.9,147.7C987.6,237.1,988.4,238.2,988.4,239.7L988.4,239.7z"/></symbol><symbol id="icon-instagram" viewBox="0 0 489.84 489.84"><path d="M249.62,50.46c65.4,0,73.14.25,99,1.43C372.47,53,385.44,57,394.07,60.32a75.88,75.88,0,0,1,28.16,18.32,75.88,75.88,0,0,1,18.32,28.16c3.35,8.63,7.34,21.6,8.43,45.48,1.18,25.83,1.43,33.57,1.43,99s-0.25,73.14-1.43,99c-1.09,23.88-5.08,36.85-8.43,45.48a81.11,81.11,0,0,1-46.48,46.48c-8.63,3.35-21.6,7.34-45.48,8.43-25.82,1.18-33.57,1.43-99,1.43s-73.15-.25-99-1.43c-23.88-1.09-36.85-5.08-45.48-8.43A75.88,75.88,0,0,1,77,423.86,75.88,75.88,0,0,1,58.69,395.7c-3.35-8.63-7.34-21.6-8.43-45.48-1.18-25.83-1.43-33.57-1.43-99s0.25-73.14,1.43-99c1.09-23.88,5.08-36.85,8.43-45.48A75.88,75.88,0,0,1,77,78.64a75.88,75.88,0,0,1,28.16-18.32c8.63-3.35,21.6-7.34,45.48-8.43,25.83-1.18,33.57-1.43,99-1.43m0-44.13c-66.52,0-74.86.28-101,1.47s-43.87,5.33-59.45,11.38A120.06,120.06,0,0,0,45.81,47.44,120.06,120.06,0,0,0,17.56,90.82C11.5,106.4,7.36,124.2,6.17,150.27s-1.47,34.46-1.47,101,0.28,74.86,1.47,101,5.33,43.87,11.38,59.45a120.06,120.06,0,0,0,28.25,43.38,120.06,120.06,0,0,0,43.38,28.25c15.58,6.05,33.38,10.19,59.45,11.38s34.46,1.47,101,1.47,74.86-.28,101-1.47,43.87-5.33,59.45-11.38a125.24,125.24,0,0,0,71.63-71.63c6.05-15.58,10.19-33.38,11.38-59.45s1.47-34.46,1.47-101-0.28-74.86-1.47-101-5.33-43.87-11.38-59.45a120.06,120.06,0,0,0-28.25-43.38,120.06,120.06,0,0,0-43.38-28.25C394.47,13.13,376.67,9,350.6,7.8s-34.46-1.47-101-1.47h0Z" transform="translate(-4.7 -6.33)" /><path d="M249.62,125.48A125.77,125.77,0,1,0,375.39,251.25,125.77,125.77,0,0,0,249.62,125.48Zm0,207.41a81.64,81.64,0,1,1,81.64-81.64A81.64,81.64,0,0,1,249.62,332.89Z" transform="translate(-4.7 -6.33)"/><circle cx="375.66" cy="114.18" r="29.39" /></symbol><symbol id="icon-linkedin" viewBox="0 0 12 14"><path d="M2.727 4.883v7.742h-2.578v-7.742h2.578zM2.891 2.492q0.008 0.57-0.395 0.953t-1.059 0.383h-0.016q-0.641 0-1.031-0.383t-0.391-0.953q0-0.578 0.402-0.957t1.051-0.379 1.039 0.379 0.398 0.957zM12 8.187v4.437h-2.57v-4.141q0-0.82-0.316-1.285t-0.988-0.465q-0.492 0-0.824 0.27t-0.496 0.668q-0.086 0.234-0.086 0.633v4.32h-2.57q0.016-3.117 0.016-5.055t-0.008-2.313l-0.008-0.375h2.57v1.125h-0.016q0.156-0.25 0.32-0.438t0.441-0.406 0.68-0.34 0.895-0.121q1.336 0 2.148 0.887t0.813 2.598z"></path></symbol><symbol id="icon-heart" viewBox="0 0 34 30"><path d="M17,29.7 L16.4,29.2 C3.5,18.7 0,15 0,9 C0,4 4,0 9,0 C13.1,0 15.4,2.3 17,4.1 C18.6,2.3 20.9,0 25,0 C30,0 34,4 34,9 C34,15 30.5,18.7 17.6,29.2 L17,29.7 Z M9,2 C5.1,2 2,5.1 2,9 C2,14.1 5.2,17.5 17,27.1 C28.8,17.5 32,14.1 32,9 C32,5.1 28.9,2 25,2 C21.5,2 19.6,4.1 18.1,5.8 L17,7.1 L15.9,5.8 C14.4,4.1 12.5,2 9,2 Z" id="Shape"></path></symbol><symbol id="icon-arrow-right" viewBox="0 0 25.452 25.452"><path d="M4.471,24.929v-2.004l12.409-9.788c0.122-0.101,0.195-0.251,0.195-0.411c0-0.156-0.073-0.31-0.195-0.409L4.471,2.526V0.522c0-0.2,0.115-0.384,0.293-0.469c0.18-0.087,0.396-0.066,0.552,0.061l15.47,12.202c0.123,0.1,0.195,0.253,0.195,0.409c0,0.16-0.072,0.311-0.195,0.411L5.316,25.34c-0.155,0.125-0.372,0.147-0.552,0.061C4.586,25.315,4.471,25.13,4.471,24.929z"/></symbol><symbol id="icon-star" viewBox="0 0 48 48"><path fill="currentColor" d="M44,24c0,11.045-8.955,20-20,20S4,35.045,4,24S12.955,4,24,4S44,12.955,44,24z"/><path fill="#ffffff" d="M24,11l3.898,7.898l8.703,1.301l-6.301,6.102l1.5,8.699L24,30.898L16.199,35l1.5-8.699l-6.301-6.102  l8.703-1.301L24,11z"/></symbol><symbol id="icon-read" viewBox="0 0 32 32"><path fill="currentColor" d="M29,4H3C1.343,4,0,5.343,0,7v18c0,1.657,1.343,3,3,3h10c0,0.552,0.448,1,1,1h4c0.552,0,1-0.448,1-1h10  c1.657,0,3-1.343,3-3V7C32,5.343,30.657,4,29,4z M29,5v20H18.708c-0.618,0-1.236,0.146-1.789,0.422l-0.419,0.21V5H29z M15.5,5  v20.632l-0.419-0.21C14.528,25.146,13.91,25,13.292,25H3V5H15.5z M31,25c0,1.103-0.897,2-2,2H18v1h-4v-1H3c-1.103,0-2-0.897-2-2V7  c0-0.737,0.405-1.375,1-1.722V25c0,0.552,0.448,1,1,1h10.292c0.466,0,0.925,0.108,1.342,0.317l0.919,0.46  c0.141,0.07,0.294,0.106,0.447,0.106c0.153,0,0.306-0.035,0.447-0.106l0.919-0.46C17.783,26.108,18.242,26,18.708,26H29  c0.552,0,1-0.448,1-1V5.278C30.595,5.625,31,6.263,31,7V25z M6,12.5C6,12.224,6.224,12,6.5,12h5c0.276,0,0.5,0.224,0.5,0.5  S11.776,13,11.5,13h-5C6.224,13,6,12.776,6,12.5z M6,14.5C6,14.224,6.224,14,6.5,14h5c0.276,0,0.5,0.224,0.5,0.5S11.776,15,11.5,15  h-5C6.224,15,6,14.776,6,14.5z M6,16.5C6,16.224,6.224,16,6.5,16h5c0.276,0,0.5,0.224,0.5,0.5S11.776,17,11.5,17h-5  C6.224,17,6,16.776,6,16.5z M20,12.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,13,25.5,13h-5  C20.224,13,20,12.776,20,12.5z M20,14.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,15,25.5,15h-5  C20.224,15,20,14.776,20,14.5z M20,16.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,17,25.5,17h-5  C20.224,17,20,16.776,20,16.5z"></path></symbol><symbol id="icon-tistory" viewBox="0 0 24 24"><path d="M4 4h16v3h-6v13h-4V7H4V4z"/></symbol></defs></svg>

        <header class="bar-header">
    <a id="menu" role="button">
        <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    </a>
    <h1 class="logo">
        <a href="/">
            
                somaz <span class="version">v3.1.2</span>
            
        </a>
    </h1>
    <a id="search" class="dosearch" role="button">
        <svg class="icon-search"><use xlink:href="#icon-search"></use></svg>
    </a>
    
        <a href="https://github.com/thiagorossener/jekflix-template" class="get-theme" role="button">
            Get this theme!
        </a>
    
</header>

<div id="mask" class="overlay"></div>

<aside class="sidebar" id="sidebar">
    <nav id="navigation">
      <h2>Menu</h2>
      <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>

    </nav>
</aside>

<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>



        <section class="post two-columns">
            <article role="article" class="post-content">
                <p class="post-info">
                    
                        <svg class="icon-calendar" id="date"><use xlink:href="#icon-calendar"></use></svg>
                        <time class="date" datetime="2026-02-09T00:00:00+00:00">
                            


February 9, 2026

                        </time>
                    
                    <svg id="clock" class="icon-clock"><use xlink:href="#icon-clock"></use></svg>
                    <span>24 min to read</span>
                </p>
                <h1 class="post-title">Building an On-Premises LLM System with Ollama + Open WebUI</h1>
                <p class="post-subtitle">Complete guide to deploying enterprise AI infrastructure with NVIDIA GPU acceleration, NFS storage, and Docker orchestration</p>

                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/v1770607787/ollama-openwebui-llm-1_dl6rlu.png" alt="Featured image" class="post-cover">
                

                <!-- Pagination links -->



                <!-- Add your table of contents here -->


                <p><br /></p>

<h2 id="table-of-contents">Table of Contents</h2>

<ol>
  <li><a href="#overview">Overview</a></li>
  <li><a href="#system-architecture--requirements">System Architecture &amp; Requirements</a></li>
  <li><a href="#step-1-nvidia-driver-installation">Step 1: NVIDIA Driver Installation</a></li>
  <li><a href="#step-2-docker--nvidia-container-toolkit">Step 2: Docker &amp; NVIDIA Container Toolkit</a></li>
  <li><a href="#step-3-nfs-storage-configuration">Step 3: NFS Storage Configuration</a></li>
  <li><a href="#step-4-docker-network-setup">Step 4: Docker Network Setup</a></li>
  <li><a href="#step-5-ollama-server-deployment">Step 5: Ollama Server Deployment</a></li>
  <li><a href="#step-6-open-webui-installation">Step 6: Open WebUI Installation</a></li>
  <li><a href="#step-7-api-integration--usage">Step 7: API Integration &amp; Usage</a></li>
  <li><a href="#step-8-monitoring--maintenance">Step 8: Monitoring &amp; Maintenance</a></li>
  <li><a href="#troubleshooting-guide">Troubleshooting Guide</a></li>
  <li><a href="#security-considerations">Security Considerations</a></li>
  <li><a href="#performance-optimization">Performance Optimization</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
  <li><a href="#references">References</a></li>
</ol>

<hr />

<h2 id="overview">Overview</h2>

<p>While commercial LLM services like ChatGPT and Claude have become mainstream, security-conscious enterprise environments often cannot transmit sensitive data to external APIs. Building an on-premises LLM infrastructure enables organizations to leverage AI capabilities while maintaining complete data sovereignty and security control.</p>

<p>This comprehensive guide covers the entire process of deploying a production-ready internal LLM system using Ollama for model serving and Open WebUI for the user interface. We’ll implement NFS-based storage management and Docker Compose orchestration, providing a complete solution ready for immediate enterprise deployment.</p>

<p>The architecture prioritizes security, cost efficiency, and operational simplicity, making it ideal for DevOps engineers and infrastructure teams building AI capabilities for their organizations.</p>

<blockquote>
  <p><strong>Key Benefits</strong>: On-premises deployment eliminates API costs, ensures data privacy, provides unlimited usage, and maintains full infrastructure control while delivering ChatGPT-like user experience to internal teams.</p>
</blockquote>

<p><br /></p>

<hr />

<h2 id="system-architecture--requirements">System Architecture &amp; Requirements</h2>

<p><br /></p>

<h3 id="hardware-environment">Hardware Environment</h3>

<p>This implementation uses the following system specifications:</p>

<p><br /></p>

<h4 id="gpu-server-192168120">GPU Server (192.168.1.20)</h4>

<div class="table-comparison-container">
  <table class="table-comparison">
    <tr>
      <th style="width: 30%;">Component</th>
      <th style="width: 70%;">Specification</th>
    </tr>
    <tr>
      <td><strong>CPU</strong></td>
      <td>Intel Xeon 6-core</td>
    </tr>
    <tr>
      <td><strong>RAM</strong></td>
      <td>32GB</td>
    </tr>
    <tr>
      <td><strong>GPU</strong></td>
      <td>NVIDIA GeForce RTX 3060 12GB</td>
    </tr>
    <tr>
      <td><strong>OS</strong></td>
      <td>Ubuntu 24.04 LTS</td>
    </tr>
    <tr>
      <td><strong>Purpose</strong></td>
      <td>Ollama model inference server</td>
    </tr>
  </table>
</div>

<p><br /></p>

<h4 id="web-server-192168121">Web Server (192.168.1.21)</h4>

<div class="table-comparison-container">
  <table class="table-comparison">
    <tr>
      <th style="width: 30%;">Component</th>
      <th style="width: 70%;">Specification</th>
    </tr>
    <tr>
      <td><strong>CPU</strong></td>
      <td>4-core</td>
    </tr>
    <tr>
      <td><strong>RAM</strong></td>
      <td>16GB</td>
    </tr>
    <tr>
      <td><strong>OS</strong></td>
      <td>Ubuntu 24.04 LTS</td>
    </tr>
    <tr>
      <td><strong>Purpose</strong></td>
      <td>Open WebUI frontend</td>
    </tr>
  </table>
</div>

<p><br /></p>

<h4 id="storage-server-192168125">Storage Server (192.168.1.25)</h4>

<div class="table-comparison-container">
  <table class="table-comparison">
    <tr>
      <th style="width: 30%;">Component</th>
      <th style="width: 70%;">Specification</th>
    </tr>
    <tr>
      <td><strong>NFS Export</strong></td>
      <td>/storage/ai-data (shared directory)</td>
    </tr>
    <tr>
      <td><strong>Purpose</strong></td>
      <td>Centralized model storage</td>
    </tr>
  </table>
</div>

<p><br /></p>

<h3 id="gpu-compatibility-verification">GPU Compatibility Verification</h3>

<p>First, verify your installed GPU specifications:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lspci | <span class="nb">grep</span> <span class="nt">-iE</span> <span class="s2">"vga|display|3d"</span>
<span class="c"># Expected output:</span>
<span class="c"># 01:00.0 VGA compatible controller: NVIDIA Corporation GA106 [GeForce RTX 3060]</span>
</code></pre></div></div>

<p><br /></p>

<p><strong>RTX 3060 Key Specifications</strong>:</p>
<ul>
  <li>Architecture: Ampere (GA106)</li>
  <li>CUDA Cores: 3,584</li>
  <li>VRAM: 12GB GDDR6</li>
  <li>Memory Bus: 192-bit</li>
  <li>CUDA Compute Capability: 8.6</li>
  <li>Recommended Driver: 525+ (535 stable)</li>
  <li>TDP: 170W</li>
</ul>

<p><br /></p>

<h3 id="architecture-diagram">Architecture Diagram</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────────────────────────┐
│                   NFS Storage Server                     │
│                  <span class="o">(</span>192.168.1.25<span class="o">)</span>                          │
│         /storage/ai-data/<span class="o">{</span>models,webui<span class="o">}</span>                  │
└────────────────────┬────────────────────────────────────┘
                     │
        ┌────────────┴────────────┐
        │                         │
┌───────▼──────────┐     ┌───────▼──────────┐
│  GPU Server      │     │  Web Server      │
│  <span class="o">(</span>192.168.1.20<span class="o">)</span>  │     │  <span class="o">(</span>192.168.1.21<span class="o">)</span>  │
│                  │     │                  │
│  ┌────────────┐  │     │  ┌────────────┐  │
│  │  Ollama    │  │     │  │ Open WebUI │  │
│  │  Container │  │     │  │  Container │  │
│  │            │  │     │  │            │  │
│  │ RTX 3060   │  │     │  │  Port 8080 │  │
│  │ 12GB VRAM  │  │     │  └────────────┘  │
│  └────────────┘  │     │                  │
│  Port: 11434     │     │                  │
└──────────────────┘     └──────────────────┘
         │                        │
         └────────┬───────────────┘
                  │
          Docker Network: ai_network
</code></pre></div></div>

<hr />

<h2 id="step-1-nvidia-driver-installation">Step 1: NVIDIA Driver Installation</h2>

<p><br /></p>

<h3 id="secure-boot-status-check">Secure Boot Status Check</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mokutil <span class="nt">--sb-state</span>
<span class="c"># Output will be either:</span>
<span class="c"># SecureBoot disabled  → Direct installation possible</span>
<span class="c"># SecureBoot enabled   → MOK (Machine Owner Key) enrollment required post-install</span>
</code></pre></div></div>

<p>If Secure Boot is enabled, you’ll need to complete the MOK enrollment process after driver installation.</p>

<p><br /></p>

<h3 id="available-driver-version-check">Available Driver Version Check</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check available driver versions in repository</span>
apt-cache search nvidia-driver

<span class="c"># View detailed information for specific version</span>
apt-cache show nvidia-driver-535 | <span class="nb">grep </span>Version
<span class="c"># Example output:</span>
<span class="c"># Version: 535.288.01-0ubuntu0.24.04.1</span>
</code></pre></div></div>

<p>Alternatively, verify the latest version on NVIDIA’s official website:</p>
<ul>
  <li>URL: https://www.nvidia.com/Download/index.aspx</li>
  <li>Product Type: GeForce</li>
  <li>Product Series: GeForce RTX 30 Series</li>
  <li>Product: GeForce RTX 3060</li>
  <li>Operating System: Linux 64-bit</li>
</ul>

<p><br /></p>

<h3 id="driver-installation-process">Driver Installation Process</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Update system packages</span>
<span class="nb">sudo </span>apt update

<span class="c"># Install NVIDIA driver</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> nvidia-driver-535

<span class="c"># Reboot system</span>
<span class="nb">sudo </span>reboot
</code></pre></div></div>

<p><br /></p>

<h3 id="mok-enrollment-if-secure-boot-enabled">MOK Enrollment (If Secure Boot Enabled)</h3>

<p>After reboot, if Secure Boot is enabled, a blue MOK management screen will appear:</p>

<ol>
  <li>Select “Enroll MOK”</li>
  <li>Select “Continue”</li>
  <li>Enter the password you set during installation</li>
  <li>Select “Reboot”</li>
</ol>

<p><br /></p>

<h3 id="installation-verification">Installation Verification</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi
</code></pre></div></div>

<p>Expected output for successful installation:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Wed Feb  5 10:15:23 2026       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.288.01             Driver Version: 535.288.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0 Off |                  N/A |
|  0%   35C    P8              15W / 170W |      0MiB / 12288MiB |      0%      Default |
+---------------------------------------------------------------------------------------+
</code></pre></div></div>

<hr />

<h2 id="step-2-docker--nvidia-container-toolkit">Step 2: Docker &amp; NVIDIA Container Toolkit</h2>

<p><br /></p>

<h3 id="docker-installation">Docker Installation</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Verify Docker installation</span>
docker <span class="nt">--version</span>

<span class="c"># If not installed, install Docker</span>
<span class="nb">sudo </span>apt update
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> docker.io
<span class="nb">sudo </span>systemctl <span class="nb">enable</span> <span class="nt">--now</span> docker

<span class="c"># Optional: Add current user to docker group</span>
<span class="nb">sudo </span>usermod <span class="nt">-aG</span> docker <span class="nv">$USER</span>
<span class="c"># Note: Logout and login required for group changes to take effect</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="nvidia-container-toolkit-installation">NVIDIA Container Toolkit Installation</h3>

<p>The NVIDIA Container Toolkit enables GPU access from within Docker containers, essential for running Ollama with GPU acceleration.</p>

<p><br /></p>

<script src="https://gist.github.com/somaz94/3c1d7221046d60a1a7b0ff72b0bd4755.js"></script>

<p><br /></p>

<h3 id="docker-runtime-configuration">Docker Runtime Configuration</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Configure Docker to use NVIDIA runtime</span>
<span class="nb">sudo </span>nvidia-ctk runtime configure <span class="nt">--runtime</span><span class="o">=</span>docker

<span class="c"># Restart Docker service</span>
<span class="nb">sudo </span>systemctl restart docker
</code></pre></div></div>

<p><br /></p>

<h3 id="gpu-access-test">GPU Access Test</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">--rm</span> <span class="nt">--gpus</span> all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
</code></pre></div></div>

<p>If GPU information is displayed correctly, the setup is complete and GPU acceleration is available to containers.</p>

<hr />

<h2 id="step-3-nfs-storage-configuration">Step 3: NFS Storage Configuration</h2>

<p>Centralizing model files and data through NFS enables efficient storage sharing across multiple servers and simplifies backup management.</p>

<p><br /></p>

<h3 id="client-installation--mounting">Client Installation &amp; Mounting</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install NFS client</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> nfs-common

<span class="c"># Create mount points</span>
<span class="nb">sudo mkdir</span> <span class="nt">-p</span> /data/llm-models
<span class="nb">sudo mkdir</span> <span class="nt">-p</span> /data/webui-storage

<span class="c"># Mount NFS shares</span>
<span class="nb">sudo </span>mount <span class="nt">-t</span> nfs4 192.168.1.25:/storage/ai-data/models /data/llm-models
<span class="nb">sudo </span>mount <span class="nt">-t</span> nfs4 192.168.1.25:/storage/ai-data/webui /data/webui-storage

<span class="c"># Verify mounts</span>
<span class="nb">df</span> <span class="nt">-h</span> | <span class="nb">grep </span>nfs
</code></pre></div></div>

<p><br /></p>

<h3 id="persistent-mount-configuration">Persistent Mount Configuration</h3>

<p>Configure <code class="language-plaintext highlighter-rouge">/etc/fstab</code> for automatic mounting after reboots:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>nano /etc/fstab

<span class="c"># Add these lines:</span>
192.168.1.25:/storage/ai-data/models  /data/llm-models     nfs4  defaults,_netdev,rw  0  0
192.168.1.25:/storage/ai-data/webui   /data/webui-storage  nfs4  defaults,_netdev,rw  0  0
</code></pre></div></div>

<h4 id="configuration-parameters-explained">Configuration parameters explained:</h4>
<ul>
  <li><code class="language-plaintext highlighter-rouge">defaults</code>: Use default NFS mount options</li>
  <li><code class="language-plaintext highlighter-rouge">_netdev</code>: Wait for network before mounting</li>
  <li><code class="language-plaintext highlighter-rouge">rw</code>: Read-write access</li>
  <li><code class="language-plaintext highlighter-rouge">0 0</code>: No dump, no fsck</li>
</ul>

<p>Verify configuration:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>mount <span class="nt">-a</span>
<span class="nb">df</span> <span class="nt">-h</span> | <span class="nb">grep </span>nfs
</code></pre></div></div>

<p><br /></p>

<h3 id="permission-configuration">Permission Configuration</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ollama data directory permissions</span>
<span class="nb">sudo chown</span> <span class="nt">-R</span> root:root /data/llm-models
<span class="nb">sudo chmod </span>755 /data/llm-models

<span class="c"># Open WebUI data directory permissions</span>
<span class="nb">sudo chown</span> <span class="nt">-R</span> root:root /data/webui-storage
<span class="nb">sudo chmod </span>755 /data/webui-storage
</code></pre></div></div>

<hr />

<h2 id="step-4-docker-network-setup">Step 4: Docker Network Setup</h2>

<p>Create a shared Docker network to enable communication between Ollama and Open WebUI containers:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create Docker bridge network</span>
docker network create ai_network

<span class="c"># Verify network creation</span>
docker network <span class="nb">ls</span>
<span class="c"># Expected output should include:</span>
<span class="c"># NETWORK ID     NAME         DRIVER    SCOPE</span>
<span class="c"># abc123def456   ai_network   bridge    local</span>
</code></pre></div></div>

<p>This dedicated network provides:</p>
<ul>
  <li><strong>Isolation</strong>: Containers communicate only within this network</li>
  <li><strong>Service Discovery</strong>: Containers can reference each other by name</li>
  <li><strong>Security</strong>: Network-level separation from other containers</li>
</ul>

<hr />

<h2 id="step-5-ollama-server-deployment">Step 5: Ollama Server Deployment</h2>

<p><br /></p>

<h3 id="docker-compose-configuration">Docker Compose Configuration</h3>

<p>Create the Ollama service configuration with GPU acceleration and NFS storage:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /opt/ollama
<span class="nb">cat</span> <span class="o">&gt;</span> /opt/ollama/docker-compose.yml <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">'
version: '3.8'

services:
  ollama:
    image: ollama/ollama:0.15.4
    container_name: ollama-server
    restart: unless-stopped
    runtime: nvidia
    
    ports:
      - "11434:11434"
    
    volumes:
      - /data/llm-models:/root/.ollama
    
    environment:
      # Model memory retention settings
      # -1: Keep loaded indefinitely (never unload, always in VRAM)
      # 0: Unload immediately after use (free VRAM instantly)
      # 5m: Unload after 5 minutes (default)
      # 30m: Unload after 30 minutes
      # 1h: Unload after 1 hour
      # 
      # Setting to -1:
      #   Pros: Instant response after first load (no loading delay)
      #   Cons: Constant VRAM occupation (qwen2.5-coder:14b = ~8GB permanently)
      # 
      # Recommendation: -1 for frequent use, 30m for occasional use
      - OLLAMA_KEEP_ALIVE=-1
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_ORIGINS=*
    
    networks:
      - ai_network

networks:
  ai_network:
    name: ai_network
    external: true
</span><span class="no">EOF
</span></code></pre></div></div>

<p><br /></p>

<h3 id="environment-variable-detailed-explanation">Environment Variable Detailed Explanation</h3>

<div class="table-comparison-container">
  <table class="table-comparison">
    <tr>
      <th style="width: 30%;">Variable</th>
      <th style="width: 70%;">Description</th>
    </tr>
    <tr>
      <td><strong>OLLAMA_KEEP_ALIVE</strong></td>
      <td>Controls model memory retention duration</td>
    </tr>
    <tr>
      <td><code>-1</code></td>
      <td>Permanent retention (first load only, then instant responses)</td>
    </tr>
    <tr>
      <td><code>5m</code> (default)</td>
      <td>5-minute retention (balances memory and responsiveness)</td>
    </tr>
    <tr>
      <td><code>30m</code></td>
      <td>30-minute retention (recommended for regular use)</td>
    </tr>
    <tr>
      <td><code>0</code></td>
      <td>Immediate unload (maximum memory conservation)</td>
    </tr>
    <tr>
      <td><strong>NVIDIA_VISIBLE_DEVICES</strong></td>
      <td>Specifies which GPUs to use (<code>all</code> = all available GPUs)</td>
    </tr>
    <tr>
      <td><strong>OLLAMA_ORIGINS</strong></td>
      <td>CORS configuration (<code>*</code> = allow all origins)</td>
    </tr>
  </table>
</div>

<p><br /></p>

<h3 id="keep_alive-configuration-guide">KEEP_ALIVE Configuration Guide</h3>

<div class="table-comparison-container">
  <table class="table-comparison">
    <tr>
      <th style="width: 30%;">Usage Pattern</th>
      <th style="width: 25%;">Recommendation</th>
      <th style="width: 45%;">Rationale</th>
    </tr>
    <tr>
      <td><strong>Frequent (10+ times/day)</strong></td>
      <td><code>-1</code></td>
      <td>Fast responses justify VRAM occupation</td>
    </tr>
    <tr>
      <td><strong>Regular (3-10 times/day)</strong></td>
      <td><code>30m</code> or <code>1h</code></td>
      <td>Balance between speed and memory efficiency</td>
    </tr>
    <tr>
      <td><strong>Occasional (1-2 times/day)</strong></td>
      <td><code>5m</code> (default)</td>
      <td>Maximize memory availability</td>
    </tr>
  </table>
</div>

<p><br /></p>

<h3 id="container-deployment">Container Deployment</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Navigate to configuration directory</span>
<span class="nb">cd</span> /opt/ollama

<span class="c"># Start container</span>
docker-compose up <span class="nt">-d</span>

<span class="c"># Monitor logs</span>
docker-compose logs <span class="nt">-f</span>

<span class="c"># Check container status</span>
docker-compose ps
</code></pre></div></div>

<p><br /></p>

<h3 id="gpu-usage-verification">GPU Usage Verification</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Execute nvidia-smi inside container</span>
docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server nvidia-smi

<span class="c"># Verify NFS mount</span>
docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server <span class="nb">df</span> <span class="nt">-h</span> /root/.ollama
</code></pre></div></div>

<p><br /></p>

<h3 id="llm-model-download">LLM Model Download</h3>

<p>Ollama provides various models at https://ollama.com/library. Recommended models for RTX 3060 12GB:</p>

<p><br /></p>

<p><strong>1) DeepSeek-Coder-V2 16B (Coding Specialist, 10GB)</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama pull deepseek-coder-v2:16b
</code></pre></div></div>

<p><strong>2) Qwen2.5-Coder 14B (Coding, 9.7GB)</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama pull qwen2.5-coder:14b
</code></pre></div></div>

<p><strong>3) Qwen3 14B (General Purpose, 11GB)</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama pull qwen3:14b
</code></pre></div></div>

<p><strong>4) Qwen2.5-Coder 32B (High Performance Coding, 21GB - CPU Offloading)</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama pull qwen2.5-coder:32b
</code></pre></div></div>

<p>Verify downloaded models:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ls</span> <span class="nt">-lh</span> /data/llm-models/models/
</code></pre></div></div>

<p><br /></p>

<h3 id="model-testing">Model Testing</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Test DeepSeek 16B</span>
docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama run deepseek-coder-v2:16b
<span class="o">&gt;&gt;&gt;</span> Write a Python <span class="k">function </span>to reverse a string

<span class="c"># Exit: Ctrl+D or type /bye</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="model-performance-comparison-rtx-3060-12gb">Model Performance Comparison (RTX 3060 12GB)</h3>

<p><br /></p>

<h4 id="deepseek-coder-v2-16b-10gb">DeepSeek-Coder-V2 16B (10GB)</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama ps

NAME                     ID              SIZE     PROCESSOR    CONTEXT
deepseek-coder-v2:16b    63fb193b3a9b    10 GB    100% GPU     4096
</code></pre></div></div>

<h4 id="memory-usage">Memory Usage:</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0 Off |                  N/A |
|  0%   49C    P2              26W / 170W |   9898MiB / 12288MiB |      0%      Default |
</code></pre></div></div>

<p><br /></p>

<h4 id="qwen25-coder-14b-97gb">Qwen2.5-Coder 14B (9.7GB)</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                 ID              SIZE      PROCESSOR    CONTEXT
qwen2.5-coder:14b    9ec8897f747e    9.7 GB    100% GPU     4096
</code></pre></div></div>

<h4 id="memory-usage-1">Memory Usage:</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0 Off |                  N/A |
|  0%   50C    P2              44W / 170W |   9482MiB / 12288MiB |      0%      Default |
</code></pre></div></div>

<p><br /></p>

<h4 id="qwen25-coder-32b-21gb---cpu-offloading">Qwen2.5-Coder 32B (21GB - CPU Offloading)</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NAME                 ID              SIZE     PROCESSOR          CONTEXT
qwen2.5-coder:32b    b92d6a0bd47e    21 GB    45%/55% CPU/GPU    4096
</code></pre></div></div>

<h4 id="memory-usage-2">Memory Usage:</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0 Off |                  N/A |
|  0%   60C    P2              60W / 170W |  11476MiB / 12288MiB |     17%      Default |
</code></pre></div></div>

<p><br /></p>

<h3 id="performance-analysis--recommendations">Performance Analysis &amp; Recommendations</h3>

<div class="table-comparison-container">
  <table class="table-comparison">
    <tr>
      <th style="width: 20%;">Model</th>
      <th style="width: 15%;">Size</th>
      <th style="width: 15%;">VRAM</th>
      <th style="width: 20%;">Processing</th>
      <th style="width: 15%;">Speed</th>
      <th style="width: 15%;">Best For</th>
    </tr>
    <tr>
      <td><strong>DeepSeek 16B</strong></td>
      <td>10GB</td>
      <td>9.9GB</td>
      <td>100% GPU</td>
      <td>★★★★★</td>
      <td>Real-time coding (fastest)</td>
    </tr>
    <tr>
      <td><strong>Qwen2.5 14B</strong></td>
      <td>9.7GB</td>
      <td>9.5GB</td>
      <td>100% GPU</td>
      <td>★★★★☆</td>
      <td>General coding tasks</td>
    </tr>
    <tr>
      <td><strong>Qwen3 14B</strong></td>
      <td>11GB</td>
      <td>11GB</td>
      <td>100% GPU</td>
      <td>★★★★☆</td>
      <td>Complex reasoning/logic</td>
    </tr>
    <tr>
      <td><strong>Qwen2.5 32B</strong></td>
      <td>21GB</td>
      <td>11.5GB</td>
      <td>45% CPU<br />55% GPU</td>
      <td>★★☆☆☆</td>
      <td>High quality needed<br />(5× slower)</td>
    </tr>
  </table>
</div>

<p><br /></p>

<h4 id="key-insights">Key Insights:</h4>
<ul>
  <li><strong>VRAM ≤12GB models</strong> → 100% GPU processing → Fast response</li>
  <li><strong>VRAM &gt;12GB models</strong> → CPU offloading → Significantly slower</li>
  <li><strong>Production recommendation</strong>: DeepSeek 16B (optimal speed/performance balance)</li>
</ul>

<p><br /></p>

<h3 id="model-management-commands">Model Management Commands</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check running models</span>
docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama ps

<span class="c"># Unload models (free VRAM)</span>
docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama stop deepseek-coder-v2:16b
docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama stop qwen2.5-coder:32b

<span class="c"># Delete models</span>
docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama <span class="nb">rm </span>qwen2.5-coder:32b

<span class="c"># List all installed models</span>
docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama list
</code></pre></div></div>

<hr />

<h2 id="step-6-open-webui-installation">Step 6: Open WebUI Installation</h2>

<p>Open WebUI is an open-source project providing a ChatGPT-like web interface for Ollama models.</p>

<p><br /></p>

<h3 id="docker-compose-configuration-1">Docker Compose Configuration</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> /opt/open-webui
<span class="nb">cat</span> <span class="o">&gt;</span> /opt/open-webui/docker-compose.yml <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">'
version: '3.8'

services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    
    ports:
      - "8080:8080"
    
    volumes:
      - /data/webui-storage:/app/backend/data
    
    environment:
      - OLLAMA_BASE_URL=http://192.168.1.20:11434
      - WEBUI_SECRET_KEY=your-random-secret-key-change-this
      - WEBUI_NAME=Company AI Assistant
      - DEFAULT_MODELS=deepseek-coder-v2:16b,qwen2.5-coder:14b
    
    networks:
      - ai_network

networks:
  ai_network:
    name: ai_network
    external: true
</span><span class="no">EOF
</span></code></pre></div></div>

<p><br /></p>

<h3 id="environment-variables-explained">Environment Variables Explained</h3>

<div class="table-comparison-container">
  <table class="table-comparison">
    <tr>
      <th style="width: 30%;">Variable</th>
      <th style="width: 70%;">Description</th>
    </tr>
    <tr>
      <td><strong>OLLAMA_BASE_URL</strong></td>
      <td>Ollama server address (use service name for same network, IP for different servers)</td>
    </tr>
    <tr>
      <td><strong>WEBUI_SECRET_KEY</strong></td>
      <td>Session encryption key (must change to random string)</td>
    </tr>
    <tr>
      <td><strong>WEBUI_NAME</strong></td>
      <td>Web UI title/branding</td>
    </tr>
    <tr>
      <td><strong>DEFAULT_MODELS</strong></td>
      <td>Comma-separated list of models to display by default</td>
    </tr>
  </table>
</div>

<p><br /></p>

<h4 id="important-notes">Important Notes:</h4>
<ul>
  <li>Within same Docker network: Use <code class="language-plaintext highlighter-rouge">http://ollama-server:11434</code></li>
  <li>Different physical servers: Use IP address <code class="language-plaintext highlighter-rouge">http://192.168.1.20:11434</code></li>
  <li>Always change <code class="language-plaintext highlighter-rouge">WEBUI_SECRET_KEY</code> to a secure random string</li>
</ul>

<p><br /></p>

<h3 id="container-deployment-1">Container Deployment</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Navigate to configuration directory</span>
<span class="nb">cd</span> /opt/open-webui

<span class="c"># Start container</span>
docker-compose up <span class="nt">-d</span>

<span class="c"># Monitor logs</span>
docker-compose logs <span class="nt">-f</span>

<span class="c"># Check status</span>
docker-compose ps
</code></pre></div></div>

<p><br /></p>

<h3 id="web-interface-access">Web Interface Access</h3>

<p>Access the web interface at <code class="language-plaintext highlighter-rouge">http://192.168.1.21:8080</code></p>

<p><strong>Initial Setup</strong>:</p>
<ol>
  <li>Create account (first user becomes admin automatically)</li>
  <li>Navigate to Settings → Models → Verify Ollama connection</li>
  <li>Start chatting</li>
</ol>

<hr />

<h2 id="step-7-api-integration--usage">Step 7: API Integration &amp; Usage</h2>

<p>Open WebUI provides OpenAI-compatible API endpoints for programmatic access.</p>

<p><br /></p>

<h3 id="api-key-generation">API Key Generation</h3>

<ol>
  <li>Log in to Web UI</li>
  <li>Navigate to Settings → Account → API Keys</li>
  <li>Click “Create new secret key”</li>
  <li>Copy generated key (e.g., <code class="language-plaintext highlighter-rouge">sk-abc123def456...</code>)</li>
</ol>

<p><br /></p>

<h3 id="api-usage-examples">API Usage Examples</h3>

<p><br /></p>

<h4 id="curl-test">cURL Test:</h4>

<script src="https://gist.github.com/somaz94/2880cc22797fbacc83d77e74c83e9e08.js"></script>

<p><br /></p>

<p><strong>Python Example:</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="n">API_URL</span> <span class="o">=</span> <span class="s">"http://192.168.1.21:8080/api/chat/completions"</span>
<span class="n">API_KEY</span> <span class="o">=</span> <span class="s">"sk-abc123def456..."</span>

<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"Authorization"</span><span class="p">:</span> <span class="sa">f</span><span class="s">"Bearer </span><span class="si">{</span><span class="n">API_KEY</span><span class="si">}</span><span class="s">"</span><span class="p">,</span>
    <span class="s">"Content-Type"</span><span class="p">:</span> <span class="s">"application/json"</span>
<span class="p">}</span>

<span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"model"</span><span class="p">:</span> <span class="s">"deepseek-coder-v2:16b"</span><span class="p">,</span>
    <span class="s">"messages"</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s">"role"</span><span class="p">:</span> <span class="s">"system"</span><span class="p">,</span>
            <span class="s">"content"</span><span class="p">:</span> <span class="s">"You are a Python coding expert assistant."</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s">"role"</span><span class="p">:</span> <span class="s">"user"</span><span class="p">,</span>
            <span class="s">"content"</span><span class="p">:</span> <span class="s">"Implement binary search algorithm"</span>
        <span class="p">}</span>
    <span class="p">],</span>
    <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
    <span class="s">"max_tokens"</span><span class="p">:</span> <span class="mi">2000</span>
<span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">post</span><span class="p">(</span><span class="n">API_URL</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">json</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="s">'choices'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s">'message'</span><span class="p">][</span><span class="s">'content'</span><span class="p">])</span>
</code></pre></div></div>

<p><br /></p>

<h4 id="streaming-response-example">Streaming Response Example:</h4>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"model"</span><span class="p">:</span> <span class="s">"qwen2.5-coder:14b"</span><span class="p">,</span>
    <span class="s">"messages"</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"user"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="s">"Explain REST API architecture"</span><span class="p">}</span>
    <span class="p">],</span>
    <span class="s">"stream"</span><span class="p">:</span> <span class="bp">True</span>
<span class="p">}</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">post</span><span class="p">(</span>
    <span class="n">API_URL</span><span class="p">,</span> 
    <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> 
    <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">,</span> 
    <span class="n">stream</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">iter_lines</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">line</span><span class="p">:</span>
        <span class="n">decoded</span> <span class="o">=</span> <span class="n">line</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">decoded</span><span class="p">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'data: '</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">decoded</span><span class="p">[</span><span class="mi">6</span><span class="p">:])</span>
            <span class="k">if</span> <span class="s">'choices'</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
                <span class="n">content</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">'choices'</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s">'delta'</span><span class="p">].</span><span class="n">get</span><span class="p">(</span><span class="s">'content'</span><span class="p">,</span> <span class="s">''</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s">''</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<hr />

<h2 id="step-8-monitoring--maintenance">Step 8: Monitoring &amp; Maintenance</h2>

<p><br /></p>

<h3 id="gpu-monitoring">GPU Monitoring</h3>

<h4 id="real-time-gpu-usage">Real-time GPU usage:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>watch <span class="nt">-n</span> 1 nvidia-smi
</code></pre></div></div>

<h4 id="log-based-monitoring">Log-based monitoring:</h4>
<script src="https://gist.github.com/somaz94/5fb729d4e2f066952f754e3333ed2531.js"></script>

<p><br /></p>

<h3 id="docker-container-management">Docker Container Management</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Restart Ollama</span>
<span class="nb">cd</span> /opt/ollama
docker-compose restart

<span class="c"># Restart Open WebUI</span>
<span class="nb">cd</span> /opt/open-webui
docker-compose restart

<span class="c"># View recent logs (last 100 lines)</span>
docker-compose logs <span class="nt">--tail</span> 100 ollama
docker-compose logs <span class="nt">--tail</span> 100 open-webui

<span class="c"># Monitor resource usage</span>
docker stats
</code></pre></div></div>

<p><br /></p>

<h3 id="backup-strategy">Backup Strategy</h3>

<p><br /></p>

<h4 id="model-files-backup">Model Files Backup:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Backup NFS directory</span>
<span class="nb">sudo </span>rsync <span class="nt">-avz</span> /data/llm-models/ /backup/llm-models-<span class="si">$(</span><span class="nb">date</span> +%Y%m%d<span class="si">)</span>/
</code></pre></div></div>

<h4 id="open-webui-data-backup">Open WebUI Data Backup:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Backup user settings and chat history</span>
<span class="nb">sudo </span>rsync <span class="nt">-avz</span> /data/webui-storage/ /backup/webui-storage-<span class="si">$(</span><span class="nb">date</span> +%Y%m%d<span class="si">)</span>/
</code></pre></div></div>

<p><br /></p>

<h3 id="automated-backup-script">Automated Backup Script</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat</span> <span class="o">&gt;</span> /usr/local/bin/backup-ai-system.sh <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">'
#!/bin/bash

BACKUP_DIR="/backup/ai-system"
DATE=</span><span class="si">$(</span><span class="nb">date</span> +%Y%m%d_%H%M%S<span class="si">)</span><span class="sh">

# Create backup directory
mkdir -p </span><span class="k">${</span><span class="nv">BACKUP_DIR</span><span class="k">}</span><span class="sh">/</span><span class="k">${</span><span class="nv">DATE</span><span class="k">}</span><span class="sh">

# Model backup (using symbolic link)
ln -sf /data/llm-models </span><span class="k">${</span><span class="nv">BACKUP_DIR</span><span class="k">}</span><span class="sh">/</span><span class="k">${</span><span class="nv">DATE</span><span class="k">}</span><span class="sh">/models

# WebUI data backup
rsync -az /data/webui-storage/ </span><span class="k">${</span><span class="nv">BACKUP_DIR</span><span class="k">}</span><span class="sh">/</span><span class="k">${</span><span class="nv">DATE</span><span class="k">}</span><span class="sh">/webui/

# Delete backups older than 7 days
find </span><span class="k">${</span><span class="nv">BACKUP_DIR</span><span class="k">}</span><span class="sh"> -type d -mtime +7 -exec rm -rf {} +

echo "Backup completed: </span><span class="k">${</span><span class="nv">BACKUP_DIR</span><span class="k">}</span><span class="sh">/</span><span class="k">${</span><span class="nv">DATE</span><span class="k">}</span><span class="sh">"
</span><span class="no">EOF

</span><span class="nb">chmod</span> +x /usr/local/bin/backup-ai-system.sh

<span class="c"># Schedule daily backup at 2 AM</span>
<span class="o">(</span>crontab <span class="nt">-l</span> 2&gt;/dev/null<span class="p">;</span> <span class="nb">echo</span> <span class="s2">"0 2 * * * /usr/local/bin/backup-ai-system.sh &gt;&gt; /var/log/ai-backup.log 2&gt;&amp;1"</span><span class="o">)</span> | crontab -
</code></pre></div></div>

<hr />

<h2 id="troubleshooting-guide">Troubleshooting Guide</h2>

<p><br /></p>

<h3 id="gpu-not-detected">GPU Not Detected</h3>

<p><strong>Symptom</strong>: <code class="language-plaintext highlighter-rouge">docker exec -it ollama-server nvidia-smi</code> fails</p>

<h4 id="solution">Solution:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Reinstall drivers</span>
<span class="nb">sudo </span>apt purge <span class="nt">-y</span> nvidia-<span class="k">*</span>
<span class="nb">sudo </span>apt <span class="nb">install</span> <span class="nt">-y</span> nvidia-driver-535
<span class="nb">sudo </span>reboot

<span class="c"># Reconfigure Container Toolkit</span>
<span class="nb">sudo </span>nvidia-ctk runtime configure <span class="nt">--runtime</span><span class="o">=</span>docker
<span class="nb">sudo </span>systemctl restart docker
</code></pre></div></div>

<p><br /></p>

<h3 id="slow-ollama-response-32b-model">Slow Ollama Response (32B Model)</h3>

<p><strong>Symptom</strong>: Very slow responses, GPU memory insufficient</p>

<h4 id="solution-use-smaller-model-or-upgrade-gpu">Solution: Use smaller model or upgrade GPU</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Switch to memory-efficient model</span>
docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama stop qwen2.5-coder:32b
docker <span class="nb">exec</span> <span class="nt">-it</span> ollama-server ollama run deepseek-coder-v2:16b
</code></pre></div></div>

<p><br /></p>

<h3 id="nfs-mount-failure">NFS Mount Failure</h3>

<p><strong>Symptom</strong>: <code class="language-plaintext highlighter-rouge">mount.nfs4: Connection timed out</code></p>

<h4 id="solution-1">Solution:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Verify NFS server</span>
showmount <span class="nt">-e</span> 192.168.1.25

<span class="c"># Check firewall (on NFS server)</span>
<span class="nb">sudo </span>ufw allow from 192.168.1.0/24 to any port nfs

<span class="c"># Remount</span>
<span class="nb">sudo </span>umount /data/llm-models
<span class="nb">sudo </span>mount <span class="nt">-a</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="open-webui-connection-error">Open WebUI Connection Error</h3>

<p><strong>Symptom</strong>: “Failed to connect to Ollama”</p>

<h4 id="solution-2">Solution:</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Test Ollama API</span>
curl http://192.168.1.20:11434/api/tags

<span class="c"># Verify network</span>
docker network inspect ai_network

<span class="c"># Update environment variable</span>
<span class="nb">cd</span> /opt/open-webui
docker-compose down
<span class="c"># Edit docker-compose.yml to update OLLAMA_BASE_URL</span>
docker-compose up <span class="nt">-d</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="long-model-loading-time">Long Model Loading Time</h3>

<p><strong>Symptom</strong>: First request takes 10-30 seconds</p>

<h4 id="solution-adjust-ollama_keep_alive-setting">Solution: Adjust <code class="language-plaintext highlighter-rouge">OLLAMA_KEEP_ALIVE</code> setting</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Edit docker-compose.yml</span>
environment:
  - <span class="nv">OLLAMA_KEEP_ALIVE</span><span class="o">=</span><span class="nt">-1</span>  <span class="c"># Change to permanent retention</span>

<span class="c"># Restart</span>
<span class="nb">cd</span> /opt/ollama
docker-compose down
docker-compose up <span class="nt">-d</span>
</code></pre></div></div>

<hr />

<h2 id="security-considerations">Security Considerations</h2>

<p><br /></p>

<h3 id="firewall-configuration">Firewall Configuration</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Enable UFW</span>
<span class="nb">sudo </span>ufw <span class="nb">enable</span>

<span class="c"># Ollama API (internal network only)</span>
<span class="nb">sudo </span>ufw allow from 192.168.1.0/24 to any port 11434

<span class="c"># Open WebUI (VPN users only if needed)</span>
<span class="nb">sudo </span>ufw allow from 10.8.0.0/24 to any port 8080

<span class="c"># SSH (management)</span>
<span class="nb">sudo </span>ufw allow 22/tcp
</code></pre></div></div>

<p><br /></p>

<h3 id="reverse-proxy-setup-nginx">Reverse Proxy Setup (Nginx)</h3>

<p>For external access, HTTPS is strongly recommended:</p>
<div class="language-nginx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># /etc/nginx/sites-available/ai-assistant</span>
<span class="k">server</span> <span class="p">{</span>
    <span class="kn">listen</span> <span class="mi">443</span> <span class="s">ssl</span> <span class="s">http2</span><span class="p">;</span>
    <span class="kn">server_name</span> <span class="s">ai.company.com</span><span class="p">;</span>
    
    <span class="kn">ssl_certificate</span> <span class="n">/etc/letsencrypt/live/ai.company.com/fullchain.pem</span><span class="p">;</span>
    <span class="kn">ssl_certificate_key</span> <span class="n">/etc/letsencrypt/live/ai.company.com/privkey.pem</span><span class="p">;</span>
    
    <span class="kn">location</span> <span class="n">/</span> <span class="p">{</span>
        <span class="kn">proxy_pass</span> <span class="s">http://192.168.1.21:8080</span><span class="p">;</span>
        <span class="kn">proxy_http_version</span> <span class="mi">1</span><span class="s">.1</span><span class="p">;</span>
        <span class="kn">proxy_set_header</span> <span class="s">Upgrade</span> <span class="nv">$http_upgrade</span><span class="p">;</span>
        <span class="kn">proxy_set_header</span> <span class="s">Connection</span> <span class="s">'upgrade'</span><span class="p">;</span>
        <span class="kn">proxy_set_header</span> <span class="s">Host</span> <span class="nv">$host</span><span class="p">;</span>
        <span class="kn">proxy_cache_bypass</span> <span class="nv">$http_upgrade</span><span class="p">;</span>
        
        <span class="c1"># WebSocket support</span>
        <span class="kn">proxy_set_header</span> <span class="s">X-Real-IP</span> <span class="nv">$remote_addr</span><span class="p">;</span>
        <span class="kn">proxy_set_header</span> <span class="s">X-Forwarded-For</span> <span class="nv">$proxy_add_x_forwarded_for</span><span class="p">;</span>
        <span class="kn">proxy_set_header</span> <span class="s">X-Forwarded-Proto</span> <span class="nv">$scheme</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="user-access-control">User Access Control</h3>

<p>Configure from Open WebUI admin page:</p>
<ol>
  <li>Settings → Users → User Permissions</li>
  <li>Set per-user model access permissions</li>
  <li>Configure daily request limits</li>
</ol>

<hr />

<h2 id="performance-optimization">Performance Optimization</h2>

<p><br /></p>

<h3 id="1-model-preloading">1. Model Preloading</h3>

<p>Preload frequently used models into memory:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create systemd service</span>
<span class="nb">cat</span> <span class="o">&gt;</span> /etc/systemd/system/ollama-preload.service <span class="o">&lt;&lt;</span> <span class="sh">'</span><span class="no">EOF</span><span class="sh">'
[Unit]
Description=Ollama Model Preloader
After=docker.service
Requires=docker.service

[Service]
Type=oneshot
ExecStart=/usr/bin/docker exec ollama-server ollama run deepseek-coder-v2:16b ""
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
</span><span class="no">EOF

</span><span class="nb">sudo </span>systemctl <span class="nb">enable </span>ollama-preload
<span class="nb">sudo </span>systemctl start ollama-preload
</code></pre></div></div>

<p><br /></p>

<h3 id="2-context-window-adjustment">2. Context Window Adjustment</h3>

<p>Prevent performance degradation in long conversations:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Limit max_tokens in API calls
</span><span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"model"</span><span class="p">:</span> <span class="s">"deepseek-coder-v2:16b"</span><span class="p">,</span>
    <span class="s">"messages"</span><span class="p">:</span> <span class="n">messages</span><span class="p">,</span>
    <span class="s">"max_tokens"</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span>  <span class="c1"># Limit appropriately
</span>    <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.7</span>
<span class="p">}</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="3-batch-processing">3. Batch Processing</h3>

<p>For handling multiple simultaneous requests:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Add to docker-compose.yml environment section</span>
<span class="na">environment</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">OLLAMA_NUM_PARALLEL=2</span>
  <span class="pi">-</span> <span class="s">OLLAMA_MAX_LOADED_MODELS=2</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="4-keep_alive-optimization-strategy">4. KEEP_ALIVE Optimization Strategy</h3>

<p>Configure based on usage patterns:</p>

<p><br /></p>

<p><strong>Scenario 1: Development Team (All-day usage)</strong></p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">environment</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">OLLAMA_KEEP_ALIVE=-1</span>  <span class="c1"># Permanent retention</span>
</code></pre></div></div>

<p><strong>Scenario 2: Department Sharing (Business hours only)</strong></p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">environment</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">OLLAMA_KEEP_ALIVE=1h</span>  <span class="c1"># 1-hour retention</span>
</code></pre></div></div>

<p><strong>Scenario 3: Occasional Use (Sporadic usage)</strong></p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">environment</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">OLLAMA_KEEP_ALIVE=5m</span>  <span class="c1"># Default value</span>
</code></pre></div></div>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>This guide covered the complete process of building an on-premises LLM system using Ollama and Open WebUI, from NVIDIA GPU driver installation through Docker Compose container orchestration, NFS storage integration, to production-ready monitoring and backup strategies.</p>

<p><br /></p>

<h3 id="key-takeaways">Key Takeaways</h3>

<div class="info-box info-box-success-not-check">
  <strong>Production-Ready On-Premises AI Infrastructure</strong>
  <ul>
    <li><strong>RTX 3060 12GB Optimization</strong>: DeepSeek 16B provides optimal speed/performance balance</li>
    <li><strong>Docker Compose Advantage</strong>: Declarative container management simplifies maintenance</li>
    <li><strong>KEEP_ALIVE Flexibility</strong>: Choose -1, 30m, or 5m based on usage patterns</li>
    <li><strong>NFS Efficiency</strong>: Centralized storage maximizes resource utilization across servers</li>
    <li><strong>OpenAI-Compatible API</strong>: Seamlessly integrates with existing workflows</li>
  </ul>
</div>

<p><br /></p>

<h3 id="strategic-benefits">Strategic Benefits</h3>

<p>For security-critical industries like finance, healthcare, and government, on-premises LLM deployment offers critical advantages:</p>

<ol>
  <li><strong>Data Sovereignty</strong>: No data transmitted to external services</li>
  <li><strong>Cost Predictability</strong>: Eliminate per-token API charges</li>
  <li><strong>Unlimited Usage</strong>: No rate limits or quotas</li>
  <li><strong>Customization</strong>: Full control over models and configurations</li>
  <li><strong>Compliance</strong>: Meet regulatory requirements for data handling</li>
</ol>

<p>While initial infrastructure investment is required, long-term benefits include API cost elimination and complete data control, making it a strategic win for organizations prioritizing security and autonomy.</p>

<p><br /></p>

<h3 id="future-roadmap">Future Roadmap</h3>

<p>Upcoming topics in this series will cover:</p>
<ul>
  <li><strong>Model Fine-tuning</strong>: Customizing models for specific domains</li>
  <li><strong>RAG Integration</strong>: Retrieval-Augmented Generation for knowledge bases</li>
  <li><strong>Kubernetes Deployment</strong>: Scalable orchestration for enterprise environments</li>
  <li><strong>Advanced Monitoring</strong>: Prometheus/Grafana integration</li>
  <li><strong>Multi-GPU Configuration</strong>: Scaling to handle larger models</li>
</ul>

<p><br /></p>

<hr />

<h2 id="references">References</h2>

<ol>
  <li>Ollama Official Documentation: https://github.com/ollama/ollama</li>
  <li>Open WebUI Official Repository: https://github.com/open-webui/open-webui</li>
  <li>NVIDIA Container Toolkit: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/</li>
  <li>Docker Compose Reference: https://docs.docker.com/compose/</li>
  <li>NFS Configuration Guide: https://ubuntu.com/server/docs/service-nfs</li>
</ol>

<hr />

<p><strong>Questions or feedback?</strong> Feel free to leave comments on the blog, and I’ll respond promptly. Thank you for reading!</p>


                <!-- Pagination links -->


            </article>

            
                <aside class="see-also">
                    <h2>See also</h2>
                    <ul>
                        
                        
                        
                            <li>
                                <a href="/category/container/docker-image-optimization/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1767693107/docker-image-opt-1_bfyljd.png">
                                    
                                    <h3>Docker Image Optimization Practical Guide</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/aws/aws-nacl-sg/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1767938733/aws-security-group-network-acl-1_tjbvpd.png">
                                    
                                    <h3>AWS Network ACL vs Security Group - Complete Comparison and Implementation Guide</h3>
                                </a>
                            </li>
                        
                            <li>
                                <a href="/category/cs/context-switching/">
                                    
                                        <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1767769259/context-switch-1_s1n69e.png">
                                    
                                    <h3>Context Switch Deep Dive - The Hidden Performance Cost in Container Environments</h3>
                                </a>
                            </li>
                        
                    </ul>
                </aside>
            

        </section>

        <!-- Add time bar only for pages without pagination -->
        
            <div class="time-bar" data-minutes="24">
    <span class="time-completed"></span>
    <span class="time-remaining"></span>
    <div class="bar">
        <span class="completed" style="width:0%;"></span>
        <span class="remaining" style="width:100%;"></span>
    </div>
</div>

            <button class="toggle-preview" onclick="togglePreview()">
    <span>Hide Preview ▼</span>
</button>

<div id="recommendationSection" class="recommendation">
    <div class="message">
        <strong>Why don't you read something next?</strong>
        <div>
            <button>
                <svg><use xlink:href="#icon-arrow-right"></use></svg>
                <span>Go back to top</span>
            </button>
        </div>
    </div>
    <div id="previewSection" class="preview-section">
        
        <a href="/category/container/docker-image-optimization/" class="post-preview">
            <div class="image">
                
                    <img src="https://res.cloudinary.com/dkcm26aem/image/upload/c_scale,w_380/v1767693107/docker-image-opt-1_bfyljd.png">
                
            </div>
            <h3 class="title">Docker Image Optimization Practical Guide</h3>
        </a>
    </div>
</div>

<style>
.toggle-preview {
    position: fixed;
    bottom: 20px;
    right: 20px;
    background: #333;
    color: white;
    border: none;
    padding: 8px 15px;
    border-radius: 4px;
    cursor: pointer;
    z-index: 1000;
    opacity: 0;
    transition: opacity 0.3s ease;
}

.toggle-preview:hover {
    background: #444;
}

.toggle-preview.visible {
    opacity: 1;
}

.recommendation {
    margin-top: 1000px;
    display: block;
    transition: all 0.3s ease;
}

.recommendation.hidden {
    display: none;
}

.hide-preview {
    margin-left: 10px;
    background: none;
    border: 1px solid #666;
    color: #666;
    padding: 5px 10px;
    border-radius: 4px;
    cursor: pointer;
}

.hide-preview:hover {
    background: #f0f0f0;
}

.preview-section {
    max-height: 1000px;
    overflow: hidden;
    transition: max-height 0.3s ease-out;
}

.preview-section.hidden {
    max-height: 0;
}
</style>

<script>
function togglePreview() {
    const recommendation = document.getElementById('recommendationSection');
    const button = document.querySelector('.toggle-preview span');
    
    if (recommendation.classList.contains('hidden')) {
        recommendation.classList.remove('hidden');
        button.textContent = 'Hide Preview ▼';
    } else {
        recommendation.classList.add('hidden');
        button.textContent = 'Show Preview ▲';
    }
}

window.addEventListener('scroll', function() {
    const toggleButton = document.querySelector('.toggle-preview');
    const recommendation = document.getElementById('recommendationSection');
    const rect = recommendation.getBoundingClientRect();
    
    if (rect.top <= window.innerHeight) {
        toggleButton.classList.add('visible');
    } else {
        toggleButton.classList.remove('visible');
    }
});
</script>

        

        <!-- Show modal if the post is the last one -->
        

        <!-- Show modal before user leaves the page -->
        

        <!-- Add your newsletter subscription form here -->

        <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;Comprehensive tutorial for DevOps engineers on building a secure, cost-effective on-premises LLM system using Ollama and Open WebUI with Docker Compose, NFS integration, and production-ready monitoring strategies&quot;%20https://somaz.blog/category/ai/llm-ollama-open-webui/%20via%20&#64;twitter_username&hashtags=ollama,open-webui,llm,on-premises-ai,docker-compose,nvidia-gpu,nfs-storage,devops,infrastructure"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://somaz.blog/category/ai/llm-ollama-open-webui/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

        

  <section class="author">
    <div class="details">
      
        <img class="img-rounded" src="/assets/img/uploads/profile.png" alt="Somaz">
      
      <p class="def">Author</p>
      <h3 class="name">
        <a href="/authors/somaz/">Somaz</a>
      </h3>
      <p class="desc">DevOps engineer focused on cloud infrastructure and automation</p>
      <p>
        
          <a href="https://github.com/somaz94" title="Github">
            <svg><use xlink:href="#icon-github"></use></svg>
          </a>
        
        
        
        
        
        
          <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
            <svg><use xlink:href="#icon-linkedin"></use></svg>
          </a>
        
        
          <a href="https://somaz.tistory.com" title="Tistory">
            <svg><use xlink:href="#icon-tistory"></use></svg>
          </a>
        
      </p>
    </div>
  </section>

  
  
  
  
  
  
  
  

  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Somaz",
      
      "image": "/assets/img/uploads/profile.png",
      
      "jobTitle": "DevOps Engineer",
      "url": "https://somaz.blog/authors/somaz/",
      "sameAs": [
        "https://github.com/somaz94","https://www.linkedin.com/in/somaz","https://{{ author.tistory_username }}.tistory.com"
      ]
  }
  </script>


        

<section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'https-somaz94-github-io';
        var disqus_title = '';
        var disqus_url = '/category/ai/llm-ollama-open-webui/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>



        <footer>
    <p>
      
        <a href="https://github.com/somaz94" title="Github">
          <svg><use xlink:href="#icon-github"></use></svg>
        </a>
      
      
        <a href="https://www.facebook.com/facebook_username" title="Facebook">
          <svg><use xlink:href="#icon-facebook"></use></svg>
        </a>
      
      
        <a href="https://twitter.com/twitter_username" title="Twitter">
          <svg><use xlink:href="#icon-twitter"></use></svg>
        </a>
      
      
        <a href="https://medium.com/@medium_username" title="Medium">
          <svg><use xlink:href="#icon-medium"></use></svg>
        </a>
      
      
        <a href="https://www.instagram.com/instagram_username" title="Instagram">
          <svg><use xlink:href="#icon-instagram"></use></svg>
        </a>
      
      
        <a href="https://www.linkedin.com/in/somaz" title="LinkedIn">
          <svg><use xlink:href="#icon-linkedin"></use></svg>
        </a>
      
      
        <a href="https://somaz.tistory.com" title="Tistory">
          <svg><use xlink:href="#icon-tistory"></use></svg>
        </a>
      
    </p>

    <ul>
  
    
      <li>
        <a href="https://somaz.blog/">Home</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/about">About</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/category">Category</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/contact">Contact</a>
      </li>
    
  
    
      <li>
        <a href="https://somaz.blog/feed.xml">Feed</a>
      </li>
    
  
</ul>


    <p>
      <a href="https://somaz.blog/sitemap.xml" title="sitemap">Sitemap</a> |
      <a href="https://somaz.blog/privacy-policy" title="Privacy Policy">Privacy Policy</a>
    </p>

    <p>
      <span>Somaz Tech Blog</span> <svg class="love"><use xlink:href="#icon-heart"></use></svg>
    </p>
</footer>










<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "somaz",
  "description": "DevOps engineer's tech blog.",
  "url": "https://somaz.blog/",
  "logo": {
      "@type": "ImageObject",
      "url": "https://somaz.blog/assets/img/icons/mediumtile.png",
      "width": "600",
      "height": "315"
  },
  "sameAs": [
    "https://github.com/somaz94","https://www.facebook.com/facebook_username","https://twitter.com/twitter_username","https://medium.com/@medium_username","https://www.instagram.com/instagram_username","https://www.linkedin.com/in/somaz","https://{{ site.tistory_username }}.tistory.com"
  ]
}
</script>

<!-- Include the script that allows Netlify CMS login -->
<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>

<!-- Include the website scripts -->
<script src="/assets/js/scripts.min.js"></script>

<!-- Include Google Analytics script -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  var host = window.location.hostname;
  if (host != 'localhost') {
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-XXXXXXXX-X');
  }
</script>
  


<!-- Include extra scripts -->



        

        
        
        
        
        
        
        
        
        <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "BlogPosting",
            "name": "Building an On-Premises LLM System with Ollama + Open WebUI",
            "headline": "Complete guide to deploying enterprise AI infrastructure with NVIDIA GPU acceleration, NFS storage, and Docker orchestration",
            "description": "Comprehensive tutorial for DevOps engineers on building a secure, cost-effective on-premises LLM system using Ollama and Open WebUI with Docker Compose, NFS integration, and production-ready monitoring strategies",
            "image": "https://res.cloudinary.com/dkcm26aem/image/upload/v1770607787/ollama-openwebui-llm-1_dl6rlu.png",
            "url": "https://somaz.blog/category/ai/llm-ollama-open-webui/",
            "articleBody": "

Table of Contents


  Overview
  System Architecture &amp;amp; Requirements
  Step 1: NVIDIA Driver Installation
  Step 2: Docker &amp;amp; NVIDIA Container Toolkit
  Step 3: NFS Storage Configuration
  Step 4: Docker Network Setup
  Step 5: Ollama Server Deployment
  Step 6: Open WebUI Installation
  Step 7: API Integration &amp;amp; Usage
  Step 8: Monitoring &amp;amp; Maintenance
  Troubleshooting Guide
  Security Considerations
  Performance Optimization
  Conclusion
  References




Overview

While commercial LLM services like ChatGPT and Claude have become mainstream, security-conscious enterprise environments often cannot transmit sensitive data to external APIs. Building an on-premises LLM infrastructure enables organizations to leverage AI capabilities while maintaining complete data sovereignty and security control.

This comprehensive guide covers the entire process of deploying a production-ready internal LLM system using Ollama for model serving and Open WebUI for the user interface. We’ll implement NFS-based storage management and Docker Compose orchestration, providing a complete solution ready for immediate enterprise deployment.

The architecture prioritizes security, cost efficiency, and operational simplicity, making it ideal for DevOps engineers and infrastructure teams building AI capabilities for their organizations.


  Key Benefits: On-premises deployment eliminates API costs, ensures data privacy, provides unlimited usage, and maintains full infrastructure control while delivering ChatGPT-like user experience to internal teams.






System Architecture &amp;amp; Requirements



Hardware Environment

This implementation uses the following system specifications:



GPU Server (192.168.1.20)


  
    
      Component
      Specification
    
    
      CPU
      Intel Xeon 6-core
    
    
      RAM
      32GB
    
    
      GPU
      NVIDIA GeForce RTX 3060 12GB
    
    
      OS
      Ubuntu 24.04 LTS
    
    
      Purpose
      Ollama model inference server
    
  




Web Server (192.168.1.21)


  
    
      Component
      Specification
    
    
      CPU
      4-core
    
    
      RAM
      16GB
    
    
      OS
      Ubuntu 24.04 LTS
    
    
      Purpose
      Open WebUI frontend
    
  




Storage Server (192.168.1.25)


  
    
      Component
      Specification
    
    
      NFS Export
      /storage/ai-data (shared directory)
    
    
      Purpose
      Centralized model storage
    
  




GPU Compatibility Verification

First, verify your installed GPU specifications:
lspci | grep -iE &quot;vga|display|3d&quot;
# Expected output:
# 01:00.0 VGA compatible controller: NVIDIA Corporation GA106 [GeForce RTX 3060]




RTX 3060 Key Specifications:

  Architecture: Ampere (GA106)
  CUDA Cores: 3,584
  VRAM: 12GB GDDR6
  Memory Bus: 192-bit
  CUDA Compute Capability: 8.6
  Recommended Driver: 525+ (535 stable)
  TDP: 170W




Architecture Diagram
┌─────────────────────────────────────────────────────────┐
│                   NFS Storage Server                     │
│                  (192.168.1.25)                          │
│         /storage/ai-data/{models,webui}                  │
└────────────────────┬────────────────────────────────────┘
                     │
        ┌────────────┴────────────┐
        │                         │
┌───────▼──────────┐     ┌───────▼──────────┐
│  GPU Server      │     │  Web Server      │
│  (192.168.1.20)  │     │  (192.168.1.21)  │
│                  │     │                  │
│  ┌────────────┐  │     │  ┌────────────┐  │
│  │  Ollama    │  │     │  │ Open WebUI │  │
│  │  Container │  │     │  │  Container │  │
│  │            │  │     │  │            │  │
│  │ RTX 3060   │  │     │  │  Port 8080 │  │
│  │ 12GB VRAM  │  │     │  └────────────┘  │
│  └────────────┘  │     │                  │
│  Port: 11434     │     │                  │
└──────────────────┘     └──────────────────┘
         │                        │
         └────────┬───────────────┘
                  │
          Docker Network: ai_network




Step 1: NVIDIA Driver Installation



Secure Boot Status Check
mokutil --sb-state
# Output will be either:
# SecureBoot disabled  → Direct installation possible
# SecureBoot enabled   → MOK (Machine Owner Key) enrollment required post-install


If Secure Boot is enabled, you’ll need to complete the MOK enrollment process after driver installation.



Available Driver Version Check
# Check available driver versions in repository
apt-cache search nvidia-driver

# View detailed information for specific version
apt-cache show nvidia-driver-535 | grep Version
# Example output:
# Version: 535.288.01-0ubuntu0.24.04.1


Alternatively, verify the latest version on NVIDIA’s official website:

  URL: https://www.nvidia.com/Download/index.aspx
  Product Type: GeForce
  Product Series: GeForce RTX 30 Series
  Product: GeForce RTX 3060
  Operating System: Linux 64-bit




Driver Installation Process
# Update system packages
sudo apt update

# Install NVIDIA driver
sudo apt install -y nvidia-driver-535

# Reboot system
sudo reboot




MOK Enrollment (If Secure Boot Enabled)

After reboot, if Secure Boot is enabled, a blue MOK management screen will appear:


  Select “Enroll MOK”
  Select “Continue”
  Enter the password you set during installation
  Select “Reboot”




Installation Verification
nvidia-smi


Expected output for successful installation:
Wed Feb  5 10:15:23 2026       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.288.01             Driver Version: 535.288.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0 Off |                  N/A |
|  0%   35C    P8              15W / 170W |      0MiB / 12288MiB |      0%      Default |
+---------------------------------------------------------------------------------------+




Step 2: Docker &amp;amp; NVIDIA Container Toolkit



Docker Installation
# Verify Docker installation
docker --version

# If not installed, install Docker
sudo apt update
sudo apt install -y docker.io
sudo systemctl enable --now docker

# Optional: Add current user to docker group
sudo usermod -aG docker $USER
# Note: Logout and login required for group changes to take effect




NVIDIA Container Toolkit Installation

The NVIDIA Container Toolkit enables GPU access from within Docker containers, essential for running Ollama with GPU acceleration.







Docker Runtime Configuration
# Configure Docker to use NVIDIA runtime
sudo nvidia-ctk runtime configure --runtime=docker

# Restart Docker service
sudo systemctl restart docker




GPU Access Test
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi


If GPU information is displayed correctly, the setup is complete and GPU acceleration is available to containers.



Step 3: NFS Storage Configuration

Centralizing model files and data through NFS enables efficient storage sharing across multiple servers and simplifies backup management.



Client Installation &amp;amp; Mounting
# Install NFS client
sudo apt install -y nfs-common

# Create mount points
sudo mkdir -p /data/llm-models
sudo mkdir -p /data/webui-storage

# Mount NFS shares
sudo mount -t nfs4 192.168.1.25:/storage/ai-data/models /data/llm-models
sudo mount -t nfs4 192.168.1.25:/storage/ai-data/webui /data/webui-storage

# Verify mounts
df -h | grep nfs




Persistent Mount Configuration

Configure /etc/fstab for automatic mounting after reboots:
sudo nano /etc/fstab

# Add these lines:
192.168.1.25:/storage/ai-data/models  /data/llm-models     nfs4  defaults,_netdev,rw  0  0
192.168.1.25:/storage/ai-data/webui   /data/webui-storage  nfs4  defaults,_netdev,rw  0  0


Configuration parameters explained:

  defaults: Use default NFS mount options
  _netdev: Wait for network before mounting
  rw: Read-write access
  0 0: No dump, no fsck


Verify configuration:
sudo mount -a
df -h | grep nfs




Permission Configuration
# Ollama data directory permissions
sudo chown -R root:root /data/llm-models
sudo chmod 755 /data/llm-models

# Open WebUI data directory permissions
sudo chown -R root:root /data/webui-storage
sudo chmod 755 /data/webui-storage




Step 4: Docker Network Setup

Create a shared Docker network to enable communication between Ollama and Open WebUI containers:
# Create Docker bridge network
docker network create ai_network

# Verify network creation
docker network ls
# Expected output should include:
# NETWORK ID     NAME         DRIVER    SCOPE
# abc123def456   ai_network   bridge    local


This dedicated network provides:

  Isolation: Containers communicate only within this network
  Service Discovery: Containers can reference each other by name
  Security: Network-level separation from other containers




Step 5: Ollama Server Deployment



Docker Compose Configuration

Create the Ollama service configuration with GPU acceleration and NFS storage:
mkdir -p /opt/ollama
cat &amp;gt; /opt/ollama/docker-compose.yml &amp;lt;&amp;lt; &apos;EOF&apos;
version: &apos;3.8&apos;

services:
  ollama:
    image: ollama/ollama:0.15.4
    container_name: ollama-server
    restart: unless-stopped
    runtime: nvidia
    
    ports:
      - &quot;11434:11434&quot;
    
    volumes:
      - /data/llm-models:/root/.ollama
    
    environment:
      # Model memory retention settings
      # -1: Keep loaded indefinitely (never unload, always in VRAM)
      # 0: Unload immediately after use (free VRAM instantly)
      # 5m: Unload after 5 minutes (default)
      # 30m: Unload after 30 minutes
      # 1h: Unload after 1 hour
      # 
      # Setting to -1:
      #   Pros: Instant response after first load (no loading delay)
      #   Cons: Constant VRAM occupation (qwen2.5-coder:14b = ~8GB permanently)
      # 
      # Recommendation: -1 for frequent use, 30m for occasional use
      - OLLAMA_KEEP_ALIVE=-1
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_ORIGINS=*
    
    networks:
      - ai_network

networks:
  ai_network:
    name: ai_network
    external: true
EOF




Environment Variable Detailed Explanation


  
    
      Variable
      Description
    
    
      OLLAMA_KEEP_ALIVE
      Controls model memory retention duration
    
    
      -1
      Permanent retention (first load only, then instant responses)
    
    
      5m (default)
      5-minute retention (balances memory and responsiveness)
    
    
      30m
      30-minute retention (recommended for regular use)
    
    
      0
      Immediate unload (maximum memory conservation)
    
    
      NVIDIA_VISIBLE_DEVICES
      Specifies which GPUs to use (all = all available GPUs)
    
    
      OLLAMA_ORIGINS
      CORS configuration (* = allow all origins)
    
  




KEEP_ALIVE Configuration Guide


  
    
      Usage Pattern
      Recommendation
      Rationale
    
    
      Frequent (10+ times/day)
      -1
      Fast responses justify VRAM occupation
    
    
      Regular (3-10 times/day)
      30m or 1h
      Balance between speed and memory efficiency
    
    
      Occasional (1-2 times/day)
      5m (default)
      Maximize memory availability
    
  




Container Deployment
# Navigate to configuration directory
cd /opt/ollama

# Start container
docker-compose up -d

# Monitor logs
docker-compose logs -f

# Check container status
docker-compose ps




GPU Usage Verification
# Execute nvidia-smi inside container
docker exec -it ollama-server nvidia-smi

# Verify NFS mount
docker exec -it ollama-server df -h /root/.ollama




LLM Model Download

Ollama provides various models at https://ollama.com/library. Recommended models for RTX 3060 12GB:



1) DeepSeek-Coder-V2 16B (Coding Specialist, 10GB)
docker exec -it ollama-server ollama pull deepseek-coder-v2:16b


2) Qwen2.5-Coder 14B (Coding, 9.7GB)
docker exec -it ollama-server ollama pull qwen2.5-coder:14b


3) Qwen3 14B (General Purpose, 11GB)
docker exec -it ollama-server ollama pull qwen3:14b


4) Qwen2.5-Coder 32B (High Performance Coding, 21GB - CPU Offloading)
docker exec -it ollama-server ollama pull qwen2.5-coder:32b


Verify downloaded models:
ls -lh /data/llm-models/models/




Model Testing
# Test DeepSeek 16B
docker exec -it ollama-server ollama run deepseek-coder-v2:16b
&amp;gt;&amp;gt;&amp;gt; Write a Python function to reverse a string

# Exit: Ctrl+D or type /bye




Model Performance Comparison (RTX 3060 12GB)



DeepSeek-Coder-V2 16B (10GB)
docker exec -it ollama-server ollama ps

NAME                     ID              SIZE     PROCESSOR    CONTEXT
deepseek-coder-v2:16b    63fb193b3a9b    10 GB    100% GPU     4096


Memory Usage:
|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0 Off |                  N/A |
|  0%   49C    P2              26W / 170W |   9898MiB / 12288MiB |      0%      Default |




Qwen2.5-Coder 14B (9.7GB)
NAME                 ID              SIZE      PROCESSOR    CONTEXT
qwen2.5-coder:14b    9ec8897f747e    9.7 GB    100% GPU     4096


Memory Usage:
|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0 Off |                  N/A |
|  0%   50C    P2              44W / 170W |   9482MiB / 12288MiB |      0%      Default |




Qwen2.5-Coder 32B (21GB - CPU Offloading)
NAME                 ID              SIZE     PROCESSOR          CONTEXT
qwen2.5-coder:32b    b92d6a0bd47e    21 GB    45%/55% CPU/GPU    4096


Memory Usage:
|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0 Off |                  N/A |
|  0%   60C    P2              60W / 170W |  11476MiB / 12288MiB |     17%      Default |




Performance Analysis &amp;amp; Recommendations


  
    
      Model
      Size
      VRAM
      Processing
      Speed
      Best For
    
    
      DeepSeek 16B
      10GB
      9.9GB
      100% GPU
      ★★★★★
      Real-time coding (fastest)
    
    
      Qwen2.5 14B
      9.7GB
      9.5GB
      100% GPU
      ★★★★☆
      General coding tasks
    
    
      Qwen3 14B
      11GB
      11GB
      100% GPU
      ★★★★☆
      Complex reasoning/logic
    
    
      Qwen2.5 32B
      21GB
      11.5GB
      45% CPU55% GPU
      ★★☆☆☆
      High quality needed(5× slower)
    
  




Key Insights:

  VRAM ≤12GB models → 100% GPU processing → Fast response
  VRAM &amp;gt;12GB models → CPU offloading → Significantly slower
  Production recommendation: DeepSeek 16B (optimal speed/performance balance)




Model Management Commands
# Check running models
docker exec -it ollama-server ollama ps

# Unload models (free VRAM)
docker exec -it ollama-server ollama stop deepseek-coder-v2:16b
docker exec -it ollama-server ollama stop qwen2.5-coder:32b

# Delete models
docker exec -it ollama-server ollama rm qwen2.5-coder:32b

# List all installed models
docker exec -it ollama-server ollama list




Step 6: Open WebUI Installation

Open WebUI is an open-source project providing a ChatGPT-like web interface for Ollama models.



Docker Compose Configuration
mkdir -p /opt/open-webui
cat &amp;gt; /opt/open-webui/docker-compose.yml &amp;lt;&amp;lt; &apos;EOF&apos;
version: &apos;3.8&apos;

services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    
    ports:
      - &quot;8080:8080&quot;
    
    volumes:
      - /data/webui-storage:/app/backend/data
    
    environment:
      - OLLAMA_BASE_URL=http://192.168.1.20:11434
      - WEBUI_SECRET_KEY=your-random-secret-key-change-this
      - WEBUI_NAME=Company AI Assistant
      - DEFAULT_MODELS=deepseek-coder-v2:16b,qwen2.5-coder:14b
    
    networks:
      - ai_network

networks:
  ai_network:
    name: ai_network
    external: true
EOF




Environment Variables Explained


  
    
      Variable
      Description
    
    
      OLLAMA_BASE_URL
      Ollama server address (use service name for same network, IP for different servers)
    
    
      WEBUI_SECRET_KEY
      Session encryption key (must change to random string)
    
    
      WEBUI_NAME
      Web UI title/branding
    
    
      DEFAULT_MODELS
      Comma-separated list of models to display by default
    
  




Important Notes:

  Within same Docker network: Use http://ollama-server:11434
  Different physical servers: Use IP address http://192.168.1.20:11434
  Always change WEBUI_SECRET_KEY to a secure random string




Container Deployment
# Navigate to configuration directory
cd /opt/open-webui

# Start container
docker-compose up -d

# Monitor logs
docker-compose logs -f

# Check status
docker-compose ps




Web Interface Access

Access the web interface at http://192.168.1.21:8080

Initial Setup:

  Create account (first user becomes admin automatically)
  Navigate to Settings → Models → Verify Ollama connection
  Start chatting




Step 7: API Integration &amp;amp; Usage

Open WebUI provides OpenAI-compatible API endpoints for programmatic access.



API Key Generation


  Log in to Web UI
  Navigate to Settings → Account → API Keys
  Click “Create new secret key”
  Copy generated key (e.g., sk-abc123def456...)




API Usage Examples



cURL Test:





Python Example:
import requests
import json

API_URL = &quot;http://192.168.1.21:8080/api/chat/completions&quot;
API_KEY = &quot;sk-abc123def456...&quot;

headers = {
    &quot;Authorization&quot;: f&quot;Bearer {API_KEY}&quot;,
    &quot;Content-Type&quot;: &quot;application/json&quot;
}

payload = {
    &quot;model&quot;: &quot;deepseek-coder-v2:16b&quot;,
    &quot;messages&quot;: [
        {
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: &quot;You are a Python coding expert assistant.&quot;
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;Implement binary search algorithm&quot;
        }
    ],
    &quot;temperature&quot;: 0.7,
    &quot;max_tokens&quot;: 2000
}

response = requests.post(API_URL, headers=headers, json=payload)
result = response.json()

print(result[&apos;choices&apos;][0][&apos;message&apos;][&apos;content&apos;])




Streaming Response Example:
import requests
import json

payload = {
    &quot;model&quot;: &quot;qwen2.5-coder:14b&quot;,
    &quot;messages&quot;: [
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain REST API architecture&quot;}
    ],
    &quot;stream&quot;: True
}

response = requests.post(
    API_URL, 
    headers=headers, 
    json=payload, 
    stream=True
)

for line in response.iter_lines():
    if line:
        decoded = line.decode(&apos;utf-8&apos;)
        if decoded.startswith(&apos;data: &apos;):
            data = json.loads(decoded[6:])
            if &apos;choices&apos; in data:
                content = data[&apos;choices&apos;][0][&apos;delta&apos;].get(&apos;content&apos;, &apos;&apos;)
                print(content, end=&apos;&apos;, flush=True)




Step 8: Monitoring &amp;amp; Maintenance



GPU Monitoring

Real-time GPU usage:
watch -n 1 nvidia-smi


Log-based monitoring:




Docker Container Management
# Restart Ollama
cd /opt/ollama
docker-compose restart

# Restart Open WebUI
cd /opt/open-webui
docker-compose restart

# View recent logs (last 100 lines)
docker-compose logs --tail 100 ollama
docker-compose logs --tail 100 open-webui

# Monitor resource usage
docker stats




Backup Strategy



Model Files Backup:
# Backup NFS directory
sudo rsync -avz /data/llm-models/ /backup/llm-models-$(date +%Y%m%d)/


Open WebUI Data Backup:
# Backup user settings and chat history
sudo rsync -avz /data/webui-storage/ /backup/webui-storage-$(date +%Y%m%d)/




Automated Backup Script
cat &amp;gt; /usr/local/bin/backup-ai-system.sh &amp;lt;&amp;lt; &apos;EOF&apos;
#!/bin/bash

BACKUP_DIR=&quot;/backup/ai-system&quot;
DATE=$(date +%Y%m%d_%H%M%S)

# Create backup directory
mkdir -p ${BACKUP_DIR}/${DATE}

# Model backup (using symbolic link)
ln -sf /data/llm-models ${BACKUP_DIR}/${DATE}/models

# WebUI data backup
rsync -az /data/webui-storage/ ${BACKUP_DIR}/${DATE}/webui/

# Delete backups older than 7 days
find ${BACKUP_DIR} -type d -mtime +7 -exec rm -rf {} +

echo &quot;Backup completed: ${BACKUP_DIR}/${DATE}&quot;
EOF

chmod +x /usr/local/bin/backup-ai-system.sh

# Schedule daily backup at 2 AM
(crontab -l 2&amp;gt;/dev/null; echo &quot;0 2 * * * /usr/local/bin/backup-ai-system.sh &amp;gt;&amp;gt; /var/log/ai-backup.log 2&amp;gt;&amp;amp;1&quot;) | crontab -




Troubleshooting Guide



GPU Not Detected

Symptom: docker exec -it ollama-server nvidia-smi fails

Solution:
# Reinstall drivers
sudo apt purge -y nvidia-*
sudo apt install -y nvidia-driver-535
sudo reboot

# Reconfigure Container Toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker




Slow Ollama Response (32B Model)

Symptom: Very slow responses, GPU memory insufficient

Solution: Use smaller model or upgrade GPU
# Switch to memory-efficient model
docker exec -it ollama-server ollama stop qwen2.5-coder:32b
docker exec -it ollama-server ollama run deepseek-coder-v2:16b




NFS Mount Failure

Symptom: mount.nfs4: Connection timed out

Solution:
# Verify NFS server
showmount -e 192.168.1.25

# Check firewall (on NFS server)
sudo ufw allow from 192.168.1.0/24 to any port nfs

# Remount
sudo umount /data/llm-models
sudo mount -a




Open WebUI Connection Error

Symptom: “Failed to connect to Ollama”

Solution:
# Test Ollama API
curl http://192.168.1.20:11434/api/tags

# Verify network
docker network inspect ai_network

# Update environment variable
cd /opt/open-webui
docker-compose down
# Edit docker-compose.yml to update OLLAMA_BASE_URL
docker-compose up -d




Long Model Loading Time

Symptom: First request takes 10-30 seconds

Solution: Adjust OLLAMA_KEEP_ALIVE setting
# Edit docker-compose.yml
environment:
  - OLLAMA_KEEP_ALIVE=-1  # Change to permanent retention

# Restart
cd /opt/ollama
docker-compose down
docker-compose up -d




Security Considerations



Firewall Configuration
# Enable UFW
sudo ufw enable

# Ollama API (internal network only)
sudo ufw allow from 192.168.1.0/24 to any port 11434

# Open WebUI (VPN users only if needed)
sudo ufw allow from 10.8.0.0/24 to any port 8080

# SSH (management)
sudo ufw allow 22/tcp




Reverse Proxy Setup (Nginx)

For external access, HTTPS is strongly recommended:
# /etc/nginx/sites-available/ai-assistant
server {
    listen 443 ssl http2;
    server_name ai.company.com;
    
    ssl_certificate /etc/letsencrypt/live/ai.company.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/ai.company.com/privkey.pem;
    
    location / {
        proxy_pass http://192.168.1.21:8080;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection &apos;upgrade&apos;;
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
        
        # WebSocket support
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}




User Access Control

Configure from Open WebUI admin page:

  Settings → Users → User Permissions
  Set per-user model access permissions
  Configure daily request limits




Performance Optimization



1. Model Preloading

Preload frequently used models into memory:
# Create systemd service
cat &amp;gt; /etc/systemd/system/ollama-preload.service &amp;lt;&amp;lt; &apos;EOF&apos;
[Unit]
Description=Ollama Model Preloader
After=docker.service
Requires=docker.service

[Service]
Type=oneshot
ExecStart=/usr/bin/docker exec ollama-server ollama run deepseek-coder-v2:16b &quot;&quot;
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl enable ollama-preload
sudo systemctl start ollama-preload




2. Context Window Adjustment

Prevent performance degradation in long conversations:
# Limit max_tokens in API calls
payload = {
    &quot;model&quot;: &quot;deepseek-coder-v2:16b&quot;,
    &quot;messages&quot;: messages,
    &quot;max_tokens&quot;: 1024,  # Limit appropriately
    &quot;temperature&quot;: 0.7
}




3. Batch Processing

For handling multiple simultaneous requests:
# Add to docker-compose.yml environment section
environment:
  - OLLAMA_NUM_PARALLEL=2
  - OLLAMA_MAX_LOADED_MODELS=2




4. KEEP_ALIVE Optimization Strategy

Configure based on usage patterns:



Scenario 1: Development Team (All-day usage)
environment:
  - OLLAMA_KEEP_ALIVE=-1  # Permanent retention


Scenario 2: Department Sharing (Business hours only)
environment:
  - OLLAMA_KEEP_ALIVE=1h  # 1-hour retention


Scenario 3: Occasional Use (Sporadic usage)
environment:
  - OLLAMA_KEEP_ALIVE=5m  # Default value




Conclusion

This guide covered the complete process of building an on-premises LLM system using Ollama and Open WebUI, from NVIDIA GPU driver installation through Docker Compose container orchestration, NFS storage integration, to production-ready monitoring and backup strategies.



Key Takeaways


  Production-Ready On-Premises AI Infrastructure
  
    RTX 3060 12GB Optimization: DeepSeek 16B provides optimal speed/performance balance
    Docker Compose Advantage: Declarative container management simplifies maintenance
    KEEP_ALIVE Flexibility: Choose -1, 30m, or 5m based on usage patterns
    NFS Efficiency: Centralized storage maximizes resource utilization across servers
    OpenAI-Compatible API: Seamlessly integrates with existing workflows
  




Strategic Benefits

For security-critical industries like finance, healthcare, and government, on-premises LLM deployment offers critical advantages:


  Data Sovereignty: No data transmitted to external services
  Cost Predictability: Eliminate per-token API charges
  Unlimited Usage: No rate limits or quotas
  Customization: Full control over models and configurations
  Compliance: Meet regulatory requirements for data handling


While initial infrastructure investment is required, long-term benefits include API cost elimination and complete data control, making it a strategic win for organizations prioritizing security and autonomy.



Future Roadmap

Upcoming topics in this series will cover:

  Model Fine-tuning: Customizing models for specific domains
  RAG Integration: Retrieval-Augmented Generation for knowledge bases
  Kubernetes Deployment: Scalable orchestration for enterprise environments
  Advanced Monitoring: Prometheus/Grafana integration
  Multi-GPU Configuration: Scaling to handle larger models






References


  Ollama Official Documentation: https://github.com/ollama/ollama
  Open WebUI Official Repository: https://github.com/open-webui/open-webui
  NVIDIA Container Toolkit: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/
  Docker Compose Reference: https://docs.docker.com/compose/
  NFS Configuration Guide: https://ubuntu.com/server/docs/service-nfs




Questions or feedback? Feel free to leave comments on the blog, and I’ll respond promptly. Thank you for reading!
",
            "wordcount": "4352",
            "inLanguage": "en",
            "dateCreated": "2026-02-09/",
            "datePublished": "2026-02-09/",
            "dateModified": "2026-02-09/",
            "author": {
                "@type": "Person",
                "name": "Somaz",
                
                "image": "/assets/img/uploads/profile.png",
                
                "jobTitle": "DevOps Engineer",
                "url": "https://somaz.blog/authors/somaz/",
                "sameAs": [
                    "https://github.com/somaz94","https://www.linkedin.com/in/somaz"
                ]
            },
            "publisher": {
                "@type": "Organization",
                "name": "somaz",
                "url": "https://somaz.blog/",
                "logo": {
                    "@type": "ImageObject",
                    "url": "https://somaz.blog/assets/img/blog-image.png",
                    "width": "600",
                    "height": "315"
                }
            },
            "mainEntityOfPage": "True",
            "genre": "AI",
            "articleSection": "AI",
            "keywords": ["ollama","open-webui","llm","on-premises-ai","docker-compose","nvidia-gpu","nfs-storage","devops","infrastructure"]
        }
        </script>
    </body>
</html>
